{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a5963b",
   "metadata": {},
   "source": [
    "# Hooks\n",
    "**1. 什么是hook？**\n",
    "hook是一个特殊的<font color=blue>函数</font>，它在autograd.Function的forward或backward method调用的前后被执行，以实现预定的功能。\\\n",
    "**2. hook的使用对象？**\n",
    "hook可以<font color=blue>用于**tensor或module**</font>，称为register on a tensor or nn.Module。典型使用场景：\\\n",
    "· <font color=green>用于tensor的时候主要用来控制从forward向backward传递信息过程中的pack/unpack information。</font>\\\n",
    "· <font color=green>用于module中的时候，可以用于模型可视化，debug，gradient check等。</font> \\\n",
    "**3. hooks的类型？**\n",
    "pytorch提供了两种hooks：forward hook和backward hook。使用hook，要先register到使用hook的位置。forward hook又有两种使用位置，<font color=lightblue>forward prehook</font>在forward method之前执行；<font color=lightblue>forward hook</font>在forward method执行完之后执行。<font color=lightblue>backward hook</font>只有一种，在backward method执行完后执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7029e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.073804Z",
     "start_time": "2023-10-25T07:15:56.894779Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchviz\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b700f",
   "metadata": {},
   "source": [
    "## 1. Hooks for (autograd saved) tensors\n",
    "用于tensor的hook只有backward hook。\\\n",
    "**signature of hook function：**<font color=red>hook(grad) -> Tensor or None </font>\\\n",
    "**典型用途：**\n",
    "1. <font color=green>改变tensor的梯度计算方式。</font>某个tensor x的x.grad用hook改变后，DAG上位于x前面，依赖于x的tensor的梯度也按chainrule改变。如果不用tensor hook，要等bp结束后手动改变x和每一个前序tensor的梯度。\n",
    "2. <font color=green>查看intermediate tensor的grad，此时不占额外内存。</font>不用tensor hook的话就要用intermediate_tensor.remain_grad()，会占用内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15431950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.077866Z",
     "start_time": "2023-10-25T07:15:58.075459Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例1：用hook改变tensor梯度(eg:x.grad)的计算方式\n",
    "\n",
    "# 定义hook function\n",
    "def func(grad):\n",
    "    return grad * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b52dcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.100803Z",
     "start_time": "2023-10-25T07:15:58.079018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9064, -0.9064, -0.9064, -0.9064, -0.9064])\n",
      "tensor([-1.8891,  0.5235,  0.4592])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 1.without hook\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "y.retain_grad()\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819dab66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.107551Z",
     "start_time": "2023-10-25T07:15:58.102511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8128, -1.8128, -1.8128, -1.8128, -1.8128])\n",
      "tensor([-3.7783,  1.0471,  0.9184])\n",
      "tensor([-3.7783,  1.0471,  0.9184])\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# 2.with hook for loss\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "y.retain_grad()\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "#  register hook\n",
    "loss.register_hook(func)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# 因为loss的gradient翻倍，导致前向传递，w和b的梯度也翻倍\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "print(y.grad)\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86341c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.113405Z",
     "start_time": "2023-10-25T07:15:58.108654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8128, -1.8128, -1.8128, -1.8128, -1.8128])\n",
      "tensor([-1.8891,  0.5235,  0.4592])\n",
      "tensor([-1.8891,  0.5235,  0.4592])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 3.with hook for w\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "y.retain_grad()\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "#  register hook for w\n",
    "w.register_hook(func)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# 因为w的gradient翻倍，前向传递不影响loss,y和b\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "print(y.grad)\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5da50f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.118735Z",
     "start_time": "2023-10-25T07:15:58.114540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8891,  0.5235,  0.4592])\n"
     ]
    }
   ],
   "source": [
    "## 例2：tensor hooks用来查看intermediate tensor的grad，且不占额外内存\n",
    "#  定义hook\n",
    "def func(grad):\n",
    "    print(grad)\n",
    "    return grad\n",
    "\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "#  register hook for y，y在这里是intermediate tensor\n",
    "#  如果不用hook，就要设置y.retain_grad()才能在bp结束后查看，会占用内存\n",
    "y.register_hook(func)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff81173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.122181Z",
     "start_time": "2023-10-25T07:15:58.119828Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例3：Gradient clipping：对module的parameter用hook，直接改变其值\n",
    "def grad_clipper(model, val):\n",
    "    for parameter in model.parameters():\n",
    "        parameter.register_hook(lambda grad: grad.climp_(floor, cap))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "555de6bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.137713Z",
     "start_time": "2023-10-25T07:15:58.123267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "bias_grad in linear layer: tensor([-0.2000, -0.2000, -0.2000, -0.2000, -0.2000])\n"
     ]
    }
   ],
   "source": [
    "## 例4：用于module中\n",
    "#  要求：\n",
    "#    1. 将linear层中bias的梯度改为0\n",
    "#    2. conv layer从downstream拿到的gradient大小都不小于0\n",
    "\n",
    "# ----------------  原模型  ----------------\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = lambda x: x.view(-1)\n",
    "        self.fc = nn.Linear(160, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv(x))         \n",
    "        return self.fc(self.flatten(x))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "x = torch.randn(1, 3, 8, 8)\n",
    "net = TestNet()\n",
    "out = net(x)\n",
    "loss = (1 - out).mean()\n",
    "loss.backward()\n",
    "\n",
    "print(out.shape)\n",
    "# bias_grad in linear layer should be -1/out.shape\n",
    "print('bias_grad in linear layer:', net.fc.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9508718b",
   "metadata": {},
   "source": [
    "#### module中给tensor加入backward hook后，Backward pass的执行顺序\n",
    "1. 从root开始按照chainrule执行backward pass\n",
    "2. 遇到hook后，对制定tensor的grad执行相应的ops，如果同一位置有多个hook，按他们在module中出现的顺序执行操作，而不是想chainrule那样反向操作。如下例：\\\n",
    "fc layer backward method     -> \\\n",
    "flatten layer                -> \\\n",
    "register for clamp           -> \\\n",
    "register for print shape     -> \\\n",
    "register for check gradient  -> \\\n",
    "relu layer backward method   -> \\\n",
    "conv layer backward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8d175b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.146036Z",
     "start_time": "2023-10-25T07:15:58.138969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of output of flatten layer: torch.Size([160])\n",
      "size of output of relu layer: torch.Size([1, 10, 4, 4])\n",
      "any grad < 0? False\n",
      "bias_grad in linear layer: tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# ----------------  加入hook  ----------------\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = lambda x: x.view(-1)\n",
    "        self.fc = nn.Linear(160, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv(x))\n",
    "        \n",
    "        # 设置hook：让x.grad >= 0\n",
    "        # 这里处理的 x 是relu layer的output\n",
    "        # relu的梯度只有0和1，所以处理relu之后的x即可\n",
    "        x.register_hook(lambda grad: torch.clamp(grad, min=0))\n",
    "        \n",
    "        x.register_hook(lambda grad: print('size of output of relu layer:', grad.shape))\n",
    "        # 再加一个hook确认有没有x.grad是负值\n",
    "        # 放在这里改变的是relu运算后的x.grad\n",
    "        x.register_hook(lambda grad: print('any grad < 0?', \\\n",
    "                                           bool((grad < 0).any())))\n",
    "        x = self.flatten(x)\n",
    "        # 这里处理的 x 是flatten layer的output\n",
    "        x.register_hook(lambda grad: print('size of output of flatten layer:', grad.shape))\n",
    "\n",
    "        y = self.fc(x)\n",
    "        return y    \n",
    "    \n",
    "torch.manual_seed(2)\n",
    "x = torch.randn(1, 3, 8, 8)\n",
    "net = TestNet()\n",
    "\n",
    "# 在model外部，给参数tenosr设置hook: 这里将linear层中bias的梯度改为0\n",
    "# 这种方式可以在不改变module定义的条件下设置hook\n",
    "for name, param in net.named_parameters():\n",
    "    if 'fc' in name and 'bias' in name:\n",
    "        param.register_hook(lambda grad: torch.zeros(grad.shape))\n",
    "\n",
    "out = net(x)\n",
    "loss = (1 - out).mean()\n",
    "loss.backward()\n",
    "\n",
    "# 确认bias的grad成功改成了全0\n",
    "print('bias_grad in linear layer:', net.fc.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72724ed",
   "metadata": {},
   "source": [
    "## 2. Hooks for nn.Module objects\n",
    "用于nn.Module的hook有forward和backward hook。\\\n",
    "**signature of hook function：** \n",
    "1. for backward hook: <font color=red>hook(module, grad_input, grad_output) -> Tensor or None </font>\n",
    "2. for foreward hook: <font color=red>hook(module, input, output) -> None </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f0faaeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.151353Z",
     "start_time": "2023-10-25T07:15:58.148292Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例1：1个用hook查看module中activation value的例子\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.conv = nn.Conv2d(3,8,2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4,4))\n",
    "        self.fc = nn.Linear(8*4*4 , 1)\n",
    "    def forward(self, x):\n",
    "          x = F.relu(self.conv(x))\n",
    "          x = self.pool(x)\n",
    "          x = x.view(x.shape[0] , -1)\n",
    "          x = self.fc(x)\n",
    "          return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a14b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.154609Z",
     "start_time": "2023-10-25T07:15:58.152632Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义hook，用来提取activation的结果，也就是feature\n",
    "features = {} \n",
    "def hook_func(model, input ,output):\n",
    "    features['feature'] = output.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c027d770",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.159361Z",
     "start_time": "2023-10-25T07:15:58.155640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "# 在pooling layer上register hook\n",
    "net.pool.register_forward_hook(hook_func)\n",
    "\n",
    "x= torch.randn(1,3,10,10)\n",
    "output = net(x)\n",
    "print(features['feature'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a5c6a",
   "metadata": {},
   "source": [
    "### 2.1 典型应用场景和应用方式\n",
    "#### 场景1. 给module加wrapper，在wrapper module上加hooks，打印所需信息\n",
    "**优点：**\n",
    "1. 方便debug，避免手动增加和删除print的麻烦\n",
    "2. 不仅可以在自定义module，还可以在pytorch自带的module和第三方module上使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3151f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.163687Z",
     "start_time": "2023-10-25T07:15:58.160582Z"
    }
   },
   "outputs": [],
   "source": [
    "## 场景1例1：在ResNet18上用hooks来打印model信息\n",
    "\n",
    "#  给model加一个wrapper class\n",
    "class VerboseNet(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "        # register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            # 虽然所有nn.Mudule的实例都继承了__name__属性，但有的module可能没赋值\n",
    "            # 这一步可以明确让layer都有对应的name\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer, _, output: print(f\"new --->: {layer.__name__}: {output.shape}\")\n",
    "            )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d80d9e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.486989Z",
     "start_time": "2023-10-25T07:15:58.164823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new --->: conv1: torch.Size([10, 64, 14, 14])\n",
      "new --->: bn1: torch.Size([10, 64, 14, 14])\n",
      "new --->: relu: torch.Size([10, 64, 14, 14])\n",
      "new --->: maxpool: torch.Size([10, 64, 7, 7])\n",
      "new --->: layer1: torch.Size([10, 64, 7, 7])\n",
      "new --->: layer2: torch.Size([10, 128, 4, 4])\n",
      "new --->: layer3: torch.Size([10, 256, 2, 2])\n",
      "new --->: layer4: torch.Size([10, 512, 1, 1])\n",
      "new --->: avgpool: torch.Size([10, 512, 1, 1])\n",
      "new --->: fc: torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "verbose_resnet = VerboseNet(resnet18(weights=ResNet18_Weights.DEFAULT))\n",
    "dummy_input = torch.ones(10, 3, 28, 28)\n",
    "\n",
    "# 这里用了assignment是为了不让jupyter打印函数的output\n",
    "_ = verbose_resnet(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "393b4183",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.492567Z",
     "start_time": "2023-10-25T07:15:58.488534Z"
    }
   },
   "outputs": [],
   "source": [
    "## 场景1例2：Feature extraction\n",
    "#  说明：在一个预训练好的模型上做transfer learning时，可能想查看该模型的feature\n",
    "#  hook可以在不改变模型本身的条件下实现这一需求\n",
    "\n",
    "## 仍然是用wrapper\n",
    "class FeatureExt(nn.Module):\n",
    "    def __init__(self, model, layers):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self._features = {layer: torch.empty(0) for layer in layers}\n",
    "        \n",
    "        for layer_i in layers:\n",
    "            # 把named_modules的sub-module list构造成dict，用layer_i索引\n",
    "            # [说明见下一个cell]\n",
    "            layer = dict([*self.model.named_modules()])[layer_i]\n",
    "            \n",
    "            # layer_i索引出来对应layer后，给该layer加上hook:\n",
    "            layer.register_forward_hook(self.save_outputs_hook(layer_i))\n",
    "         \n",
    "    # 定义hook function：\n",
    "    def save_outputs_hook(self, layer_i):\n",
    "        def fn(_, __, output):  # 下划线长度不同，因为arguments name不能相同\n",
    "            # 将该layer的output提取出来，存入feature dict\n",
    "            self._features[layer_i] = output\n",
    "        return fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _ = self.model(x)\n",
    "        return self._features  # 返回dict存放了指定layers的activation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "493b1387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.711488Z",
     "start_time": "2023-10-25T07:15:58.493996Z"
    }
   },
   "outputs": [],
   "source": [
    "res18 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "## 对resnet.named_modules的说明：\n",
    "#  1. resnet18.named_modules是一个generator\n",
    "#     每次yield返回的是tuple，形如('layer_name', module)\n",
    "#  2. 加'*'来把generator解包成单独的tuples：*res18.named_modules()\n",
    "#  3. 将dict()用到list of tuples上，可以转换成key-value pair，便于用key索引\n",
    "#     这里要先将解包后的tuples打包到一个list里面，所以是用 dict([])\n",
    "\n",
    "#  ----- 打印sample layers 来查看网络的结构  -----\n",
    "# print([*res18.named_modules()][0]) # nest结构的顶层，描绘整个结构\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][5]) # nest结构的第2层，'layer1'\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][6]) # nest结构的第3层，'layer1'的sub-module\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][7]) # nest结构的第4层，..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d935a11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:15:58.733686Z",
     "start_time": "2023-10-25T07:15:58.713240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4 -> torch.Size([10, 512, 1, 1])\n",
      "avgpool -> torch.Size([10, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "res18_feats = FeatureExt(res18, layers=['layer4', 'avgpool'])\n",
    "\n",
    "fests = res18_feats(dummy_input)\n",
    "for name, output in fests.items():\n",
    "    print(name, '->', output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae42a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T07:02:32.343985Z",
     "start_time": "2023-10-25T07:02:32.338718Z"
    }
   },
   "source": [
    "#### 场景2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
