{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44edbaab",
   "metadata": {},
   "source": [
    "# Examples2_Torch.nn tutorial\n",
    "torch.nn package中的内容是graph的basic building blocks。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1391efe",
   "metadata": {},
   "source": [
    "### 1. preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7598c9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:46.138391Z",
     "start_time": "2024-08-07T12:18:46.048103Z"
    }
   },
   "outputs": [],
   "source": [
    "## download MNIST dataset\n",
    "\n",
    "from pathlib import Path # python3 standary lib\n",
    "import requests          # used to download dataset\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH  = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c11b0b",
   "metadata": {},
   "source": [
    "MNIST dataset是numpy array格式, 用python专门处理serializing data的pickle格式存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb02f3bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:47.176745Z",
     "start_time": "2024-08-07T12:18:46.140098Z"
    }
   },
   "outputs": [],
   "source": [
    "## extract data from original files\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae2f95",
   "metadata": {},
   "source": [
    "Each image is 28 x 28, and is being stored as a flattened row of length 784 (=28x28). To plot one image, we need to reshape it to 2d first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de8fc7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:47.559000Z",
     "start_time": "2024-08-07T12:18:47.179234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## plot a image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "plt.show()\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595a173e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:48.865371Z",
     "start_time": "2024-08-07T12:18:47.561037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "## convert the date from numpy arrays to tensors\n",
    "\n",
    "import torch\n",
    "\n",
    "x_train, y_train, x_val, y_val = map(torch.tensor, \n",
    "                                     (x_train, y_train, x_valid, y_valid))\n",
    "\n",
    "N, C = x_train.shape\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d57fd",
   "metadata": {},
   "source": [
    "## 2. construct a NN\n",
    "### 2.1 只用最底层tensor operation\n",
    "<font color=blue>**· 完全手写函数**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45896412",
   "metadata": {},
   "source": [
    "**step1: 初始化weights and bias** \\\n",
    "初始化一般都是in-place操作，节省memory。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e67eea1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:48.877969Z",
     "start_time": "2024-08-07T12:18:48.866959Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "torch.manual_seed(231)\n",
    "weights = torch.randn(784, 10)    # random init\n",
    "weights /= math.sqrt(784)         # Xaiver init\n",
    "weights.requires_grad_()          # init之后再set gradient，因为init不用求梯度\n",
    "\n",
    "# 区分：\n",
    "# tensor.requires_grad是tensor的属性，tensor.requires_grad_()是设置该属性的method\n",
    "# tensor.requires_grad_()设置方式是in-place，用途是告诉autograd开始记录这个tensor上的操作\n",
    "\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8f61b",
   "metadata": {},
   "source": [
    "**step2: 定义forward function, 用model打包**\\\n",
    "<font color=blue>· tensor调用的function都有对应的grad_fn，能自动构造graph，计算backward</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c320249",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:48.883583Z",
     "start_time": "2024-08-07T12:18:48.878984Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1) # 'unsqueeze' is for broadcast   \n",
    "\n",
    "def model(x_batch):\n",
    "    return log_softmax(x_batch @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe17fbf",
   "metadata": {},
   "source": [
    "**step3: 定义loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b51067",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:48.888716Z",
     "start_time": "2024-08-07T12:18:48.884667Z"
    }
   },
   "outputs": [],
   "source": [
    "def negative_log_likelyhood(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = negative_log_likelyhood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded2c2b",
   "metadata": {},
   "source": [
    "**step4: 定义accuracy function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e218ee84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:48.892639Z",
     "start_time": "2024-08-07T12:18:48.889892Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(out, y_batch):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds==y_batch).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71fcfcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T05:45:30.593113Z",
     "start_time": "2023-10-12T05:45:30.588334Z"
    }
   },
   "source": [
    "#### 一个完整的trainning loop\n",
    "step1: mini-batch data\\\n",
    "step2: forward, make prediction\\\n",
    "step3: loss\\\n",
    "step4: backward\\\n",
    "step5: update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61dd40f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:49.607295Z",
     "start_time": "2024-08-07T12:18:48.893772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 get loss:0.39476436376571655\n",
      "epoch1 get loss:0.305912047624588\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((N - 1) // batch_size + 1):\n",
    "        start_i = i * batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        x_batch = x_train[start_i:end_i]\n",
    "        y_batch = y_train[start_i:end_i]\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_func(pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "    print(f'epoch{epoch} get loss:{loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f89083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:49.623051Z",
     "start_time": "2024-08-07T12:18:49.610648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0809, grad_fn=<NegBackward0>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(x_batch), y_batch), accuracy(model(x_batch), y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e2268",
   "metadata": {},
   "source": [
    "### 2.2 用相对底层的nn.functional重写\n",
    "- <font color=blue>**用很底层的function构件，只是不用手动实现act和loss func**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc5ce74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:49.631723Z",
     "start_time": "2024-08-07T12:18:49.624257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6648, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 参数初始化\n",
    "torch.manual_seed(231)\n",
    "weights = torch.randn(784, 10)    # random init\n",
    "weights /= math.sqrt(784)         # Xaiver init\n",
    "weights.requires_grad_()          # init之后再set gradient，因为init不用求梯度\n",
    "\n",
    "def model(x_batch):\n",
    "    return x_batch @ weights + bias\n",
    "\n",
    "loss_func = F.cross_entropy # loss function用nn.functional\n",
    "print(loss_func(model(x_batch), y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b08f7dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:50.096000Z",
     "start_time": "2024-08-07T12:18:49.632826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 get loss:0.3966763913631439\n",
      "epoch1 get loss:0.307575523853302\n",
      "tensor(0.0814, grad_fn=<NllLossBackward0>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((N - 1) // batch_size + 1):\n",
    "        start_i = i * batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        x_batch = x_train[start_i:end_i]\n",
    "        y_batch = y_train[start_i:end_i]\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_func(pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "    print(f'epoch{epoch} get loss:{loss.item()}')\n",
    "\n",
    "print(loss_func(model(x_batch), y_batch), accuracy(model(x_batch), y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633b3a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T06:22:26.570282Z",
     "start_time": "2023-10-12T06:22:26.557834Z"
    }
   },
   "source": [
    "### 2.3 用nn.Module和nn.Parameter重写\n",
    "- <font color=blue>**自定义nn module**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "083b6da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:50.102316Z",
     "start_time": "2024-08-07T12:18:50.097520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2686, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "class MnistLogistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(231)\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        return x_batch @ self.weights + self.bias\n",
    "\n",
    "model = MnistLogistic()\n",
    "loss_func = F.cross_entropy\n",
    "print(loss_func(model(x_batch), y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43d858c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:50.600971Z",
     "start_time": "2024-08-07T12:18:50.103641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0809, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# run a training loop\n",
    "lr = 0.5\n",
    "epoch = 2\n",
    "\n",
    "def fit(model, epoch, lr):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((N - 1) // batch_size + 1):\n",
    "            start_i = i * batch_size\n",
    "            end_i = start_i + batch_size\n",
    "            x_batch = x_train[start_i: end_i]\n",
    "            y_batch = y_train[start_i: end_i]\n",
    "            pred = model(x_batch)\n",
    "            loss = loss_func(pred, y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():  # nn.Module中的\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "fit(model, epoch, lr)\n",
    "print(loss_func(model(x_batch), y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c28125",
   "metadata": {},
   "source": [
    "### 2.4 在nn.Module中用nn中的layers\n",
    "- <font color=blue>**forward直接用nn自带的layer module**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f9506e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:50.612238Z",
     "start_time": "2024-08-07T12:18:50.602254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2879, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MnistLogistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        return self.lin(x_batch)\n",
    "\n",
    "model = MnistLogistic()\n",
    "\n",
    "print(loss_func(model(x_batch), y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdae5392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:51.067275Z",
     "start_time": "2024-08-07T12:18:50.613342Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0808, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# run a training loop\n",
    "lr = 0.5\n",
    "epoch = 2\n",
    "fit(model, epoch, lr)\n",
    "print(loss_func(model(x_batch), y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4624b442",
   "metadata": {},
   "source": [
    "### 2.5 用torch.optim\n",
    "- <font color=blue>**用opt.step()和opt.zero_grad()**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56af3216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:52.178760Z",
     "start_time": "2024-08-07T12:18:51.068562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3711, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0822, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "lr = 0.5\n",
    "epoch = 2\n",
    "\n",
    "model = MnistLogistic()\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# 一个batch\n",
    "print(loss_func(model(x_batch), y_batch))\n",
    "\n",
    "# run a trainning loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range((N - 1) // batch_size + 1):\n",
    "        start_i = i * batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        x_batch = x_train[start_i: end_i]\n",
    "        y_batch = y_train[start_i: end_i]\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_func(pred, y_batch)\n",
    "            \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(x_batch), y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65370ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-12T07:20:17.836445Z",
     "start_time": "2023-10-12T07:20:17.825065Z"
    }
   },
   "source": [
    "### 2.6 用Dataset\n",
    "- <font color=blue>**用TensorDataset是用来wrapping tensor的Dataset。提供了便于iterate, index, slice a tensor along its first dimension的方式。**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b2bf638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:52.182538Z",
     "start_time": "2024-08-07T12:18:52.180575Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3939351e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:52.701502Z",
     "start_time": "2024-08-07T12:18:52.183685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0823, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 用TensorDataset来wrap dataset\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "\n",
    "# 设置超参数\n",
    "lr = 0.5\n",
    "epoch = 2\n",
    "\n",
    "# 设置模型\n",
    "model = MnistLogistic()\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# run a trainning loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range((N - 1) // batch_size + 1):\n",
    "        start_i = i * batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        x_batch, y_batch = train_ds[start_i: end_i]  # 提取batch\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_func(pred, y_batch)\n",
    "            \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "print(loss_func(model(x_batch), y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab932ad3",
   "metadata": {},
   "source": [
    "### 2.7 用DataLoader\n",
    "- <font color=blue>DataLoader负责管理batch，主要是方便地iterate over batches，而不再用data[start_i: end_i]的方式手动提取数据</font>\n",
    "- 可以用任意Dataset构建DataLoader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eaab18f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:52.705379Z",
     "start_time": "2024-08-07T12:18:52.703033Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoader返回的是iterate\n",
    "train_dset = TensorDataset(x_train, y_train)\n",
    "train_dloader = DataLoader(train_dset, batch_size=batch_size, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "158d62f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:53.945549Z",
     "start_time": "2024-08-07T12:18:52.706622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0363, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "lr = 0.5\n",
    "epoch = 2\n",
    "\n",
    "# 设置模型\n",
    "model = MnistLogistic()\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# run a trainning loop\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, y_batch in train_dloader:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_func(pred, y_batch)\n",
    "            \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "print(loss_func(model(x_batch), y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b674ff9",
   "metadata": {},
   "source": [
    "### 2.8 增加validation\n",
    "- <font color=blue>**validation不做shuffle，batchsize可以比training时大，因为这时候不计算梯度，占用的memory少。**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ddb390a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:53.950023Z",
     "start_time": "2024-08-07T12:18:53.947317Z"
    }
   },
   "outputs": [],
   "source": [
    "# 准备trainning和validation data\n",
    "train_dset = TensorDataset(x_train, y_train)\n",
    "train_dloader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dset = TensorDataset(x_val, y_val)\n",
    "val_dloader = DataLoader(val_dset, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "182d64e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:55.371226Z",
     "start_time": "2024-08-07T12:18:53.951070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3069)\n",
      "tensor(0.4276)\n"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "lr = 0.5\n",
    "epoch = 2\n",
    "\n",
    "# 设置模型\n",
    "model = MnistLogistic()\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# run a trainning loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for x_batch, y_batch in train_dloader:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_func(pred, y_batch)\n",
    "            \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(x_batch), y_batch) for x_batch, y_batch in val_dloader)\n",
    "\n",
    "    print(valid_loss / len(val_dloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff6d63",
   "metadata": {},
   "source": [
    "### 2.9 自定义简化流程的wrapper\n",
    "- <font color=blue>**简化get_data和fit**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4b6628d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:55.375324Z",
     "start_time": "2024-08-07T12:18:55.372626Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb78c3b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:55.386841Z",
     "start_time": "2024-08-07T12:18:55.376299Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf0c13ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:55.392690Z",
     "start_time": "2024-08-07T12:18:55.387851Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(train_dset, val_dset, batch_size):\n",
    "    return (\n",
    "        DataLoader(train_dset, batch_size=batch_size, shuffle=True), \n",
    "        DataLoader(val_dset, batch_size=batch_size*2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f56fa1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:18:56.785447Z",
     "start_time": "2024-08-07T12:18:55.393882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.36141845049858096\n",
      "1 0.29648065532445905\n"
     ]
    }
   ],
   "source": [
    "train_dloader, val_dloader = get_data(train_dset, val_dset, batch_size)\n",
    "model = MnistLogistic()\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "fit(epochs, model, loss_func, opt, train_dloader, val_dloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac67ea",
   "metadata": {},
   "source": [
    "### 2.10 可以用nn.Module或者nn.sequential建模型\n",
    "- <font color=blue>**后面开始改用CNN模型**</font>: $(cnn-relu) * 3 \\rightarrow average_pooling$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f7816",
   "metadata": {},
   "source": [
    "#### 直接用nn.Module调用各类底层nn.Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04e9e20a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:19:01.641052Z",
     "start_time": "2024-08-07T12:18:56.788766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3801825850486755\n",
      "1 0.25237053772211077\n"
     ]
    }
   ],
   "source": [
    "class MnistCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # init中要给有parameters的function做初始化\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        x_batch = x_batch.view(-1, 1, 28, 28)\n",
    "        x_batch = F.relu(self.conv1(x_batch))\n",
    "        x_batch = F.relu(self.conv2(x_batch))\n",
    "        x_batch = F.relu(self.conv3(x_batch))\n",
    "        x_batch = F.avg_pool2d(x_batch, 4)\n",
    "        return x_batch.view(-1, x_batch.size(1))\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "model = MnistCNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dloader, val_dloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd066c",
   "metadata": {},
   "source": [
    "#### 用nn.sequential\n",
    "- 如果sequencial中要用的layer没有现成的，可以自定义：\n",
    "  - step1：先将layer的功能表达成function\n",
    "  - step2：将function打包成nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9ae7861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:19:01.645420Z",
     "start_time": "2024-08-07T12:19:01.642544Z"
    }
   },
   "outputs": [],
   "source": [
    "#  自定义一个起数据预处理作用的view layer\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "    \n",
    "def preprocess_view(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9a47b8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:19:06.382840Z",
     "start_time": "2024-08-07T12:19:01.646922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.391561469244957\n",
      "1 0.2692460147976875\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess_view), \n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), \n",
    "    nn.ReLU(), \n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(), \n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), \n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dloader, val_dloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb702c",
   "metadata": {},
   "source": [
    "### 2.11 wrapping DataLoader\n",
    "- <font color=blue>**把数据预处理打包到自定义的DataLoader中，同时修改model，让model可以处理任意长宽的channel=1的图片**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "113ac20d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:19:06.389642Z",
     "start_time": "2024-08-07T12:19:06.384829Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dloader, val_dloader = get_data(train_dset, val_dset, batch_size)\n",
    "train_dloader = WrappedDataLoader(train_dloader, preprocess)\n",
    "val_dloader = WrappedDataLoader(val_dloader, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfbeab4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:19:11.247271Z",
     "start_time": "2024-08-07T12:19:06.391052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5283738411188126\n",
      "1 0.2449990958869457\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dloader, val_dloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbc47f4",
   "metadata": {},
   "source": [
    "### 2.12 用GPU做运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c0c22fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:19:15.871135Z",
     "start_time": "2024-08-07T12:19:11.249116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.22688052369356154\n",
      "1 0.18584200881123542\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = torch.device(\"cuda\") \n",
    "else:\n",
    "    dev = torch.device(\"cpu\")\n",
    "\n",
    "# 数据预处理的时候将tensor移到gpu上\n",
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "\n",
    "train_dloader, val_dloader = get_data(train_dset, val_dset, batch_size)\n",
    "train_dloader = WrappedDataLoader(train_dloader, preprocess)\n",
    "val_dloader = WrappedDataLoader(val_dloader, preprocess)\n",
    "\n",
    "# 将model定义在gpu上\n",
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dloader, val_dloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
