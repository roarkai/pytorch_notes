{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f560c37",
   "metadata": {},
   "source": [
    "# Examples1_learning_with_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec597b",
   "metadata": {},
   "source": [
    "## 1. 第一次尝试：手动forward and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73043d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T06:42:58.183872Z",
     "start_time": "2023-10-13T06:42:56.977798Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661e7b85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T06:42:58.396151Z",
     "start_time": "2023-10-13T06:42:58.186038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: 1477.5787353515625\n",
      "299: 722.48388671875\n",
      "499: 356.6500549316406\n",
      "699: 178.82565307617188\n",
      "899: 92.1266098022461\n",
      "1099: 49.73776626586914\n",
      "1299: 28.959972381591797\n",
      "1499: 18.751493453979492\n",
      "1699: 13.72527027130127\n",
      "1899: 11.245824813842773\n",
      "Result: y = -0.04202675819396973 + 0.8455353379249573x + 0.007250312250107527x^2 + -0.09173650294542313x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "# print(x**3)\n",
    "\n",
    "# randomly initialize weights\n",
    "torch.manual_seed(231)\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "# print(a, b, c, d)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # forward\n",
    "    y_pred = a + b * x + c * (x ** 2) + d * (x ** 3)\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 200 == 99:\n",
    "        print(f\"{t}: {loss}\")\n",
    "    \n",
    "    # backward\n",
    "    grad_y_pred = 2 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "  \n",
    "    # update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "    \n",
    "print(f'Result: y = {a.item()} + {b.item()}x + {c.item()}x^2 + {d.item()}x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe219f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T12:58:09.786470Z",
     "start_time": "2023-10-11T12:58:09.549490Z"
    }
   },
   "source": [
    "## 2. autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24984b1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T06:42:58.810145Z",
     "start_time": "2023-10-13T06:42:58.397755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: 1477.5787353515625\n",
      "299: 722.48388671875\n",
      "499: 356.6500549316406\n",
      "699: 178.82565307617188\n",
      "899: 92.1266098022461\n",
      "1099: 49.73776626586914\n",
      "1299: 28.959972381591797\n",
      "1499: 18.751493453979492\n",
      "1699: 13.72527027130127\n",
      "1899: 11.245824813842773\n",
      "Result: y = -0.04202675819396973 + 0.8455353379249573x + 0.007250312250107527x^2 + -0.09173650294542313x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# randomly initialize weights\n",
    "torch.manual_seed(231)\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "# print(a, b, c, d)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # forward\n",
    "    y_pred = a + b * x + c * (x ** 2) + d * (x ** 3)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 200 == 99:\n",
    "        print(f\"{t}: {loss.item()}\")\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "  \n",
    "    # 手动update weights，这里weights的运算不用计grad，所以要暂停autograd\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "    \n",
    "    # 每次迭代后要将grad置零，以免下次迭代时从非0值开始累加各个path的gradient\n",
    "        a.grad = b.grad = c.grad = d.grad = None\n",
    "    \n",
    "print(f'Result: y = {a.item()} + {b.item()}x + {c.item()}x^2 + {d.item()}x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1591655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T13:33:35.342002Z",
     "start_time": "2023-10-11T13:33:33.638102Z"
    }
   },
   "source": [
    "## 3. 自定义autograd function\n",
    "1. each primitive autograd operator包含了两个function：forward function 用input tensor计算output tensor；backward function收到output Tensors 相当于某个scalar value的梯度，然后用这个梯度来计算input tensor相对于该scalar value的梯度\n",
    "2. 可以通过定义torch.autograd.Function的子类的方式来自定义autograd operator，实现forward and backward functions。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4b3ce",
   "metadata": {},
   "source": [
    "这里取$y=a + b*P_3(c + d*x)$来代替前面的$y=a + b*x^2 + c*x^3$。$P_3(x)=\\frac{1}{2}(5x^3-3x)$是Legendre polynomial。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc74ff26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T06:42:59.323108Z",
     "start_time": "2023-10-13T06:42:58.813198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: 209.95834350585938\n",
      "299: 100.70249938964844\n",
      "499: 50.978511810302734\n",
      "699: 28.206867218017578\n",
      "899: 17.7457275390625\n",
      "1099: 12.93176555633545\n",
      "1299: 10.71425724029541\n",
      "1499: 9.692105293273926\n",
      "1699: 9.220745086669922\n",
      "1899: 9.003361701965332\n",
      "Result: y = -6.71270206087371e-10 + -2.208526849746704x + -3.392665037793563e-10x^2 + 0.2554861009120941x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        ctx is a context object.可以用来stash information for backward。\n",
    "        可以用ctx.save_for_backward method来cache arbitrary objects for use in the backward.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 这里initialize weights刻意放在了正确值附近\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "# print(a, b, c, d)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # autograd.Function要先apply\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "    \n",
    "    # forward\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 200 == 99:\n",
    "        print(f\"{t}: {loss.item()}\")\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "  \n",
    "    # 手动update weights，这里weights的运算不用计grad，所以要暂停autograd\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "    \n",
    "    # 每次迭代后要将grad置零，以免下次迭代时从非0值开始累加各个path的gradient\n",
    "        a.grad = b.grad = c.grad = d.grad = None\n",
    "    \n",
    "print(f'Result: y = {a.item()} + {b.item()}x + {c.item()}x^2 + {d.item()}x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab621e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T14:34:44.605362Z",
     "start_time": "2023-10-11T14:34:44.600535Z"
    }
   },
   "source": [
    "## 4. 用nn package中的module\n",
    "1. 直接用autograd对于构建复杂的NN而言，还是太底层。使用nn module直接以layer的形式来安排layer更方便，因为抽象层次更高。\n",
    "2. nn package中定义了很多modules，这些modules实现了NN中的layers, loss functions等。module接收input tensor，计算output tensor。同时也会存放weights等internal state。\n",
    "\n",
    "· <font color=blue>这里以linear layer为例，所以拟合的函数改为$y = f(x, x^2, x^3)$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b444143f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T06:42:59.698341Z",
     "start_time": "2023-10-13T06:42:59.324673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: 413.55877685546875\n",
      "299: 194.99159240722656\n",
      "499: 94.82963562011719\n",
      "699: 48.7336311340332\n",
      "899: 27.42612648010254\n",
      "1099: 17.532577514648438\n",
      "1299: 12.917770385742188\n",
      "1499: 10.755277633666992\n",
      "1699: 9.73727798461914\n",
      "1899: 9.255874633789062\n",
      "Result: y = -0.015080885030329227 +                     0.8469998240470886 x +                     0.0026017031632363796 x^2 +                     -0.09194481372833252 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# x.unsqueeze(-1) has shape (2000, 1), and p has shape (3,)\n",
    "# broadcasting semantics will apply, xx has shape (2000, 3) \n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# define model as a sequence of layers\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1), # output of linear layer has shape (200, 1)\n",
    "    torch.nn.Flatten(0, 1) # flatens the linear output to 1D to match`y`.\n",
    ")\n",
    "\n",
    "# define loss funtion\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    \n",
    "    # forward: Module objects override the __call__ operator\n",
    "    # so you can call them like functions.\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 200 == 99:\n",
    "        print(f\"{t}: {loss.item()}\")\n",
    "    \n",
    "    # BP前要将weights置零\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "  \n",
    "    # 手动update weights，这里weights的运算不用计grad，所以要暂停autograd\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    \n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + \\\n",
    "                    {linear_layer.weight[:, 0].item()} x + \\\n",
    "                    {linear_layer.weight[:, 1].item()} x^2 + \\\n",
    "                    {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889641c4",
   "metadata": {},
   "source": [
    "## 5. 使用optim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f821cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T06:43:00.180644Z",
     "start_time": "2023-10-13T06:42:59.700099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: 237.22927856445312\n",
      "299: 32.75457000732422\n",
      "499: 18.58349609375\n",
      "699: 10.452796936035156\n",
      "899: 8.833868980407715\n",
      "1099: 8.817211151123047\n",
      "1299: 8.822708129882812\n",
      "1499: 8.891637802124023\n",
      "1699: 8.916147232055664\n",
      "1899: 8.917805671691895\n",
      "Result: y = -0.0005856486386619508 +                     0.8562260866165161 x +                     -0.0005856484640389681 x^2 +                     -0.0938451737165451 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# x.unsqueeze(-1) has shape (2000, 1), and p has shape (3,)\n",
    "# broadcasting semantics will apply, xx has shape (2000, 3) \n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# define model as a sequence of layers\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1), # output of linear layer has shape (200, 1)\n",
    "    torch.nn.Flatten(0, 1) # flatens the linear output to 1D to match`y`.\n",
    ")\n",
    "\n",
    "# define loss funtion\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 定义优化器，第一个argument告诉优化器要update的参数是哪些\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    \n",
    "    # forward: Module objects override the __call__ operator\n",
    "    # so you can call them like functions.\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 200 == 99:\n",
    "        print(f\"{t}: {loss.item()}\")\n",
    "    \n",
    "    # BP前要将weights置零，用了优化器后，使用优化器来置零\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "  \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + \\\n",
    "                    {linear_layer.weight[:, 0].item()} x + \\\n",
    "                    {linear_layer.weight[:, 1].item()} x^2 + \\\n",
    "                    {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3dec39",
   "metadata": {},
   "source": [
    "## 6. 自定义nn module\n",
    "· subclassing nn.Module and defining a forward which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca5d10c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T06:43:00.625538Z",
     "start_time": "2023-10-13T06:43:00.182168Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1036.6336669921875\n",
      "199 690.2674560546875\n",
      "299 460.70635986328125\n",
      "399 308.53466796875\n",
      "499 207.6463165283203\n",
      "599 140.74659729003906\n",
      "699 96.37659454345703\n",
      "799 66.94302368164062\n",
      "899 47.413673400878906\n",
      "999 34.452789306640625\n",
      "1099 25.849281311035156\n",
      "1199 20.136619567871094\n",
      "1299 16.342504501342773\n",
      "1399 13.82181167602539\n",
      "1499 12.146681785583496\n",
      "1599 11.033086776733398\n",
      "1699 10.29256534576416\n",
      "1799 9.799927711486816\n",
      "1899 9.472057342529297\n",
      "1999 9.253796577453613\n",
      "Result: y = 0.007822850719094276 + 0.8377413749694824 x + -0.0013495712773874402 x^2 + -0.09062787890434265 x^3\n"
     ]
    }
   ],
   "source": [
    "## implement the third order polynomial as a custom Module\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 注意这里的初始化方式\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = Polynomial3()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters (defined \n",
    "# with torch.nn.Parameter) which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72933a42",
   "metadata": {},
   "source": [
    "## 7. dynamic graph: control flow and weight sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a27f086e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T06:43:09.640769Z",
     "start_time": "2023-10-13T06:43:00.627280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 2335.856689453125\n",
      "3999 1150.7891845703125\n",
      "5999 473.41619873046875\n",
      "7999 230.9722900390625\n",
      "9999 107.4931411743164\n",
      "11999 50.93584442138672\n",
      "13999 27.7421932220459\n",
      "15999 17.346866607666016\n",
      "17999 12.601629257202148\n",
      "19999 10.762849807739258\n",
      "21999 9.551115036010742\n",
      "23999 9.021665573120117\n",
      "25999 8.971240043640137\n",
      "27999 8.670414924621582\n",
      "29999 8.902894973754883\n",
      "Result: y = -0.0016190327005460858 + 0.852457582950592 x + -0.0002379447250859812 x^2 + -0.09315378218889236 x^3                + 0.00012578832684084773 x^4 ? + 0.00012578832684084773 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "## implement a custom Module,\n",
    "#  第4，5次式可能有可能没有，且共享参数\n",
    "import random\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 注意这里的初始化方式\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(())) # 第4、5次式共享的参数\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        # 随机决定第4、5次式是否存在\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 \\\n",
    "               + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters (defined \n",
    "# with torch.nn.Parameter) which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
