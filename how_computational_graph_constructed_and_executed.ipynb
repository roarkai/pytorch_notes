{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57bd4164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T08:01:37.612082Z",
     "start_time": "2024-08-09T08:01:37.608774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/roark/.local/lib/python3.10/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchviz\n",
    "\n",
    "print(torch.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828b863",
   "metadata": {},
   "source": [
    "# how computational graph is constructed and executed\n",
    "参考：\n",
    "1. How Computational Graphs are Constructed in PyTorch https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/\n",
    "2. How Computational Graphs are Executed in PyTorch\n",
    "https://pytorch.org/blog/how-computational-graphs-are-executed-in-pytorch/\n",
    "3. pytorch internals http://blog.ezyang.com/category/pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b614b1",
   "metadata": {},
   "source": [
    "## I. pytorch的基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92981599",
   "metadata": {},
   "source": [
    "### I.1 pytorch源码核心文件概况\n",
    "<font color=blue>**四个最重要的文件夹：**</font>\n",
    "1. **torch**: 用户import和使用的python modules\n",
    "2. **torch/csrc/：**pytorch前端功能的c++ code实现，主要是包括：\n",
    "   1. python代码与c++代码之间转换的binding code。\n",
    "   2. autograd engine在其中的子文件夹autograd中。\n",
    "   3. JIT compiler在其中的子文件夹jit中。\n",
    "   4. c++前端代码\n",
    "3. **aten/：**‘A tensor library’的简称，用c++写的。是fundational tensor operation library。基本所有tensor的其他操作都是在aten提供的基础上实现的。 \n",
    "   - kernel proper在aten中\n",
    "4. **c10/：**'Caffe2'和'A Ten'的简称。是core abstractions of pytorch，包括Tensor和Storage data structure的具体实现。\n",
    "\n",
    "<font color=blue>**用户代码的工作方式：**</font>\n",
    "1. 用户用torch中提供的modules写代码\n",
    "2. pytorch的编译器会将用户代码转化成用户代码的c++实现。\n",
    "   - <font color=red>注:这部分代码是动态生成的，不是调用提前写好的package或者lib。</font>\n",
    "   - 这部分代码中包含了torch/csrc/的binding code，autograd engine等内容，用于明确用户需要的功能如何用c++实现。\n",
    "3. 上述c++代码调用torch/csrc/、aten和c10中的内容完成计算。\n",
    "   - aten中的operation lib\n",
    "   - c10中的data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f90b25",
   "metadata": {},
   "source": [
    "### I.2 tensor相关的数据结构：tensor, variable和storage\n",
    "#### 1. Storage\n",
    "   - storage是tensor的underlying data buffer。其数据结构定义为struct Storage，只有基础data相关的信息，如：data_ptr, data type, byte_size, device等，没有view需要的stride信息和autograd需要的autoMeta信息。\n",
    "   - 由于storage struct不含有stride信息，因此可以实现同一个data对应tensor的不同view。\n",
    "   - Storage的定义在<font color=green>'c10/core/StorageImpl.h'</font>文件中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd296d2d",
   "metadata": {},
   "source": [
    "#### 2. Variable：最初Variable wraps tensor\n",
    "   - 过去为了完成autograd，对tensor和function都做了wrap。Variable wraps tensor，这里wrapper的主要工作是加上了tensor参加autograd时需要的信息AutogradMeta。\n",
    "     - 所以，<font color=blue>过去一般把tensor理解成不需要requires_grad的Variable，此时就不需要对它们apply autograd machinery了。</font><font color=red>现在Variable也改为定义成了tensor。未来pytorch团队会取消Variable，把现在Variable实现的功能都在tensor中直接实现。</font>\n",
    "   - Variable以c++的形式定义在<font color=green>'torch/csrc/autograd/variable.h'</font>文件中。这里定义了struct AutogradMeta类型，主要包括的信息有：\n",
    "     - grad_\n",
    "     - 指向Node类型的ptr：grad_fn_\n",
    "     - 指向Node类型的ptr：grad_accumulator_\n",
    "   - 提供的常用接口：requires_grad(), grad()\n",
    "- <font color=orange>虽然现在varible和tensor是一样的。但为了方便理解，还是可以将Variable简单理解成：Variable可以实现tensor能实现的所有ops，并且能interact with autograd machinery.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02043973",
   "metadata": {},
   "source": [
    "#### 3. Tensor\n",
    "   - tensor的定义在<font color=green>'c10/core/TensorImpl.h'</font>文件中。其中主要的信息包括：\n",
    "     - 一个指向storage struct的pointer，在这个storage struct中还有一个指向底层data的pointer\n",
    "     - metadata：view specifical metadata，包括size, offset和strides等信息\n",
    "   - tensor的定义提供给用户的接口主要有两类：\n",
    "     1. <font color=blue>**查询tensor attribute的接口**</font>: layout(), device(), dtype(), stride(), is_continuous(), is_sparse(), is_cpu(), is_cuda(), etc.\n",
    "     2. <font color=blue>**autograd interface**</font>: set_requires_grad(), requires_grad(), grad(), is_neg(),etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80cbb98",
   "metadata": {},
   "source": [
    "## II. Computation Graph是如何构建的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5da08a",
   "metadata": {},
   "source": [
    "### II.1 autograd相关的源码文件\n",
    "三个关键文件夹：\n",
    "1. <font color=blue>**tools/autograd：**</font>其中包括以下内容：\n",
    "   1. 文档derivatives.yaml：定义了tensor基本ops的梯度计算方法，也就是这些function的vjp()。\n",
    "   2. 一个名为templates的文件夹和一些python scripts。他们在building time期间，按照derivatives.yaml中对应函数的定义，生成完成对应computation需要的c++ code。\n",
    "      - 另外，scripts还负责为aten中的functions生成wrapper，用它们wrapped之后的版本来构建计算图。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259a838",
   "metadata": {},
   "source": [
    "2. <font color=blue>**torch/autograd：**</font>用户用pytorch的autograd功能的接口都在这个文件中。其中主要有以下几个python script：\n",
    "   - <font color=norange>function.py：</font>这里定义了class torch.autograd.Function，所有pytorch中的differentiable function都是通过继承这个class定义的。\n",
    "     - 另外以function为base class，又定义了InplaceFunction和NestedIOFunction两个class\n",
    "   - <font color=norange>functional.py：</font>\"vjp\", \"jvp\", \"jacobian\", \"hessian\", \"hvp\", \"vhp\"这6个函数的实现。\n",
    "   - <font color=norange>grad_mode.py：</font>提供了用来set grad mode的几个function\n",
    "   - <font color=norange>grad_check.py：</font>gradcheck()和gradgradcheck()两个函数的实现\n",
    "   - 另外还有实现autograd profiler, anomaly detection等功能的python scripts。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9f061",
   "metadata": {},
   "source": [
    "3. <font color=blue>**torch/csrc/autograd：**</font>这里全部是c++ code。所有computation graph生成和执行相关的代码都在这个文件夹中。<font color=green>主要有三类文件，分别implement：autograd engine, metadata storage和其他需要的components。</font>另外还有一些python_开头的文件，用来允许autograd engine使用python objects。一些重要的文档包括：\n",
    "   - <font color=norange>variable.h：</font>定义class Variable，详见前文。\n",
    "   - <font color=norange>edge.h：</font>定义struct Edge，用来表示a particular input of a function.\n",
    "   - <font color=norange>function.h:</font> 定义class Node, 用Node定义的struct TraceableFunction\n",
    "   - <font color=norange>engine.h：</font>定义了struct Engine，用于管理整个梯度计算的工作进程。\n",
    "   - <font color=norange>autograd.h: </font>定义了backward()和grad()两个函数，他们的功能都是计算梯度，区别是:backward把梯度计算的结构直接accumulate到input tensor上；grad()会将计算出来的梯度作为函数返回值返回，不会改变对应input tensor的grad属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b0e5f",
   "metadata": {},
   "source": [
    "### II.2 计算图的三个数据结构\n",
    "#### Graph：只是抽象数据结构，没有具体的object\n",
    "- Graph在pytorch中是一个抽象概念，并没有一个Graph对应的数据结构，但是有Node和Edge的数据结构定义。Node obj由Edge obj连接起来构成了Graph这个抽象的数据结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c38ce1f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T13:06:48.323980Z",
     "start_time": "2024-08-09T13:06:48.319555Z"
    }
   },
   "outputs": [],
   "source": [
    "## 3个operation function构成的计算图示意：Forward chain与Backward chain的对应关系\n",
    "#\n",
    "#    in2    |||||||||||||   out1=in2   |||||||||||||   out2=in3   ||||||||||||| out3\n",
    "#  -------> |    f_1    | -----------> |    f_2    | -----------> |    f_3    |-----> \n",
    "#           |||||||||||||              |||||||||||||              |||||||||||||\n",
    "#               |                          |                         |\n",
    "#               |out1.grad_fn              |out1.grad_fn             |out3.grad_fn\n",
    "#               V                          V                         V\n",
    "#           |||||||||||||              |||||||||||||              |||||||||||||\n",
    "#   outf1B  | Node:     | inf1B=outf2B | Node:     | inf2B=outf3B | Node:     |  inf3B\n",
    "#  <------- |f1_Backward| <----------- |f2_Backward| <----------- |f3_Backward| <-----\n",
    "#           |||||||||||||     dout1    |||||||||||||     dout2    |||||||||||||  dout3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcce9be",
   "metadata": {},
   "source": [
    "#### Node obj\n",
    "- 定义在<font color=orange>**torch/csrc/autograd/function.h**</font>中。定义为struct Node，但是继承了c++标准库的Node类，因此是一个class。\n",
    "- <font coloblue>所有autograd machinery中的函数都继承了这个class</font>\n",
    "- 定义中包括的信息: \n",
    "  1. <font color=blue>**override of the operator()**</font>，在该override函数定义中，通过apply(inputs)来perform call to the actual function。\n",
    "     - 有了这里operator()的重定义，Node类型的函数f就可以用f()来invoke函数自己定义的apply(inputs)。\n",
    "     - 但operator()的定义中，apply()是virtual function，没有给实际定义，所以autograd machinery中的函数都要override apply()。<font color=deeppink>building time期间，tools/autograd中的python scripts文件自动生成grad_fn指向的Backward对象的c++ code，其中就包括override apply()函数。</font>\n",
    "  2. Nodes通过他们的next_edge接口而连在一起，实现Graph connectivity。Node定义中除了operator()的override之外，就是维护它的list of next edges。实际上Node本身在实例化的时候用的arguments就是edge_list()。\n",
    "     - 定义中提供的相关接口有：\n",
    "       - num_inputs()和num_outputs()：分别返回Nodes的inputs和outputs数量。注意，'grad_fn'指向的Node的inputs是原函数的outputs。 \n",
    "       - add_next_edge(edge)：用来给Nodes增加一个edge\n",
    "       - get_next_edge(index)：用来retrieve Node的next_edge信息\n",
    "       - next_edges(): 用来遍历所有的next_edges\n",
    "       - set_next_edge(index, edge): \n",
    "- 例子：如下图\n",
    "  - func3的output的grad_fn指向Node1\n",
    "  - func3有两个inputs，分别是func1和func2的output，因此在backward chain中：\n",
    "    - func3对应的Node1就有两个grad_outputs，分别是Node1和Node2的grad_inputs\n",
    "    - Node1的edge_list()中有两个next_edge，分别指向Node1和Node2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d45196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T14:12:52.128860Z",
     "start_time": "2024-08-09T14:12:52.124663Z"
    }
   },
   "source": [
    "              Node2    <---------------------- next_edge1 ------------------------  \n",
    "           (bacward)             ptr                                             |\n",
    "               |                                                                 |\n",
    "              func1 :  output1 --------                                          |\n",
    "            (forward)                  \\                                         |\n",
    "                                  input,number=1                                 |\n",
    "                                          \\                                      |\n",
    "                                           --> func3 -- output3 -- grad_fn --> Node1 \n",
    "                                          /                                  (bacward) \n",
    "                                  input,number=2                                 |\n",
    "                                        /                                        |\n",
    "              func2 :  output2 ---------                                         |\n",
    "            (forward)                                                            |\n",
    "                |                 ptr                                            |\n",
    "              Node3    <---------------------- next_edge2 ------------------------  \n",
    "           (backward)\n",
    "           \n",
    "                            <==================   ==================> \n",
    "                             backward chain方向    forward chain方向"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556c787",
   "metadata": {},
   "source": [
    "#### Edge obj\n",
    "- 定义在<font color=orange>**torch/csrc/autograd/edge.h**</font>中。定义为struct Edge.\n",
    "- 实例化的argument是Node pointer和input_nr，所以Edge可以用(Node, input_nr)pair来表示。\n",
    "  - Node：direct edge的pointer所指向的Node。这个Nodey也是某个grad_fn指向的obj。这个grad_fn对应的forward function的output是backward chain中前序Node的input。\n",
    "  - input_nr：Node对应forward func的output作为bacward chain中前序Node的forward func的input时的序号。\n",
    "  - 还有几个Edge定义的ops: ==,!=,is_valid()  \n",
    "- Edge用pointer的方式把backward chain中涉及的grad_fn Nodes顺序连接起来。\n",
    "- 例子：如上图：\n",
    "    - func3对应的Node1有两个next_edge，分别指向Node1和Node2\n",
    "    - 这两个edge有各自相应的input number序号，即input_nr的取值     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d71439",
   "metadata": {},
   "source": [
    "### II.3 计算图的构造过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd7222",
   "metadata": {},
   "source": [
    "<font color=blue>**step1：定义一个requires_grad-True**</font>\n",
    "- 之后c10中的allocator会allocate一个AutogradMeta对象来hold graph信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29748e95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T07:41:25.986442Z",
     "start_time": "2024-08-09T07:41:25.983164Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([0.5, 0.5], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bd64c",
   "metadata": {},
   "source": [
    "- <font color=orange>struct AutogradMeta定义在torch/csrc/autograd/variable.h中，其中主要信息包括：\n",
    "  1. tensor的梯度\n",
    "  2. 一个指针grad_fn指向一个函数，后面engine会call这个function来计算梯度；\n",
    "  3. 一个梯度累积对象\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec5449d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T07:41:25.999336Z",
     "start_time": "2024-08-09T07:41:25.987969Z"
    }
   },
   "outputs": [],
   "source": [
    "## struct TORCH_API AutogradMeta的主要内容\n",
    "\n",
    "# struct TORCH_API AutogradMeta : Public c10::AutogradMetaInerface{\n",
    "#     std::string name_;\n",
    "    \n",
    "#     Variable grad_;\n",
    "#     std::shared_ptr<Node> grad_fn_;\n",
    "#     std::weak_ptr<Node> grad_accumulator_;\n",
    "#     // other contents\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5a998",
   "metadata": {},
   "source": [
    "<font color=blue>**step2：让tensor作为input，call a differentiable function**</font>\n",
    "- 此时，tools/autograd中的python scripts文件会为aten中的乘法函数生成wrapper，这个wrapper就是为了提供除了函数自身运算之外，autograd machine需要的其他信息。\n",
    "  - 其中包括按照derivatives.yaml中定义的规则生成MulBackward的c++ code，实际就是backward的apply()函数。\n",
    "- 之后，function的**output tensor**的AutogradMeta中的grad_fn会指向function的backward method。\n",
    "  - 如：下面v的grad_fn=\\<MulBackward0\\>\n",
    "- 每个backward对象都继承了TraceableFunction class，而TraceableFunciton又继承了Node，只另外设置了enable tracing的性质。所以grad_fn指向的backward对象也是一个Node obj。<font color=green>**当执行的计算不止简单的一个operation的时候，他们的backward obj就构成Graph上的Nodes。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e28ff2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228c3baa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T07:51:46.553184Z",
     "start_time": "2024-08-09T07:51:46.523997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2500, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = x[0] * x[1]\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c34c3",
   "metadata": {},
   "source": [
    "tensor能做的基础运算都定义在Aten中。他们"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c37657",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c841be5c",
   "metadata": {},
   "source": [
    "## III. 计算图是如何执行的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257924ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9047de82",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "884ddb51",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
