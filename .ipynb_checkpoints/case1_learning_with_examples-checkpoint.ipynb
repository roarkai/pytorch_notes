{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f560c37",
   "metadata": {},
   "source": [
    "# Examples1_learning_with_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec597b",
   "metadata": {},
   "source": [
    "## 1. 手动forward and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73043d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:04.852526Z",
     "start_time": "2024-08-07T12:20:03.515429Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277d6b74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:04.857542Z",
     "start_time": "2024-08-07T12:20:04.854803Z"
    }
   },
   "outputs": [],
   "source": [
    "# 配置\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1b22dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:05.072371Z",
     "start_time": "2024-08-07T12:20:04.858921Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用多项式拟合sin函数\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a102e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:05.200846Z",
     "start_time": "2024-08-07T12:20:05.074648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6443, device='cuda:0'),\n",
       " tensor(-0.7900, device='cuda:0'),\n",
       " tensor(1.4691, device='cuda:0'),\n",
       " tensor(-0.6675, device='cuda:0'),\n",
       " torch.Size([]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数初始化\n",
    "torch.manual_seed(231)\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "a, b, c, d, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2d9cd6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:05.882382Z",
     "start_time": "2024-08-07T12:20:05.202861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303990.4375\n",
      "1905.811279296875\n",
      "889.9012451171875\n",
      "419.92266845703125\n",
      "201.5188446044922\n",
      "99.55885314941406\n",
      "51.74040222167969\n",
      "29.211084365844727\n",
      "18.548545837402344\n",
      "13.479888916015625\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-6\n",
    "\n",
    "for i in range(2000):\n",
    "    y_pred = a + b*x + c*(x**2) + d*(x**3)\n",
    "    loss = ((y_pred - y)**2).sum().item()\n",
    "    \n",
    "    # 打印training loss\n",
    "    if i % 200 == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    # 计算梯度\n",
    "    grad_y_pred = 2 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = grad_y_pred @ x\n",
    "    grad_c = grad_y_pred @ (x**2)\n",
    "    grad_d = grad_y_pred @ (x**3)\n",
    "    \n",
    "    # 参数更新\n",
    "    a -= lr * grad_a\n",
    "    b -= lr * grad_b\n",
    "    c -= lr * grad_c\n",
    "    d -= lr * grad_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe219f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T12:58:09.786470Z",
     "start_time": "2023-10-11T12:58:09.549490Z"
    }
   },
   "source": [
    "## 2. 用autograd package自动化BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b57dfa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:05.890736Z",
     "start_time": "2024-08-07T12:20:05.884334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数初始化时打开requires_grad\n",
    "torch.manual_seed(231)\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# 但x和y不用打开\n",
    "x.requires_grad, y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24984b1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:07.670696Z",
     "start_time": "2024-08-07T12:20:05.892233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: 2807.8134765625\n",
      "299: 1305.8782958984375\n",
      "499: 612.6148681640625\n",
      "699: 291.18499755859375\n",
      "899: 141.47540283203125\n",
      "1099: 71.42547607421875\n",
      "1299: 38.4979248046875\n",
      "1499: 22.949541091918945\n",
      "1699: 15.574642181396484\n",
      "1899: 12.061371803283691\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # FP\n",
    "    y_pred = a + b * x + c * (x ** 2) + d * (x ** 3)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    # 打印loss\n",
    "    if t % 200 == 99:\n",
    "        print(f\"{t}: {loss.item()}\")\n",
    "    \n",
    "    # 用autograd自动计算梯度\n",
    "    loss.backward()\n",
    "  \n",
    "    # 参数更新，这里weights的运算不用计grad，所以要暂停autograd\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "    \n",
    "    # 每次迭代后要将grad置零，以免下次迭代时从非0值开始累加各个path的gradient\n",
    "        a.grad = b.grad = c.grad = d.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1591655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T13:33:35.342002Z",
     "start_time": "2023-10-11T13:33:33.638102Z"
    }
   },
   "source": [
    "## 3. 自定义autograd function\n",
    "1. each primitive autograd operator包含了两个function：forward function 用input tensor计算output tensor；backward function收到output Tensors 相当于某个scalar value的梯度，然后用这个梯度来计算input tensor相对于该scalar value的梯度\n",
    "2. 可以通过定义torch.autograd.Function的子类的方式来自定义autograd operator，实现forward and backward functions。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4b3ce",
   "metadata": {},
   "source": [
    "这里取$y=a + b*P_3(c + d*x)$来代替前面的$y=a + b*x^2 + c*x^3$。$P_3(x)=\\frac{1}{2}(5x^3-3x)$是Legendre polynomial。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc74ff26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:10.013156Z",
     "start_time": "2024-08-07T12:20:07.673776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: 209.95834350585938\n",
      "299: 100.70249938964844\n",
      "499: 50.978511810302734\n",
      "699: 28.20686912536621\n",
      "899: 17.745729446411133\n",
      "1099: 12.931766510009766\n",
      "1299: 10.714248657226562\n",
      "1499: 9.692106246948242\n",
      "1699: 9.220745086669922\n",
      "1899: 9.003361701965332\n",
      "Result: y = 1.2777713782885503e-11 + -2.208526849746704x + -2.5764071431844116e-10x^2 + 0.2554861009120941x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        ctx is a context object.可以用来stash information for backward。\n",
    "        可以用ctx.save_for_backward method来cache arbitrary objects for use in the backward.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 这里initialize weights刻意放在了正确值附近\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "# print(a, b, c, d)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # autograd.Function要先apply\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "    \n",
    "    # forward\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 200 == 99:\n",
    "        print(f\"{t}: {loss.item()}\")\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "  \n",
    "    # 手动update weights，这里weights的运算不用计grad，所以要暂停autograd\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "    \n",
    "    # 每次迭代后要将grad置零，以免下次迭代时从非0值开始累加各个path的gradient\n",
    "        a.grad = b.grad = c.grad = d.grad = None\n",
    "    \n",
    "print(f'Result: y = {a.item()} + {b.item()}x + {c.item()}x^2 + {d.item()}x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab621e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T14:34:44.605362Z",
     "start_time": "2023-10-11T14:34:44.600535Z"
    }
   },
   "source": [
    "## 4. 用nn package中的module\n",
    "1. 直接用autograd对于构建复杂的NN而言，还是太底层。使用nn module直接以layer的形式来安排layer更方便，因为抽象层次更高。\n",
    "2. nn package中定义了很多modules，这些modules实现了NN中的layers, loss functions等。module接收input tensor，计算output tensor。同时也会存放weights等internal state。\n",
    "\n",
    "· <font color=blue>这里以linear layer为例，所以拟合的函数改为$y = f(x, x^2, x^3)$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b444143f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:11.190775Z",
     "start_time": "2024-08-07T12:20:10.016891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: 1481.5634765625\n",
      "299: 664.4503784179688\n",
      "499: 301.3392333984375\n",
      "699: 139.65023803710938\n",
      "899: 67.49002075195312\n",
      "1099: 35.20598220825195\n",
      "1299: 20.723323822021484\n",
      "1499: 14.207231521606445\n",
      "1699: 11.26616096496582\n",
      "1899: 9.934158325195312\n",
      "Result: y = 0.016986023634672165 +                     0.8350483775138855 x +                     -0.0029303710907697678 x^2 +                     -0.0902448296546936 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# x.unsqueeze(-1) has shape (2000, 1), and p has shape (3,)\n",
    "# broadcasting semantics will apply, xx has shape (2000, 3) \n",
    "p = torch.tensor([1, 2, 3], device=device)\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# define model as a sequence of layers\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1), # output of linear layer has shape (200, 1)\n",
    "    torch.nn.Flatten(0, 1) # 将input的dim0到dim1 flatten成1D，以便match`y`.\n",
    ")\n",
    "\n",
    "# 把model移动到cuda上\n",
    "model = model.to(device)\n",
    "\n",
    "# define loss funtion\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    \n",
    "    # forward: Module objects override the __call__ operator\n",
    "    # so you can call them like functions.\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 400 == 399:\n",
    "        print(f\"{t}: {loss.item()}\")\n",
    "    \n",
    "    # BP前要将weights置零\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "  \n",
    "    # 手动update weights，这里weights的运算不用计grad，所以要暂停autograd\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    \n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + \\\n",
    "                    {linear_layer.weight[:, 0].item()} x + \\\n",
    "                    {linear_layer.weight[:, 1].item()} x^2 + \\\n",
    "                    {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889641c4",
   "metadata": {},
   "source": [
    "## 5. 使用optim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57f821cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:13.320243Z",
     "start_time": "2024-08-07T12:20:11.195573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: 2154.765380859375\n",
      "299: 1385.784423828125\n",
      "499: 933.8905639648438\n",
      "699: 601.6036376953125\n",
      "899: 354.37677001953125\n",
      "1099: 180.43382263183594\n",
      "1299: 73.17616271972656\n",
      "1499: 22.37281608581543\n",
      "1699: 9.53388500213623\n",
      "1899: 8.922995567321777\n",
      "Result: y = 0.0004955814220011234 +                     0.8563342690467834 x +                     0.0004956094198860228 x^2 +                     -0.09381970018148422 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# define model as a sequence of layers\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1), # output of linear layer has shape (200, 1)\n",
    "    torch.nn.Flatten(0, 1) # flatens the linear output to 1D to match`y`.\n",
    ")\n",
    "\n",
    "# 把model移动到cuda上\n",
    "model = model.to(device)\n",
    "\n",
    "# define loss funtion\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 定义优化器，第一个argument告诉优化器要update的参数是哪些\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    \n",
    "    # forward: Module objects override the __call__ operator\n",
    "    # so you can call them like functions.\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 400 == 399:\n",
    "        print(f\"{t}: {loss.item()}\")\n",
    "    \n",
    "    # BP前要将weights置零，用了优化器后，使用优化器来置零\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "  \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + \\\n",
    "                    {linear_layer.weight[:, 0].item()} x + \\\n",
    "                    {linear_layer.weight[:, 1].item()} x^2 + \\\n",
    "                    {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3dec39",
   "metadata": {},
   "source": [
    "## 6. 自定义nn module\n",
    "· subclassing nn.Module and defining a forward which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca5d10c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:14.839795Z",
     "start_time": "2024-08-07T12:20:13.323026Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 524.7474365234375\n",
      "199 363.1872863769531\n",
      "299 252.4933319091797\n",
      "399 176.56356811523438\n",
      "499 124.42117309570312\n",
      "599 88.57331848144531\n",
      "699 63.900386810302734\n",
      "799 46.89997482299805\n",
      "899 35.17347717285156\n",
      "999 27.076091766357422\n",
      "1099 21.47881507873535\n",
      "1199 17.605751037597656\n",
      "1299 14.923057556152344\n",
      "1399 13.063041687011719\n",
      "1499 11.772197723388672\n",
      "1599 10.875513076782227\n",
      "1699 10.252089500427246\n",
      "1799 9.818252563476562\n",
      "1899 9.516105651855469\n",
      "1999 9.305503845214844\n",
      "Result: y = -0.02114352583885193 + 0.8475532531738281 x + 0.00364761077798903 x^2 + -0.09202353656291962 x^3\n"
     ]
    }
   ],
   "source": [
    "## implement the third order polynomial as a custom Module\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 注意这里的初始化方式\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = Polynomial3().to(device)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters (defined \n",
    "# with torch.nn.Parameter) which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 400 == 399:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72933a42",
   "metadata": {},
   "source": [
    "## 7. dynamic graph: control flow and weight sharing\n",
    "- 因为pytorch在每次执行forward pass的时候会新建一个dynamic computation graph，所以python中的control flow，比如for,while loops和if statement等，在module的forward method定义中都可以使用。不会给自动计算梯度造成问题。\n",
    "  - 本例中，forward的函数计算中有随机性，每次forward的函数形态会变化，但不影响梯度计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a27f086e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:20:39.080106Z",
     "start_time": "2024-08-07T12:20:14.842242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 676.8985595703125\n",
      "3999 319.4315185546875\n",
      "5999 147.26318359375\n",
      "7999 68.19235229492188\n",
      "9999 37.63022232055664\n",
      "11999 20.69076156616211\n",
      "13999 14.038339614868164\n",
      "15999 11.104504585266113\n",
      "17999 9.824529647827148\n",
      "19999 9.250890731811523\n",
      "21999 9.04260540008545\n",
      "23999 8.7088623046875\n",
      "25999 8.90151309967041\n",
      "27999 8.577032089233398\n",
      "29999 8.898475646972656\n",
      "Result: y = 0.0017547697061672807 + 0.8550370931625366 x + -0.0009069516672752798 x^2 + -0.09357427805662155 x^3                + 0.0001444164226995781 x^4 ? + 0.0001444164226995781 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "## implement a custom Module,\n",
    "#  第4，5次式可能有可能没有，且共享参数\n",
    "import random\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 注意这里的初始化方式\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(())) # 第4、5次式共享的参数\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        # 随机决定第4、5次式是否存在\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 \\\n",
    "               + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "\n",
    "model = DynamicNet().to(device)\n",
    "\n",
    "# The call to model.parameters() in the SGD constructor will contain the \n",
    "# learnable parameters (defined with torch.nn.Parameter) which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # Forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 4000 == 3999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
