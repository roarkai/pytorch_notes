{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c18bb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.762581Z",
     "start_time": "2024-08-03T15:22:04.432770Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1151dcc0",
   "metadata": {},
   "source": [
    "# Storage and View\n",
    "1. 每个tensor都有对应的底层data放在memory上，用tensor.storage()查看底层data的信息\n",
    "2. tensor有两种layout attribute，对应两种不同类型的tensor：dense和sparse tensor。\n",
    "   - dense tensor的底层data占用连续的存储空间，对应layout attribute是torch.stride。它用stride对象提供信息，决定如何从连续的data sequence上读取tensor element。\n",
    "   - sparse tensor则占用非连续空间，对应的layout attribute是torch.sparse。略<font color=red>[这里只关注dense tensor]</font>\n",
    "3. 每个dense tensor基于它的底层data有自己呈现data的方式，也就是view。不同的tensor可以共享相同的data memory，但是view不同。\n",
    "4. **常见的只改变view，不copy data的reshape method**：本质上是用不同的stride信息来读取底层data，所以不需要copy data。\n",
    "   1. <font color=blue>**tensor.view()**</font>：dense tensor如果是continuous tensor，可以用tensor.view() method改变tensor的view形态，得到的新tensor的data sequence的排序和原数据在memory中的排序一样，即不改变原tensor的continuity。<font color=green>如果要得到一个新shape的tensor，但是不知道是否能保证keep continuity，最好改用tensor.reshape()</font>\n",
    "   2. <font color=blue>**tensor.transpose()**</font>：两种dense tensor(continuous和non-continuous)都可以用tensor.transpose() method改变view，但得到的tensor的data sequence的排序和原数据在memory中的排序不同，因此此时continuity会被改变。\n",
    "   3. <font color=blue>**tensor.squeeze() and tensor.unsqueeze()**</font>: 两种dense tensor(continuous和non-continuous)都可以用\n",
    "   4. <font color=blue>**tensor.unflatten()**</font>，注意，tensor.flatten()可能会创建新的data\n",
    "   5. <font color=blue>**tensor.vsplit(), tensor.hsplit() and tensor.split()**</font>\n",
    "      - hsplit和vsplit中，如果参数是int，要求对应dim的length要整除该int\n",
    "      - split最灵活，如果给的参数是int，对应dim的length如果不能整除，那么最后一个分出来的section的数量最少\n",
    "   6. <font color=blue>**tensor.expand()**</font>：扩展维度为1的dim，或者扩展新的维度（新的维度相当于先unsqueeze，然后expand）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d86957",
   "metadata": {},
   "source": [
    "## 1. tensor的存储方式和特点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6945895",
   "metadata": {},
   "source": [
    "### 1.1 tensor.storage()\n",
    "输出tensor存在memory中的1D data sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fca0855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.773288Z",
     "start_time": "2024-08-03T15:22:05.764961Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20669/3405978919.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage(), x.is_contiguous()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "( 1\n",
       "  2\n",
       "  3\n",
       "  6\n",
       "  7\n",
       "  8\n",
       " [torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6],\n",
       " True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [6, 7, 8]])\n",
    "x.storage(), x.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7cadaa",
   "metadata": {},
   "source": [
    "### 1.2 dense和sparse tensor有不同的layout attribute\n",
    "1. 根据存储方式的不同，tensor有两种类型，dense tensor和sparse tensor，他们可以通过tensor.layout attribute来区分\\\n",
    "(1)**dense tensor** is stored linearly in a contiguous block of memory，也就是其存储占用了连续的memory block。\\\n",
    "(2)**sparse tensor**中大部分是0，只有很少比例的值是非零值，为了提高存储效率，pytorch为这类数据提供了特殊的存储方式。\n",
    "2. torch.layout attribute是表达tensor的memory layout特征的object。这个attribute有两种类型的取值：torch.stride和torch.sparse_coo，分别对应上面的两种tensor类型\\\n",
    "(1)如果<font color=blue>**layout=torch.strided**</font>，那么该tensor是dense tensor\\\n",
    "(2)如果<font color=blue>**layout=torch.sparse**</font>，那么该tensor是sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8a8811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.780626Z",
     "start_time": "2024-08-03T15:22:05.774883Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, (3, 1), False, (1, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 新建dense tensor，得到连续的存储空间\n",
    "#  连续的存储空间是指底层的data，改变view后的新tensor的元素可能不连续\n",
    "x = torch.rand((2, 3), layout=torch.strided, requires_grad=False)\n",
    "\n",
    "# 这里y是x做transpose后的View，这个view的element相对实际存储的data而言不连续\n",
    "y = x.T\n",
    "x.is_contiguous(), x.stride(), y.is_contiguous(), y.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86bed454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.788272Z",
     "start_time": "2024-08-03T15:22:05.782971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(indices=tensor([], size=(2, 0)),\n",
       "        values=tensor([], size=(0,)),\n",
       "        size=(100, 200), nnz=0, layout=torch.sparse_coo),\n",
       " False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 新建sparse tensor，存储空间本身就不连续\n",
    "z = torch.zeros((100, 200), layout=torch.sparse_coo, requires_grad=False)\n",
    "z, z.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f53313",
   "metadata": {},
   "source": [
    "### 1.3 tensor.stride()可以查看dense tensor读取底层data的方式\n",
    "1. dense tensor才有**tensor.stride()** method\n",
    "   - dense tensor不论是contiguous还是non-contiguous tensor都有stride。\n",
    "2. strided tensor的stride是一个int list,其第k-th元素的值表示tensor的k-th dimension上，一个元素到下个元素之间，用units of elements衡量的长度。\n",
    "3. 每个strided tensor都有一个与之关联的torch.Storage存放data. 这些tensors 则提供strided view of a storage。一个torch.Storage可以对应不同的strided tensors。这些tensors有相同的data，不同的tensor.view()\n",
    "   - <font color=blue>**Numpy strides()** returns (N bytes to Next Row, M bytes to Next Column)\n",
    "   - **Pytorch stride()** returns (N elements to Next Row, M elements to Next Column).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef9bd37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.793901Z",
     "start_time": "2024-08-03T15:22:05.789503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "5\n",
      "1\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5],\n",
    "                  [6, 7, 8, 9, 10]])\n",
    "\n",
    "print(x.stride())  # 不指定dim，A tuple of all strides is returned\n",
    "                   # 在tuple(5, 1)中，5在第0维，1在第1维对应traverse的维度\n",
    "print(x.stride(0)) # 指定dim=0，返回该维度上的stride值5\n",
    "                   # traverse along the 0th dim, 比如从'1'跳到'6',距离是5\n",
    "print(x.stride(-1))# 指定dim=1，返回该维度上的stride值1\n",
    "                   # traverse along the 1st dim, 比如从'7'跳到'8',距离是1\n",
    "\n",
    "## 改变view，stride也改变\n",
    "y = x.view(5, 2)\n",
    "print(y.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20998a20",
   "metadata": {},
   "source": [
    "### 1.4 dense tensor的底层data存储在连续位置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495f825",
   "metadata": {},
   "source": [
    "1. dense tensor可能是contiguous tensor或者non-contiguous tensor。虽然他们在存储空间中都占用了连续的存储单元，但non-contiguous tensor的元素排列顺序与其元素在存储空间的排列顺序不一致。\\\n",
    "**· <font color=blue>tensor.view()</font>** 会保持contiguous tensor的contiguity。也只有contiguous tensor有view() method\\\n",
    "**· <font color=blue>tensor.transpose()</font>** 会改变tensor的contiguity。contiguous和non-contiguous tensor都可以transpose。当base tensor是dense tensor时，transpose不copy data，所以output tensor也只是原underlying data的一个new view。\n",
    "2. 用is_contiguous()可以查看是否contiguous\n",
    "3. 用tensor.contiguous()可以把non-contiguous tensor转变成contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403313af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.800361Z",
     "start_time": "2024-08-03T15:22:05.795338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = torch.tensor([[0, 1],[2, 3]])\n",
    "t = base.transpose(0, 1) \n",
    "print(base.data_ptr() == t.data_ptr()) # 同一data\n",
    "base.is_contiguous(), t.is_contiguous() # continuity变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fee84a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.807018Z",
     "start_time": "2024-08-03T15:22:05.801571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6] \n",
      "\n",
      "y: \n",
      " tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "(3, 1) True \n",
      "\n",
      "z: \n",
      " tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "(1, 3) False\n"
     ]
    }
   ],
   "source": [
    "## 用tensor.storage()可以获得该tensor在memory上的1D sequence\n",
    "x = torch.arange(0,6)\n",
    "print(x.storage(), '\\n')\n",
    "\n",
    "## view返回的tensor中，data sequence的排序和原数据相同，都是0, 1, 2, 3, 4...\n",
    "y = x.view(2,3)\n",
    "print('y: \\n', y)\n",
    "print(y.stride(), y.is_contiguous(), '\\n')\n",
    "\n",
    "## transpose返回的tensor中，data sequence的排序和原数据在memory中的排序不同\n",
    "#  z中元素的排序是：0, 3, 1, 4...\n",
    "#  transpose不copy data，它仍然是原data的view，但是是non-contiguous ‘View’\n",
    "#  按新tensor的element顺序从memory中取值的时候不是连续取值\n",
    "z = y.T\n",
    "print('z: \\n', z)\n",
    "print(z.stride(), z.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592de342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.812600Z",
     "start_time": "2024-08-03T15:22:05.808596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## transpose将non-contiguous转变成contiguous\n",
    "w = z.T\n",
    "w.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c6c55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.818337Z",
     "start_time": "2024-08-03T15:22:05.814026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: \n",
      " tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]]) True\n",
      " 0\n",
      " 3\n",
      " 1\n",
      " 4\n",
      " 2\n",
      " 5\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]\n"
     ]
    }
   ],
   "source": [
    "## 用tensor.contiguous()转变成contiguous tensor\n",
    "#  此时发生了copy，按照tensor view的排序方式复制了一个新的data\n",
    "v = z.contiguous()\n",
    "print('v: \\n', v, v.is_contiguous())\n",
    "print(v.storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e777cf",
   "metadata": {},
   "source": [
    "## 2. 通过改变stride改变view的method\n",
    "### 2.1 用tensor.view()不改变torch.layout属性\n",
    "<font color=red>相当于numpy里面的reshape。</font>\n",
    "1. 只有contiguous dense tensor才有view method。\n",
    "2. dense tensor在计算机中以1D data sequence的形式存在congtiguous block of memory上。tensor.view()本质上是告诉计算机如何(根据其参数设置)stride over the 1D data sequence。所以，只提供了new view of the base tensor，不改变底层data。view的改变也就意味着tensor.stride()也会随之改变。\n",
    "3. tensor.view()返回的new tensor从memory上读取elements的顺序与其线性存储的顺序一致，<font color=red>只是改变了划分layer,column和row等unit单位的位置。</font>\n",
    "4. 因为View tensor与它的base tensor的underlying data是同一个data，即tensor.view()返回new tensor，但并不copy data，所以，其中一个tensor元素值改变，另一个也变。\n",
    "\n",
    "<font color=blue>**tensor.data_ptr()**</font>: 返回1st element的存储地址，即data pointer指向的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b9588b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.827145Z",
     "start_time": "2024-08-03T15:22:05.820994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原data pointer: 110620544\n",
      "原id: 133619926634704\n",
      "原stride和size: (1,) torch.Size([15]) \n",
      "\n",
      "新data pointer与原值相同: 110620544\n",
      "id变化，说明新建了一个tensor: 133619926634784\n",
      "stride和size变化: (5, 1) torch.Size([3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tensor.view()返回的新tensor不copy data，但是会改变stride和shape(size)\n",
    "a = torch.arange(15)\n",
    "print('原data pointer:', a.data_ptr()) # \n",
    "print('原id:', id(a))\n",
    "print('原stride和size:', a.stride(), a.size(), '\\n')\n",
    "\n",
    "# change view\n",
    "b = a.view(3, 5)\n",
    "print('新data pointer与原值相同:', b.data_ptr())\n",
    "print('id变化，说明新建了一个tensor:', id(b))                 # id变化\n",
    "print('stride和size变化:', b.stride(), b.size()) \n",
    "\n",
    "# data地址不变\n",
    "x.data_ptr() == y.data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be1765a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.833814Z",
     "start_time": "2024-08-03T15:22:05.828629Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      " tensor([0.3423, 0.7748, 0.1995, 0.1907, 0.0331, 0.8003, 0.5276, 0.5248, 0.9516,\n",
      "        0.3708, 0.9208, 0.0926])\n",
      "b: \n",
      " tensor([[[0.3423, 0.7748],\n",
      "         [0.1995, 0.1907]],\n",
      "\n",
      "        [[0.0331, 0.8003],\n",
      "         [0.5276, 0.5248]],\n",
      "\n",
      "        [[0.9516, 0.3708],\n",
      "         [0.9208, 0.0926]]])\n",
      "133619926633904 133619926634864 133619926634784\n"
     ]
    }
   ],
   "source": [
    "## tensor.view()不改变tensor layout in memory\n",
    "a = torch.rand(12)\n",
    "b = a.view(3, 2, 2)\n",
    "c = a.view(2, 3, 2)  # 不改变tensor layout in memory\n",
    "\n",
    "print('a: \\n', a)\n",
    "print('b: \\n', b) # b的元素排序和a完全相同，只是每个dim的截断位置改变\n",
    "\n",
    "print(id(a), id(b), id(c)) # id变化, 是不同的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926af611",
   "metadata": {},
   "source": [
    "### 2.2 用tensor.transpose()会改变torch.layout属性\n",
    "1. 和view method不同的是，transpose改变了elements在view中的layout。如果把transpose之后的tensor排成一个vector，其顺序与input tensor不相同。\n",
    "2. 和view method相同的是，当tensor是dense tensor时，output只是input的一个view，不发生copy。改变其中一个的元素值，另一个也变\n",
    "3. <font color=blue>**tensor.adjoint(),tensor.mT和tensor.mH**</font>在tensor元素为实数的时候相当于tensor.transpose(-2,-1)。返回原tensor交换最后两个维度后的view。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f197ecf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:23:30.556402Z",
     "start_time": "2024-08-03T15:23:30.552803Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原shape: \n",
      " tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11]],\n",
      "\n",
      "         [[12, 13, 14, 15],\n",
      "          [16, 17, 18, 19],\n",
      "          [20, 21, 22, 23]]]])\n",
      "torch.Size([1, 2, 3, 4]) 133619877710320 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(1, 2, 3, 4)\n",
    "print(\"原shape:\", '\\n', a)\n",
    "print(a.size(), id(a), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d741c414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:23:31.396702Z",
     "start_time": "2024-08-03T15:23:31.388115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用transpose交换了2nd和3rd dim: \n",
      " tensor([[[[ 0,  1,  2,  3],\n",
      "          [12, 13, 14, 15]],\n",
      "\n",
      "         [[ 4,  5,  6,  7],\n",
      "          [16, 17, 18, 19]],\n",
      "\n",
      "         [[ 8,  9, 10, 11],\n",
      "          [20, 21, 22, 23]]]])\n",
      "torch.Size([1, 3, 2, 4]) 133619877711920 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
       "        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.transpose(1, 2)\n",
    "print(\"用transpose交换了2nd和3rd dim:\", '\\n', b)\n",
    "print(b.size(), id(b), '\\n')\n",
    "\n",
    "a.view(2, 12)\n",
    "# b.view(24) # 错，b这时不是contiguous，不能用view method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0de76531",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:41:08.640037Z",
     "start_time": "2024-08-03T15:41:08.625224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0,  4,  8],\n",
       "           [ 1,  5,  9],\n",
       "           [ 2,  6, 10],\n",
       "           [ 3,  7, 11]],\n",
       " \n",
       "          [[12, 16, 20],\n",
       "           [13, 17, 21],\n",
       "           [14, 18, 22],\n",
       "           [15, 19, 23]]]]),\n",
       " torch.Size([1, 2, 4, 3]),\n",
       " tensor(True))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.mT # 注意mT不是函数，mutate tail\n",
    "d = a.mH # 注意mH不是函数，mutate Head\n",
    "\n",
    "c, c.shape, torch.all(c == d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c65ce",
   "metadata": {},
   "source": [
    "### 2.3 squeeze and unsqueeze\n",
    " - squeeze和unsqueeze只改变原数据的view，新生成的tensor与input tensor指向相同的memory\n",
    "#### 2.3.1 tensor.unsqueeze()\n",
    "1. pytorch默认是按照batch处理数据，以image为例，默认的input大小是(N, C, H, W)。如果想让model处理单个数据，就要把当个样本的大小从(C, H, W)改成(1, C, H, W)\n",
    "2. 常用于ease broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ff4f526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.853699Z",
     "start_time": "2024-08-03T15:22:05.849006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "增加一个长度为1的维度到新的第0维： torch.Size([1, 3, 226, 226])\n",
      "增加一个长度为1的维度到新的第1维： torch.Size([3, 1, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "# 用tensor.unsqueeze()来增加长度为1的新维度\n",
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0) # 在第0维增加一个维度\n",
    "print('增加一个长度为1的维度到新的第0维：',b.shape)\n",
    "\n",
    "c = a.unsqueeze(1)\n",
    "print('增加一个长度为1的维度到新的第1维：',c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80164246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.860867Z",
     "start_time": "2024-08-03T15:22:05.855388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before: torch.Size([3])\n",
      "shape after: torch.Size([3, 1])\n",
      "tensor([[[0., 0.],\n",
      "         [1., 1.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [1., 1.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [1., 1.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [1., 1.],\n",
      "         [2., 2.]]])\n"
     ]
    }
   ],
   "source": [
    "# unsqueeze常用于方便broadcast\n",
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.arange(0,3)     # a * b不能直接运算\n",
    "print('shape before:', b.shape)\n",
    "c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\n",
    "print('shape after:', c.shape)\n",
    "print(a * c)             # broadcasting works again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c727082",
   "metadata": {},
   "source": [
    "#### 2.3.2 tensor.squeeze()\n",
    "1. 用tensor.squeeze()来压缩长度为1的维度(dimensions of extent 1)\n",
    "2. 最好指定dim number，因为pytorch按batch处理数据，squeeze()会把第1维N压掉，导致模型出错 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f9c3cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.868794Z",
     "start_time": "2024-08-03T15:22:05.862551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "( 2.0\n",
       "  2.0\n",
       "  2.0\n",
       "  2.0\n",
       "  2.0\n",
       "  2.0\n",
       " [torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6],\n",
       " (6, 3, 3, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(1, 2, 1, 3)\n",
    "b = a.squeeze()   # 压缩所有维度为1的dims\n",
    "c = a.squeeze(0)  # 压缩第0维\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "\n",
    "## 改变b之后，a的值也变\n",
    "b += 1\n",
    "a.storage(), a.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcacdcce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.874792Z",
     "start_time": "2024-08-03T15:22:05.870360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 1), (1, 5))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
    "x.stride(), x.T.stride()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
