{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a572b5b",
   "metadata": {},
   "source": [
    "# 用不同的抽象层次工具构建network"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af2b3081",
   "metadata": {},
   "source": [
    "第一层：tensor ops    |工具tensor operation\n",
    "第二层：function      |工具torch.nn.Functional <- 可以实例化torch.autograd.Function来自定义\n",
    "第三层：layer         |工具torch.nn            <- 可以实例化torch.nn.Module来自定义\n",
    "第四层：nested model  |工具nested module/ module sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061f1768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:12.327956Z",
     "start_time": "2023-10-30T07:46:11.145375Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0d9c0",
   "metadata": {},
   "source": [
    "## I.tensor operation：在tensor包中\n",
    "1. 直接以tensor为对象的操作，比如shape manipulate(eg:view)，math ops\n",
    "2. 是最基础的运算方式，是构成function的基础。[可以理解成最简单的function]\n",
    "3. 已经内建了forward和backward method。autograd根据tensor的requires_grad属性值来判断是否构建DAG，是否计算gradient并保存到tensor.grad\n",
    "4. 可以将tensor ops处理成python函数，此时autograd也可以自动完成函数的DAG构建和backward propagation\n",
    "\n",
    "**典型使用场景：**\n",
    "1. 数据预处理\n",
    "2. 自定义torch.autograd.Function时，用来实现函数运算\n",
    "3. module forward method中可以将所需ops打包成python函数形式。通常用于ops不涉及learnable parameter的时候讲函数用于module的forward method中，功能和nn.functional函数一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6c2be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:12.353020Z",
     "start_time": "2023-10-30T07:46:12.329575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000, 0.5000, 0.5000]) None\n"
     ]
    }
   ],
   "source": [
    "# 直接使用底层tensor ops\n",
    "\n",
    "# 仅示例，实际不要这样使用！！！稍微复杂的网络都无法实现\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "w = torch.ones(3) * 0.5\n",
    "y = (w * x).sum()\n",
    "y.backward()\n",
    "print(x.grad, w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ca5fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:12.362624Z",
     "start_time": "2023-10-30T07:46:12.356470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0045, -0.0214,  0.1023],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n",
      "tensor([[ 0.0353,  0.0000],\n",
      "        [ 0.0004,  0.0000],\n",
      "        [-0.0453,  0.0000],\n",
      "        [ 0.0096,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 将tensor ops处理成python函数实现: affine -> relu -> affine -> softmax\n",
    "\n",
    "# 仅示例，实际不要这样使用！！！\n",
    "\n",
    "# 定义函数：\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1) # 'unsqueeze' is for broadcast   \n",
    "\n",
    "def model(x, w1, w2):\n",
    "    x = x @ w1.T\n",
    "    x = torch.relu(x)\n",
    "    x = x @ w2.T\n",
    "    return log_softmax(x)\n",
    "\n",
    "def negative_log_likelyhood(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "loss_func = negative_log_likelyhood\n",
    "\n",
    "# 执行运算：\n",
    "x = torch.randn(2, 3)\n",
    "y = torch.tensor([2, 3])\n",
    "w1 = torch.randn(2, 3, requires_grad=True)\n",
    "w2 = torch.randn(4, 2, requires_grad=True)\n",
    "\n",
    "scores = model(x, w1, w2)\n",
    "loss = loss_func(scores, y)\n",
    "loss.backward()\n",
    "\n",
    "print(w1.grad)\n",
    "print(w2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bdda41",
   "metadata": {},
   "source": [
    "## II. function：在torch.nn.functional包中\n",
    "1. 定义了forward和backward method\n",
    "2. 可以通过实例化torch.autograd.Function来自定义\n",
    "3. autograd基于function构建graph中的Node，如果output tensor requires_grad，grad_fn指向backward method\n",
    "\n",
    "**特征：**\n",
    "1. 缺点：要手动管理learnable parameters：新建、初始化、赋值函数argument、用梯度做update\n",
    "2. 当函数本身没有learnable parameter的时候，上述缺点就不存在，此时function和layer功能一样\n",
    "\n",
    "**典型使用方式：**\n",
    "1. <font color=green>**没有learnable parameter的function常用于module的forward method中做对应运算。如:F.relu** </font>\n",
    "2. <font color=green>**自定义function如果有learnable parameter，通常打包成layers使用；如果没有也直接用于module的forward method。** </font>\n",
    "\n",
    "<font color=red>注：通过实例化autograd.Function得到的function要apply后才能当做函数使用。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b4c0bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:12.376769Z",
     "start_time": "2023-10-30T07:46:12.363737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[-0.2859,  0.2067,  0.0354],\n",
      "        [-0.0313,  0.0271, -0.0164]])\n",
      "tensor([[ 0.0249,  0.0534],\n",
      "        [ 0.0355,  0.1165],\n",
      "        [-0.0469,  0.5916],\n",
      "        [-0.0135, -0.7615]])\n"
     ]
    }
   ],
   "source": [
    "# 用function实现: affine -> relu -> affine -> softmax\n",
    "\n",
    "# 仅示例，实际不要这样使用！！！\n",
    "\n",
    "# 网络结构复杂时，应该通过实例化nn.Module来自定义net，见下例\n",
    "\n",
    "torch.manual_seed(2)\n",
    "x = torch.randn(2, 3)\n",
    "y = torch.tensor([2, 3])\n",
    "w1 = torch.randn(2, 3, requires_grad=True)\n",
    "w2 = torch.randn(4, 2, requires_grad=True)\n",
    "scores = F.linear(F.relu(F.linear(x, w1)), w2)               # F.linear运算方式：x @ w.T\n",
    "loss = F.cross_entropy(scores, y)\n",
    "loss.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(w1.grad)\n",
    "print(w2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66210f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:12.383028Z",
     "start_time": "2023-10-30T07:46:12.377989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w1', Parameter containing:\n",
      "tensor([[-0.0591, -1.5653,  0.4258],\n",
      "        [-1.4818, -0.4766,  0.2480]], requires_grad=True))\n",
      "('w2', Parameter containing:\n",
      "tensor([[ 0.1559, -0.1607],\n",
      "        [ 0.4172,  1.0004],\n",
      "        [ 0.6008,  0.1098],\n",
      "        [-0.8411, -0.2908]], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 用nn.Module封装nn.Functional实现simple net：affine -> relu -> affine -> softmax\n",
    "\n",
    "# 仅示例，实际不要这样使用！！！\n",
    "\n",
    "# 优点：nn.Module可以利用autograd system来自动管理learnable parameters\n",
    "# 缺点：要手动向nn.Parameter登记nn.Functional中的参数，参数初始化和向函数argument传参也要手动\n",
    "#      当网络复杂后，很难再手动管理参数\n",
    "\n",
    "class Simple_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Parameter(torch.randn(2, 3))\n",
    "        self.w2 = nn.Parameter(torch.randn(4, 2))\n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, w1)\n",
    "        x = F.relu(x)\n",
    "        x = F.linear(x, w2)\n",
    "        return x\n",
    "\n",
    "x = torch.ones(2, 3)\n",
    "y = torch.tensor([2, 3])\n",
    "model = Simple_net()\n",
    "loss = F.cross_entropy(model(x), y)\n",
    "loss.backward()\n",
    "\n",
    "for param in model.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b5a5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:50:51.914971Z",
     "start_time": "2023-10-30T01:50:51.910083Z"
    }
   },
   "source": [
    "## III. layer：在torch.nn包中\n",
    "1. forward method中定义了layer运算用的functions，以及functions用到的learnable parameters\n",
    "2. 可以通过实例化torch.nn.Module来自定义\n",
    "3. autograd可以基于layer中的function构建Node；用backward method计算梯度，用optimizer自动更新parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9bf452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:12.389933Z",
     "start_time": "2023-10-30T07:46:12.384535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('linear1.weight', Parameter containing:\n",
      "tensor([[ 1.3269,  0.1434, -0.6918],\n",
      "        [-0.6898, -0.6815, -0.5943]], requires_grad=True))\n",
      "('linear2.weight', Parameter containing:\n",
      "tensor([[-1.7490,  1.3883],\n",
      "        [-0.1098,  1.2384],\n",
      "        [ 0.5407,  0.5787],\n",
      "        [-0.3134, -0.0843]], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 用nn.Module示例封装layers实现simple net: affine -> relu -> affine -> softmax\n",
    "# 优点：\n",
    "#   1. nn.Module可以利用autograd system来自动管理learnable parameters\n",
    "#   2. 基本不用手动管理参数：初始化、传参、参数更新都由autograd system自动完成\n",
    "\n",
    "class Simple_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(3, 2, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(2, 4, bias=False)\n",
    "        # 初始化: 使用in-place操作\n",
    "        nn.init.kaiming_normal_(self.linear1.weight)\n",
    "        nn.init.kaiming_normal_(self.linear2.weight)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)        # 没有learnable parameter的layer则可以直接用nn.Funcitonal\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "x = torch.ones(2, 3)\n",
    "y = torch.tensor([2, 3])\n",
    "model = Simple_net()\n",
    "loss = F.cross_entropy(model(x), y)\n",
    "loss.backward()\n",
    "\n",
    "for param in model.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35396a",
   "metadata": {},
   "source": [
    "## IV. 复杂网络：nn.Sequential, nn.ModuleList, nn.ModuleDict\n",
    "**比较nn.Sequential和nn.ModuleList：** \\\n",
    "<font color=blue>**在nn.Sequential中：** </font>\n",
    "1. 堆叠的nn.Module时相互关联的，上一的输出是下一个的输入。因此，定义nn.Sequential的时候要保证output size of a layer/block matches the input size of the following layer/block\n",
    "2. object of type nn.Sequential有forward method。所以sequential是可执行的。整个sequence会构成一个功能模块，具体执行方式是按顺序执行其中的layers。比如sequential中放了conv2D，ReLU和Linearlayer的话，实际上会构成一个可运行的network: Conv2D -> ReLU -> Linear layer。output = Linear(relu(conv2D(x))). \n",
    "3. nn.Sequential可以用add_module() method来添加layer。\n",
    "\n",
    "<font color=blue>**在ModulList中：** </font>  \n",
    "1. 没有forward method，所以不能执行运算，list不构成一个可运行的Network。\n",
    "2. list中存放的nn.Modules之间也没有前后关联。就和python的list相似，只是存的数据类型是nn.Modules。不用python list而用ModuleList的区别在于，直接用python list的话，list中的layer的parameter就无法自动计入model.parameters()中\n",
    "3. ModuleList和list一样，用append() method来添加layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad092007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:12.395807Z",
     "start_time": "2023-10-30T07:46:12.391339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (deep_nn): Sequential(\n",
      "    (ff0): Linear(in_features=16, out_features=128, bias=True)\n",
      "    (activation0): ReLU()\n",
      "    (ff1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (activation1): ReLU()\n",
      "    (ff2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (activation2): ReLU()\n",
      "    (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## 用add_module() method来添加layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_hidden_layers=3, hidden_layer_size=128):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.deep_nn = nn.Sequential()\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.deep_nn.add_module(f'ff{i}', nn.Linear(input_size, hidden_layer_size))\n",
    "            self.deep_nn.add_module(f'activation{i}', nn.ReLU())\n",
    "            input_size = hidden_layer_size\n",
    "        self.deep_nn.add_module(f'classifier', nn.Linear(hidden_layer_size, output_size))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tensor = self.deep_nn(inputs)\n",
    "        return tensor\n",
    "\n",
    "model = NeuralNet(16, 2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22058969",
   "metadata": {},
   "source": [
    "## 1. nn.Sequential：用于堆叠layers/blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339014e",
   "metadata": {},
   "source": [
    "### i. 直接用layers\n",
    "**缺点：**\n",
    "1. layer多了之后model的结构性很差\n",
    "2. 内部复用性和外部复用性都很差： \\\n",
    "(1) 如果要增加layer，就要修改__init__()和forward()，如果要加conv3和bn3，并不能复用前面的内容 \\\n",
    "(2) 其中模块也不能被其他model使用，比如其中的'conv-batchnorm-relu'block时很常见的block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad9b218c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:12.730448Z",
     "start_time": "2023-10-30T07:46:12.397091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=50176, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 1024)\n",
    "        self.fc2 = nn.Linear(1024, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = MyCNNClassifier(1, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053edd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T04:47:37.457866Z",
     "start_time": "2023-10-30T04:47:37.452299Z"
    }
   },
   "source": [
    "### ii. 用nn.Sequential()来stack layers\n",
    "**优点：**结构比前面更清晰，可以模块化地管理每个Sequential中的参数等 \\\n",
    "**缺点：**内外部复用性还是差，所有模块仍然要在__init__中逐个定义，比如下面2个conv_block要写两次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e75e246e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:13.090889Z",
     "start_time": "2023-10-30T07:46:12.732649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=50176, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_c, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64 * 28 * 28, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, n_classes)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "model = MyCNNClassifier(1, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc61e37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T04:46:57.102282Z",
     "start_time": "2023-10-30T04:46:57.097142Z"
    }
   },
   "source": [
    "### iii. 用nn.Sequential()来stack layers，用function来wrap Sequential\n",
    "**优点：** 结构同样清晰 \\\n",
    "**缺点：**内外部可复用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a3eb5f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:13.094567Z",
     "start_time": "2023-10-30T07:46:13.092030Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_block(in_f, out_f, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b6346a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:13.458503Z",
     "start_time": "2023-10-30T07:46:13.095732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=50176, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = conv_block(in_c, 32, kernel_size=3, padding=1)\n",
    "        self.conv_block2 = conv_block(32, 64, kernel_size=3, padding=1)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64 * 28 * 28, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = MyCNNClassifier(1, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2aa8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T04:46:22.036047Z",
     "start_time": "2023-10-30T04:46:22.031484Z"
    }
   },
   "source": [
    "### iv. 进一步结构化：用nn.Sequential()来stack layers，用function来wrap Sequential，再用Sequential来stack function\n",
    "**优点：** 结构同样清晰 \\\n",
    "**缺点：** 内外部可复用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "330c042d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:13.816124Z",
     "start_time": "2023-10-30T07:46:13.459834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=50176, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, n_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(in_c, 32, kernel_size=3, padding=1),\n",
    "            conv_block(32, 64, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64 * 28 * 28, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "model = MyCNNClassifier(1, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03a923d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T04:55:36.396023Z",
     "start_time": "2023-10-30T04:55:36.390303Z"
    }
   },
   "source": [
    "### v.Dynamic Sequential：一次创建多个layers\n",
    "1. 用list comprehension生成block list\n",
    "2. 用list generator来作为nn.Sequential的argument，实现一次构建多个layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "272be128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:14.174838Z",
     "start_time": "2023-10-30T07:46:13.817715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=50176, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, n_classes):\n",
    "        super().__init__()\n",
    "        # number of hidden layers\n",
    "        self.enc_sizes = [in_c, 32, 64]\n",
    "        \n",
    "        # 用list comprehension生成block list\n",
    "        conv_blocks = [conv_block(in_f, out_f, kernel_size=3, padding=1) \n",
    "                       for in_f, out_f in zip(self.enc_sizes, self.enc_sizes[1:])]\n",
    "        \n",
    "        # 将list decompose后作nn.Sequential的argument，实现一次构建多个layers\n",
    "        self.encoder = nn.Sequential(*conv_blocks)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64 * 28 * 28, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "model = MyCNNClassifier(1, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f346a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T05:02:59.645677Z",
     "start_time": "2023-10-30T05:02:59.642071Z"
    }
   },
   "source": [
    "### vi. 进一步分拆encoder和decoder\n",
    "<font color=red>这里用python function来warp nn.Sequential，可以方便给Sequential中的layers传参。此时，返回值是nn.Sequential。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e036fbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:14.181505Z",
     "start_time": "2023-10-30T07:46:14.176369Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoder module\n",
    "def conv_block(in_f, out_f, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "class MyEncoder(nn.Module):\n",
    "    def __init__(self, enc_sizes):\n",
    "        super().__init__()\n",
    "        self.conv_blocks = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=3, padding=1) \n",
    "                       for in_f, out_f in zip(enc_sizes, enc_sizes[1:])])\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.conv_blocks(x)\n",
    "\n",
    "# decoder module\n",
    "def dec_block(in_f, out_f):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_f, out_f),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "class MyDecoder(nn.Module):\n",
    "    def __init__(self, dec_sizes, n_classes):\n",
    "        super().__init__()\n",
    "        self.dec_blocks = nn.Sequential(*[dec_block(in_f, out_f) \n",
    "                       for in_f, out_f in zip(dec_sizes, dec_sizes[1:])])\n",
    "        self.last = nn.Linear(dec_sizes[-1], n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dec_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e1339b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:14.548901Z",
     "start_time": "2023-10-30T07:46:14.182661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (encoder): MyEncoder(\n",
      "    (conv_blocks): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): MyDecoder(\n",
      "    (dec_blocks): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=50176, out_features=1024, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (last): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, enc_sizes, dec_sizes,  n_classes):\n",
    "        super().__init__()\n",
    "        # super parameter configuration\n",
    "        self.enc_sizes = [in_c, *enc_sizes]\n",
    "        self.dec_sizes = [self.enc_sizes[-1] * 28 * 28, *dec_sizes]\n",
    "        # encoder and decoder\n",
    "        self.encoder = MyEncoder(self.enc_sizes)\n",
    "        self.decoder = MyDecoder(self.dec_sizes, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(1) # flat\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "model = MyCNNClassifier(1, [32,64], [1024, 512], 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1420e",
   "metadata": {},
   "source": [
    "## 2. nn.ModuleList：用于iterate module as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c574841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:14.580552Z",
     "start_time": "2023-10-30T07:46:14.550796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule()\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super().__init__()\n",
    "#         self.linears = nn.ModuleList([nn.Linear(in_f, out_f) for in_f, out_f in zip(sizes, sizes[1:])])\n",
    "        self.linears = [nn.Linear(in_f, out_f) for in_f, out_f in zip(sizes, sizes[1:])]\n",
    "        self.linears.append(nn.Linear(32, 10))  # 这里写死了参数，只是为了示例append()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:   # iterate\n",
    "            x = layer(x)\n",
    "        return x\n",
    "        \n",
    "model = MyModule([1, 16, 32])\n",
    "\n",
    "torch.manual_seed(2)\n",
    "x = torch.rand(4,1)\n",
    "y = model(x)\n",
    "print(model)\n",
    "\n",
    "# 注意：如果不用nn.ModuleList而用python list，无法自动登记参数\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644338a",
   "metadata": {},
   "source": [
    "## 3. nn.ModuleDict：用于需要根据条件选择不同module的场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00ccc6ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:14.588572Z",
     "start_time": "2023-10-30T07:46:14.582997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def conv_block(in_f, out_f, activation='relu', *args, **kwargs):\n",
    "    # dict用于实现：根据参数选择不同的layer作为activations\n",
    "    activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['relu', nn.ReLU()]\n",
    "    ])\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        activations[activation]\n",
    "    )\n",
    "\n",
    "print(conv_block(1, 32,'lrelu', kernel_size=3, padding=1))\n",
    "print(conv_block(1, 32,'relu', kernel_size=3, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a11fb",
   "metadata": {},
   "source": [
    "## 一个综合案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13f471f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:14.596273Z",
     "start_time": "2023-10-30T07:46:14.589723Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用python function warp nn.Sequential传参\n",
    "def conv_block(in_f, out_f, activation='relu', *args, **kwargs):\n",
    "    activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['relu', nn.ReLU()]\n",
    "    ])\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        activations[activation]\n",
    "    )\n",
    "\n",
    "class MyEncoder(nn.Module):\n",
    "    def __init__(self, enc_sizes, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv_blocks = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=3, padding=1, *args, **kwargs) \n",
    "                       for in_f, out_f in zip(enc_sizes, enc_sizes[1:])])\n",
    "        def forward(self, x):\n",
    "            return self.conv_blocks(x)\n",
    "\n",
    "        \n",
    "# 用python function warp nn.Sequential传参\n",
    "def dec_block(in_f, out_f):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_f, out_f),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "class MyDecoder(nn.Module):\n",
    "    def __init__(self, dec_sizes, n_classes):\n",
    "        super().__init__()\n",
    "        self.dec_blocks = nn.Sequential(*[dec_block(in_f, out_f) \n",
    "                       for in_f, out_f in zip(dec_sizes, dec_sizes[1:])])\n",
    "        self.last = nn.Linear(dec_sizes[-1], n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dec_blocks()\n",
    "    \n",
    "    \n",
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, enc_sizes, dec_sizes,  n_classes, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.enc_sizes = [in_c, *enc_sizes]\n",
    "        self.dec_sizes = [32 * 28 * 28, *dec_sizes]\n",
    "\n",
    "        self.encoder = MyEncoder(self.enc_sizes, activation=activation)\n",
    "        self.decoder = MyDecoder(dec_sizes, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(1) # flat\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "340dd6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T07:46:14.604456Z",
     "start_time": "2023-10-30T07:46:14.597401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (encoder): MyEncoder(\n",
      "    (conv_blocks): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): MyDecoder(\n",
      "    (dec_blocks): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (last): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNNClassifier(1, [32,64], [1024, 512], 10, activation='lrelu')\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
