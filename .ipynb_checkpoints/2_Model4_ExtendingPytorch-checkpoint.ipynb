{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0433507a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:49.289700Z",
     "start_time": "2024-08-10T14:29:47.935435Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a7305",
   "metadata": {},
   "source": [
    "# Extending Pytorch\n",
    "## 1. Extending torch.autograd\n",
    "### 1.1 用法理解\n",
    "1. 这里的扩展指的是以自定义autograd.Function的方式增加autograd处理的operation\n",
    "2. **使用场景：**\n",
    "   1. 想在模型中使用不可导函数\n",
    "   2. 虽然依赖non-Pytorch library(比如用numpy)来implement operation，但仍然想让operation可以chain with其他pytorch library提供的operations，并且使用autograd engine来做forward和backward。\n",
    "   3. 为了提升内存利用率或者performance，使用了c++ extension来写operation，也可以wrap成Function来应用autograd engine。\n",
    "   4. 为了减少内存占用，想要减少number of buffers saved for backward pass，也可以用自定义函数来combine ops together。\n",
    "3. **不要使用自定义Function的场景：**\n",
    "   1. pytorch函数库已经有想要执行的运算，而且已经可以record backward Graph，就没有必要自己写。\n",
    "   2. 如果只是想要maintain state，比如：trainable parameters，可以用自定义的module，而不需要自定义Function。\n",
    "   3. 如果想要改变pytorch库中函数在backward pass中计算gradient的方式，以实现其他的side effect，可以用registering a tensor或者registering a Module hook的方式实现，也没有必要额外定义函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff45e5",
   "metadata": {},
   "source": [
    "### 1.2 4. 实现方式\n",
    "**step1: 定义Function的子类，并在其中implement三个methods：forward(), setup_context(), backward()。**\n",
    "1. forward()：\n",
    "   1. 任意python的数据类型都可以作为forward的input，并且有的input可以是optional的，只要指定了default value就可以。\n",
    "   2. 如果用tensor作为argument，并且该tensor要track history(设置了requires_grad=True)，那么在调用forward之前，会被转换成不需要track history的tensor，他们在forward中参与的ops会计入graph。如果用list or dict of tensor作为参数，auto engine不会遍历其中的每个tensor来应用上述规则。\n",
    "   3. 可以返回single tensor作为单一output，或者a tuple of tensors作为multiple outputs。\n",
    "2. setup_context()：只处理信息保存，不能做computation。\n",
    "3. backward(),即vjp()：定义梯度计算公式。\n",
    "   1. 它收到的inputs数量应该和函数outputs数量相同，对应的就是这些outputs各自的梯度。一定不能对这些inputs做in-place modification。\n",
    "   2. 它返回的output tensors数量应该和函数的inputs数量相同。对应的就是各个inputs的梯度。<font color=blue>如果有的inputs不需要计算梯度，或者他们不是tensor类型</font>，那么也要用None作为它的梯度返回值。\n",
    "   3. 如果forward()中有optional argument，那么backward要预留返回值的位置，此时返回的gradient数量可能超过实际input arguments数量，但只要将他们都取值为None就行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571edf24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:49.296272Z",
     "start_time": "2024-08-10T14:29:49.291923Z"
    }
   },
   "outputs": [],
   "source": [
    "# 说明上文中的optional argument的情况\n",
    "# 本例中a2是optional argument，如果没有出现，在backward中返回None就好\n",
    "class Foo(torch.autograd.Function):\n",
    "    def forward(a1, a2=None):\n",
    "        if a2 is not None:\n",
    "            return a2 * a1\n",
    "        else:\n",
    "            return a1\n",
    "    \n",
    "    def setup_context(ctx, inputs, output):\n",
    "        a1, a2 = inputs\n",
    "        ctx.save_for_backward(a2)\n",
    "\n",
    "    def backward(ctx, dout):\n",
    "        a2 = ctx.saved_tensors\n",
    "        if a2 is not None:\n",
    "            return a2, a1\n",
    "        else:\n",
    "            return torch.ones_like(a1), None    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae4643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T13:05:42.848006Z",
     "start_time": "2023-10-22T13:05:42.836286Z"
    }
   },
   "source": [
    "**step2: 合理使用ctx，确保新定义的函数可以使用autograd engine。** \n",
    "1. **save_for_backward()**: 存放backward中要使用的tensor，non-tensor直接存在ctx上作为attribute。<font color=red>如果既不是input，又不是output的tensor要在ctx中存放供backward使用，则整个函数可能无法支持double backward。</font>\n",
    "2. <font color=green>**mark_dirty()**</font>: 如果有input被modified in-place，要用mark_dirty做标记\n",
    "3. <font color=green>**mark_non_differetiable()**</font>: 如果有的output不可微，必须用该method来做标记。默认所有可微的outputs tensor都会被设置为requires_grad=True。一旦mark后，auto engine就不会帮该output设置requires_grad=True。\n",
    "4. <font color=green>**set_materialize_grads()**</font>: 默认设置为True。当函数的output计算不依赖input的取值时，可以设参数为False来节省梯度计算的时间。\n",
    "   - 设为False相当于告诉autograd engine不要materialize给到backward function的output_grad参数。也就是设置False后, backward的output_grad参数如果拿到的值是None object in Python or “undefined tensor” in C++，auto engine将不会在调用backward之前将它们转变成<font colro=red>**全零tensor**</font>, 此时需要手动处理backward的output_grad参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d8622",
   "metadata": {},
   "source": [
    "**step3: 如果函数不支持double backward，要对backward method用decorate once_differentiable()来明确声明。** \\\n",
    "**step4: 用torch.autograd.gradcheck()来检查backward的定义方式是否正确**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a2f6df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:49.310799Z",
     "start_time": "2024-08-10T14:29:49.297886Z"
    }
   },
   "outputs": [],
   "source": [
    "# 例1：自定义线性函数 y = x @ w.T + b\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input, weight, bias):\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    # inputs是传给forward的所有inputs构成的tuple\n",
    "    # output是forward()的output\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        input, weight, bias = inputs\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "\n",
    "    # 本例中的函数只有单一output, 所以backward只输出1个gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # unpack saved_tensors\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        \n",
    "        # inputs中所有元素的梯度都初始化为None是好习惯\n",
    "        # 当函数有optional inputs的时候，return statement可以很简单\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # 这里的条件判断optional，算一种好习惯，可以提升效率\n",
    "        if ctx.needs_input_grad[0]:   \n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        # 给不需要gradient的那些input也返回gradient并不会报错\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1678689",
   "metadata": {},
   "source": [
    "**使用自定义Function的两种常见方式：**\n",
    "1. 直接apply成一个函数\n",
    "2. wrap成一个新的函数：此时支持default args and keyword args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c75f07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:49.317741Z",
     "start_time": "2024-08-10T14:29:49.312936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Option 1: 直接apply一个函数\n",
    "linear = LinearFunction.apply\n",
    "\n",
    "# Option 2: wrap in a function\n",
    "def linear(input, weight, bias=None):\n",
    "    return LinearFunction.apply(input, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e745b817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:49.324395Z",
     "start_time": "2024-08-10T14:29:49.319305Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2，使用non-tensor argument: y = const * x\n",
    "class MulConstant(Function):\n",
    "    @staticmethod\n",
    "    def forward(tensor, constant):\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        # ctx是一个context object，用于为backward存储所需信息\n",
    "        tensor, constant = inputs\n",
    "        ctx.constant = constant   # constant直接存为ctx的attribute\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # 所有input都要有梯度返回值，non-Tensor arguments的梯度返回值为None\n",
    "        return grad_output * ctx.constant, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "468a4dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:49.331565Z",
     "start_time": "2024-08-10T14:29:49.325810Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例3，对上例做优化：\n",
    "#  这里ctx.constant已经保存了backward函数所需信息，计算梯度不需要inputs\n",
    "#  所以可以设置set_materialize_grads(False)\n",
    "#  但是要手动处理grad_output为None的情形\n",
    "class MulConstant(Function):\n",
    "    @staticmethod\n",
    "    def forward(tensor, constant):\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        tensor, constant = inputs\n",
    "        # backward不需要setup_context中的inputs的信息\n",
    "        # 所以设置set_materialize_grads(False)\n",
    "        ctx.set_materialize_grads(False)\n",
    "        ctx.constant = constant\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # 手动处理grad_output为None的情形，本例中直接return None就行\n",
    "        if grad_output is None:\n",
    "            return None, None\n",
    "\n",
    "        return grad_output * ctx.constant, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade927a",
   "metadata": {},
   "source": [
    "**如果需要保存forward()中计算出来的中间值tensor**\n",
    "1. 有两种处理方式：\\\n",
    "(1)将其处理成forward的outputs。 <font color=blue>要计算高阶导数的时候，要用这种方式，同时还要将该tensor存到ctx.save_for_backward()中。</font> \\\n",
    "说明：\\\n",
    "backward的input，如grad_output, 也可能requires_grad=True。如果backward中用的operation是可微的，可以进一步计算高阶导数。但是ctx中存储的tensor本身并不会有gradients flowing back for them. 如果需要gradient flowing back到这些tensor上，就要将他们处理成output of the custom Function，同时存在ctx.save_for_backward()中. 这样，tensor就既能被backward()使用，又能有gradient flowing back to it。\\\n",
    "(2)联用forward和setup_context\n",
    "2. 如果计算图要通过该tensor，就要为他定义gradient的计算公式。\n",
    "\n",
    "<font color=red>[详见笔记：Double backward]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c9059aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:49.337340Z",
     "start_time": "2024-08-10T14:29:49.332983Z"
    }
   },
   "outputs": [],
   "source": [
    "## 处理成forward的outputs\n",
    "# 例4：支持高阶导数\n",
    "class MyCube(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        # 要存dx在backward中使用，为此，在forward中将其作为返回值\n",
    "        dx = 3 * x ** 2\n",
    "        result = x ** 3\n",
    "        return result, dx\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x, = inputs\n",
    "        result, dx = output\n",
    "        ctx.save_for_backward(x, dx)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, grad_dx):\n",
    "        x, dx = ctx.saved_tensors\n",
    "        result = grad_output * dx\n",
    "        # grad_dx=0，对原梯度计算没有影响\n",
    "        result += grad_dx * 6 * x\n",
    "        return result\n",
    "\n",
    "# Wrap MyCube in a function以便确定唯一的output值\n",
    "def my_cube(x):\n",
    "    result, dx = MyCube.apply(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ff75f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.119347Z",
     "start_time": "2024-08-10T14:29:49.338763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 例5：check gradient\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradcheck的输入要处理成a tuple of tensors\n",
    "input = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\n",
    "test = gradcheck(linear, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fac734c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.125060Z",
     "start_time": "2024-08-10T14:29:50.120918Z"
    }
   },
   "outputs": [],
   "source": [
    "# 例6：将forward和setup_context合并到forward中（not recommended）\n",
    "class LinearFunction(Function):\n",
    "    @staticmethod\n",
    "    # ctx is the first argument to forward\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        # The forward pass can use ctx.\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa43a7",
   "metadata": {},
   "source": [
    "## 2. Extending torch.nn\n",
    "nn输出两种类型的interface：modules和他们的functional version。\\\n",
    "extend nn可以用上述两种方式，但建议：\\\n",
    "(1)如果layer中有parameters或者buffers，用module。如：conv, affine. \\\n",
    "(2)如果没有，用function，比如activation function, pooling, relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08485b08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.140482Z",
     "start_time": "2024-08-10T14:29:50.127857Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # 1. nn.Parameter is a special kind of Tensor, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. \n",
    "        # 2. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # 3. nn.Parameters require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.empty(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        nn.init.uniform_(self.weight, -0.1, 0.1)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return LinearFunction.apply(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. \n",
    "        return 'input_features={}, output_features={}, bias={}'.format(\n",
    "            self.input_features, self.output_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7379a",
   "metadata": {},
   "source": [
    "## 3. Extending torch\n",
    "1. 目标：创建与tensor类似的自定义python type，既可以是与tensor所实现的操作相似但不相同的数据类型，也可以是tensor的子类型。在自定义的class中定义相应的同名methods，让pytorch中torch namespace里面原本接受tensor operands(操作数)的函数也能处理该自定义的数据类型。\n",
    "2. pytorch中提供的机制：如果自定义的python type class中定义了名为\\_\\_torch_function\\_\\_()的method，那么当自定义类型的实例作为参数传给torch namespace的函数的时候，pytorch就会invoke这个\\_\\_torch_function\\_\\_()。\\\n",
    "通过这种方式，可以自定义torch namespace中各种函数的implementation。当以自定义类型为参数来调用torch中的函数时，\\_\\_torch_function\\_\\_()实际上会调用这里自定义的implementation。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45941258",
   "metadata": {},
   "source": [
    "### 3.1 自定义与tensor类似的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "335ec6b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.147705Z",
     "start_time": "2024-08-10T14:29:50.141703Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例1：自定义一个2D scalar tensor\n",
    "#  1. n*n matrix，每个元素都是一个tensor scalar\n",
    "#  2. 对角线上的n个元素的值相同，由arguments决定，其他都是0\n",
    "\n",
    "class ScalarTensor(object):\n",
    "    def __init__(self, N, value):\n",
    "        self._N = N\n",
    "        self._value = value\n",
    "    def __repr__(self):\n",
    "        return f\"ScalarTensor(N={self._N}, value={self._value})\"\n",
    "    def tensor(self):\n",
    "        return self._value * torch.eye(self._N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8f2fa57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.157005Z",
     "start_time": "2024-08-10T14:29:50.148901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalarTensor(N=3, value=2)\n",
      "tensor([[2., 0., 0.],\n",
      "        [0., 2., 0.],\n",
      "        [0., 0., 2.]])\n"
     ]
    }
   ],
   "source": [
    "d = ScalarTensor(3, 2)\n",
    "print(d)\n",
    "print(d.tensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74adf36f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.160980Z",
     "start_time": "2024-08-10T14:29:50.158623Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  此时没有定义__torch_function__(),不支持torch operation\n",
    "# d.mean() # error:'ScalarTensor' object has no attribute 'mean'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43208639",
   "metadata": {},
   "source": [
    "### 让自定义类型支持自定义torch operation\n",
    "1. 定义__torch_function__()，按照规则定义好4个参数 \\\n",
    "(1)func: reference to the torch API function that is being overridden \\\n",
    "(2)types: 说明__torch_function__ 支持的type list\\\n",
    "(3)args: 传递给function的arguments tuple \\\n",
    "(4)kwargs: 传递给function的keyword arguments dict\n",
    "2. 定义该类型数据上与torch operation匹配的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acd1d81f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.167026Z",
     "start_time": "2024-08-10T14:29:50.162191Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2：让自定义类型支持自定义torch operation: torch.mean()\n",
    "\n",
    "# 1.指定该数据类型implement的operation，函数名要与torch namespace中的匹配\n",
    "HANDLED_FUNCTIONS = {}\n",
    "\n",
    "class ScalarTensor(object):\n",
    "    def __init__(self, N, value):\n",
    "        self._N = N\n",
    "        self._value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n",
    "\n",
    "    def tensor(self):\n",
    "        return self._value * torch.eye(self._N)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        if func not in HANDLED_FUNCTIONS or not all(\n",
    "            issubclass(t, (torch.Tensor, ScalarTensor))\n",
    "            for t in types\n",
    "        ):\n",
    "            return NotImplemented\n",
    "        return HANDLED_FUNCTIONS[func](*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca554bb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.170996Z",
     "start_time": "2024-08-10T14:29:50.168186Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2.定义operation的implementation，这里是torch.mean\n",
    "import functools\n",
    "\n",
    "# 定义一个decorator\n",
    "def implements(torch_function):\n",
    "    \"\"\"Register a torch function override for ScalarTensor\"\"\"\n",
    "    def decorator(func):\n",
    "        functools.update_wrapper(func, torch_function)\n",
    "        HANDLED_FUNCTIONS[torch_function] = func\n",
    "        return func\n",
    "    return decorator\n",
    "\n",
    "# 用decorator来实现具体函数的implementation\n",
    "@implements(torch.mean)\n",
    "def mean(input):\n",
    "    return float(input._value) / input._N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63800bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.176545Z",
     "start_time": "2024-08-10T14:29:50.172061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = ScalarTensor(5, 2)\n",
    "torch.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1af09377",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.180667Z",
     "start_time": "2024-08-10T14:29:50.177545Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. 函数operand有多个，其中有的是tensor，有的是自定义的类型\n",
    "def ensure_tensor(data):\n",
    "    # torch.tensor()通过深拷贝数据，构造一个新tensor\n",
    "    if isinstance(data, ScalarTensor):\n",
    "        return data.tensor()\n",
    "    # as_tensor可以将python对象转换为tensor\n",
    "    return torch.as_tensor(data)\n",
    "\n",
    "@implements(torch.add)\n",
    "def add(input, other):\n",
    "    # 如果输入都是ScalarTensor，加总两个底层tensor\n",
    "    try:\n",
    "        if input._N == other._N:\n",
    "            return ScalarTensor(input._N, input._value + other._value)\n",
    "        else:\n",
    "            raise ValueError(\"Shape mismatch!\")\n",
    "    # 如果两种输入中有ScalarTensor又有Tensor，要将scalarTensor转变成tensor\n",
    "    except AttributeError:\n",
    "        return torch.add(ensure_tensor(input), ensure_tensor(other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6c7df89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.185027Z",
     "start_time": "2024-08-10T14:29:50.181813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalarTensor(N=2, value=4)\n",
      "tensor([[3., 1.],\n",
      "        [1., 3.]])\n"
     ]
    }
   ],
   "source": [
    "s = ScalarTensor(2, 2)\n",
    "print(torch.add(s, s))\n",
    "\n",
    "t = torch.tensor([[1, 1,], [1, 1]])\n",
    "print(torch.add(s, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d707ceb",
   "metadata": {},
   "source": [
    "### 处理torch namespace中有，但是没有自定义函数implementation的情形\n",
    "1. 如果对自定义类型用torch namespace中的函数，但没有implement对应函数，会报错\n",
    "2. 一种不报错的处理方式是，判断自定义类型中有tensor method，调用该method把tensor传给torch原函数处理。<font color=red>注意这是因为自定义类型中一般都有tensor method。此时也要判断原生的运算规则是否符合自定义类型本身的需求。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b219e5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.187960Z",
     "start_time": "2024-08-10T14:29:50.186276Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.mul(s, 3) # error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a40d0b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.193027Z",
     "start_time": "2024-08-10T14:29:50.189055Z"
    }
   },
   "outputs": [],
   "source": [
    "# invoke tensor method生成tensor对象后，调用torch原有的函数\n",
    "class ScalarTensor(object):\n",
    "    def __init__(self, N, value):\n",
    "        self._N = N\n",
    "        self._value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n",
    "\n",
    "    def tensor(self):\n",
    "        return self._value * torch.eye(self._N)\n",
    "    \n",
    "    # 改变__torch_function__中的执行逻辑\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        if func not in HANDLED_FUNCTIONS or not all(\n",
    "                issubclass(t, (torch.Tensor, ScalarTensor))\n",
    "                for t in types\n",
    "            ):\n",
    "            args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]\n",
    "            return func(*args, **kwargs)\n",
    "        return HANDLED_FUNCTIONS[func](*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfa29e76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.197278Z",
     "start_time": "2024-08-10T14:29:50.194011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 0.],\n",
       "        [0., 4.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ScalarTensor(2, 2)\n",
    "torch.mul(s, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7410517",
   "metadata": {},
   "source": [
    "### 3.2 自定义tensor的子类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31159e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.649255Z",
     "start_time": "2024-08-10T14:29:50.198389Z"
    }
   },
   "source": [
    "略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7fb2f",
   "metadata": {},
   "source": [
    "### 3.3 自定义tensor wrapper类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6af0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T14:29:50.650696Z",
     "start_time": "2024-08-10T14:29:50.650686Z"
    }
   },
   "source": [
    "略"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
