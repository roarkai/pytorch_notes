{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7029e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.654214Z",
     "start_time": "2024-08-12T15:37:31.302160Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchviz\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a5963b",
   "metadata": {},
   "source": [
    "# Hooks\n",
    "参考资料：\n",
    "1. pytorch101系列5，understanding hooks https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/\n",
    "2. 官方文档：\n",
    "   - hooks for saved tensors\n",
    "   - autograd mechanics中的Hooks for saved tensors\n",
    "   - modules中的module hooks部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d289081",
   "metadata": {},
   "source": [
    "#### 基本概念\n",
    "- **什么是hook**\n",
    "  - hook是一个特殊函数，它在autograd.Function的forward或backward method被调用前，或者计算结束后被执行。用来在常规的forward/backward pass中执行任意的code。在不调整forward/backward method定义，从而不会影响autograd machine的执行逻辑的条件下，完成hook Function中指定的‘支线任务’。\n",
    "\n",
    "- **hooks的类型**:\n",
    "  - 按照接入autograd过程的位置，可以分为：\n",
    "    - forward hook: 在forward method计算开始前或计算完成后执行\n",
    "    - backward hook：在backward method计算开始前或计算完成后执行\n",
    "  - 按照作用对象，可以分为3种，其中两种都用在tensor对象上：\n",
    "    - 作用在tensor上的tentor.register_hook()，torch.Tensor.register_post_accumulate_grad_hook()，torch.autograd.graph.Node.register_hook()，torch.autograd.graph.Node.register_prehook()\n",
    "    - 专门用在graph中saved_tensor上的torch.autograd.graph.saved_tenors_hooks()\n",
    "    - 作用在module上的torch.nn.module.register_module_(forward_/bacward_)hook\n",
    "\n",
    "- **hook的使用对象**：tensor或者nn.Module\n",
    "  - **on tensor**：用于tensor的hook只有backward hook\n",
    "  - **on nn.Module：** 既有forward又有backward hook\n",
    "  - 总的来说，他们能实现的功能类似，只不过tensor hook定义在tensor上，module hook一般定义在layer上，对整个layer中的tensor都起作用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e45f28",
   "metadata": {},
   "source": [
    "- **用法**：\n",
    "  1. saved_tensor上的hook功能明确，一般用于将forward method向backward method传递的tensor转移到GPU之外的存储介质上，以节省memory space\n",
    "  2. 另外两种hook一个用于generic tensor，另一个用于module。\n",
    "     1. tensor.register_hook(hookfunc)只有backward hook，在每次register的tensor在backward pass中计算出gradient值之后被执行。\n",
    "     2. module类的hook既有forward hook，又有backward hook\n",
    "        1. forward hook定义方式是hook_name(module, input, output)。可见作用于module的input和output上。\n",
    "        2. backward hook定义方式是hook_name(module, grad_input, grad_output)。可见作用于module的grad_input和grad_output上。\n",
    "- 基本的做用是：**inspect或者modify** input, output, 或者gradient tensors\n",
    "- 具体场景有：\n",
    "  1. 查看gradient或者forward计算的intermediate result，比如activation的信息(value, shape)用来做模型分析。比如：\n",
    "     - debug，比如看gradient有没有vanishing/exploding\n",
    "     - feature visualization：用intermediate activations的值\n",
    "     - get feature map statistics：查看acitvation的mean, std, max/min\n",
    "  2. gradient manipulation。这个可以用来实现：\n",
    "     - gradient clipping\n",
    "     - custom regularization：用自定义的方式限制weight的值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd58d23",
   "metadata": {},
   "source": [
    "## I. Hooks for generic tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb6f39",
   "metadata": {},
   "source": [
    "#### 1. 打印intermediate value的grad信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1957327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.659396Z",
     "start_time": "2024-08-12T15:37:32.656453Z"
    }
   },
   "outputs": [],
   "source": [
    "#  定义hook function\n",
    "def printgrad(grad):\n",
    "    return print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291c2df1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.702922Z",
     "start_time": "2024-08-12T15:37:32.660698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8891,  0.5235,  0.4592])\n"
     ]
    }
   ],
   "source": [
    "# 如果不用hook就要设置retain_grad参数，会占用额外的memory\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "loss = (y ** 2).sum()\n",
    "\n",
    "y.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03f98c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.710497Z",
     "start_time": "2024-08-12T15:37:32.705248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8891,  0.5235,  0.4592])\n"
     ]
    }
   ],
   "source": [
    "# 用了hook就不需要设置\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "loss = (y ** 2).sum()\n",
    "\n",
    "y.register_hook(printgrad)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655994d7",
   "metadata": {},
   "source": [
    "#### 2. 修改gradient计算方式\n",
    "- 使用hook修改gradient的计算方式后，autograd能自动将修改后的值向前传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2298021",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.714750Z",
     "start_time": "2024-08-12T15:37:32.712198Z"
    }
   },
   "outputs": [],
   "source": [
    "#  定义hook function\n",
    "def doublegrad(grad):\n",
    "    return grad * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054152b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.720869Z",
     "start_time": "2024-08-12T15:37:32.716260Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8891,  0.5235,  0.4592])\n",
      "w: tensor([-0.9064, -0.9064, -0.9064, -0.9064, -0.9064])\n"
     ]
    }
   ],
   "source": [
    "# 原函数\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "loss = (y ** 2).sum()\n",
    "\n",
    "y.register_hook(printgrad)\n",
    "\n",
    "loss.backward()\n",
    "print('w:', w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51d8637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.726754Z",
     "start_time": "2024-08-12T15:37:32.722101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([-3.7783,  1.0471,  0.9184])\n",
      "w: tensor([-0.9064, -0.9064, -0.9064, -0.9064, -0.9064])\n"
     ]
    }
   ],
   "source": [
    "# 如果不用hook，而是在retain_grad之后直接修改y的gradient：\n",
    "# 修改后的值不会向前传递，forward中前序参数的梯度值没有变化\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "loss = (y ** 2).sum()\n",
    "\n",
    "y.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "y.grad *= 2\n",
    "print('y:', y.grad)\n",
    "print('w:', w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a733af9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.732995Z",
     "start_time": "2024-08-12T15:37:32.728363Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.7783,  1.0471,  0.9184])\n",
      "w: tensor([-1.8128, -1.8128, -1.8128, -1.8128, -1.8128])\n"
     ]
    }
   ],
   "source": [
    "# 改成hook之后，hook函数的输出会代替y的grad在autograd执行过程中使用\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "loss = (y ** 2).sum()\n",
    "\n",
    "y.register_hook(doublegrad) # hook for gradient manipulation\n",
    "y.register_hook(printgrad) # hook for print\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('w:', w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a30629",
   "metadata": {},
   "source": [
    "#### 3. 对module中指定的tensor做gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0750a030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.742365Z",
     "start_time": "2024-08-12T15:37:32.734215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "bias_grad in linear layer: tensor([-0.2000, -0.2000, -0.2000, -0.2000, -0.2000])\n"
     ]
    }
   ],
   "source": [
    "# ----------------  原模型  ----------------\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = lambda x: x.view(-1)\n",
    "        self.fc = nn.Linear(160, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv(x))         \n",
    "        return self.fc(self.flatten(x))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "x = torch.randn(1, 3, 8, 8)\n",
    "net = TestNet()\n",
    "out = net(x)\n",
    "loss = (1 - out).mean()\n",
    "loss.backward()\n",
    "\n",
    "print(out.shape)\n",
    "# bias_grad in linear layer should be -1/out.shape\n",
    "print('bias_grad in linear layer:', net.fc.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd1605",
   "metadata": {},
   "source": [
    "- <font color=blue>**给tensor加入backward hook后，Backward pass的执行顺序**</font>\n",
    "  1. 从root开始按照chainrule执行backward pass\n",
    "  2. 遇到hook后，对制定tensor的grad执行相应的ops，如果同一位置有多个hook，按他们在module中出现的顺序执行操作，而不是想chainrule那样反向操作。如下例：\\\n",
    "     - fc layer backward method     -> \n",
    "     - flatten layer                ->   \n",
    "     - register for clamp           ->\n",
    "     - register for print shape     -> \n",
    "     - register for check gradient  -> \n",
    "     - relu layer backward method   -> \n",
    "     - conv layer backward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28e03686",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.753271Z",
     "start_time": "2024-08-12T15:37:32.745269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of relu output: torch.Size([1, 10, 4, 4])\n",
      "any grad < 0? False\n",
      "bias_grad in linear layer: tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "#    1. 将linear层中bias的梯度改为0\n",
    "#    2. conv layer从downstream拿到的gradient大小都不小于0\n",
    "\n",
    "# ----------------  加入hook  ----------------\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = lambda x: x.view(-1)\n",
    "        self.fc = nn.Linear(160, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        # 设置hook：让x.grad >= 0\n",
    "        # 处理的是relu的output,因为relu的梯度只有0和1，所以处理relu之后的x即可\n",
    "        x.register_hook(lambda grad: torch.clamp(grad, min=0))\n",
    "        \n",
    "        x.register_hook(lambda grad: print('size of relu output:', grad.shape))\n",
    "        # 再加一个hook确认有没有x.grad是负值\n",
    "        # 这里改变的是relu运算后的x.grad，穿透relu的0，1mask，传给conv的仍然>0\n",
    "        x.register_hook(lambda grad: print('any grad < 0?', \\\n",
    "                                           bool((grad < 0).any())))\n",
    "        x = self.flatten(x)\n",
    "        y = self.fc(x)\n",
    "        return y    \n",
    "    \n",
    "torch.manual_seed(2)\n",
    "x = torch.randn(1, 3, 8, 8)\n",
    "net = TestNet()\n",
    "\n",
    "# 在model外部，给参数tenosr设置hook: 这里将linear层中bias的梯度改为0\n",
    "# 这种方式可以在不改变module定义的条件下设置hook\n",
    "for name, param in net.named_parameters():\n",
    "    if 'fc' in name and 'bias' in name:\n",
    "        param.register_hook(lambda grad: torch.zeros(grad.shape))\n",
    "\n",
    "out = net(x)\n",
    "loss = (1 - out).mean()\n",
    "loss.backward()\n",
    "\n",
    "# 确认bias的grad成功改成了全0\n",
    "print('bias_grad in linear layer:', net.fc.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242b2bb",
   "metadata": {},
   "source": [
    "#### Whether a particular hook will be fired\n",
    "1. Hooks registered to a Tensor via torch.Tensor.register_hook() are executed when gradients are being computed for that Tensor. (Note that this does not require the Tensor’s grad_fn to be executed. For example, if the Tensor is passed as part of the inputs argument to torch.autograd.grad(), the Tensor’s grad_fn may not be executed, but the hook register to that Tensor will always be executed.)\n",
    "2. Hooks registered to a Tensor via torch.Tensor.register_post_accumulate_grad_hook()在完成梯度累积操作后执行。\n",
    "\n",
    "3. Hooks registered to torch.autograd.graph.Node using torch.autograd.graph.Node.register_hook() or torch.autograd.graph.Node.register_prehook() are only fired if the Node it was registered to is executed.\n",
    "\n",
    "\n",
    "- 用torch.Tensor.register_hook()登记的hook在每次gradients计算的时候都要执行, 但用torch.Tensor.register_post_accumulate_grad_hook()登记的hook只在backward pass的最后，完成梯度累积后执行一次。因此，post-accumulate-grad hooks的作用对象只能是leaf Tensors. 登记在非leaf tensor上会报错。\n",
    "\n",
    "- Whether a particular Node is executed may depend on whether the backward pass was called with torch.autograd.grad() or torch.autograd.backward(). Specifically, you should be aware of these differences when you register a hook on a Node corresponding to a Tensor that you are passing to torch.autograd.grad() or torch.autograd.backward() as part of the inputs argument.\n",
    "\n",
    "- 如果用的是torch.autograd.backward(), 上述所有hook类型都会被执行, whether or not you specified the inputs argument. 因为.backward() executes all Nodes, even if they correspond to a Tensor specified as an input. (Note that the execution of this additional Node corresponding to Tensors passed as inputs is usually unnecessary, but done anyway. This behavior is subject to change; you should not depend on it.)\n",
    "\n",
    "- 如果用的是torch.autograd.grad(), the backward hooks registered to Nodes that correspond to the Tensors passed to input may not be executed, because those Nodes will not be executed unless there is another input that depends on the gradient result of this Node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e936595",
   "metadata": {},
   "source": [
    "#### The order in which the different hooks are fired\n",
    "1. hooks registered to Tensor are executed\n",
    "2. pre-hooks registered to Node are executed (if Node is executed).\n",
    "3. the .grad field is updated for Tensors that retain_grad\n",
    "4. Node is executed (subject to rules above)\n",
    "5. for leaf Tensors that have .grad accumulated, post-accumulate-grad hooks are executed\n",
    "6. post-hooks registered to Node are executed (if Node is executed)\n",
    "\n",
    "If multiple hooks of the same type are registered on the same Tensor or Node they are executed in the order in which they are registered. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc33edc",
   "metadata": {},
   "source": [
    "## II. Hooks for nn.Module objects\n",
    "- <font color=red>用于nn.Module的hook要注意module中可能涉及多个nn.Function，因此对应多个forward/backward calls，hook的作用会涉及多个call，如果不清楚有的pytorch自定义的module是如何工作的，hook的设置会容易弄错，所以要慎重。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de807d1",
   "metadata": {},
   "source": [
    "### II.1 register hooks\n",
    "#### II.1.1 register forward hooks\n",
    "forward hook在forward pass中被调用，具体又有两种执行位置，**pre_hook**在forward method之前执行；**hook**在forward method执行完之后执行。\\\n",
    "下面1和2只对当前hook所reigster上的module有效。3和4是global hook，也就是installed for all modules。\n",
    "1. <font color=green>**register_forward_pre_hook(hook_func_name)**</font>\n",
    "2. <font color=green>**register_forward_hook(hook_func_name)**</font>\n",
    "3. <font color=green>**register_module_forward_pre_hook(hook_func_name)**</font>\n",
    "4. <font color=green>**register_module_forward_hook(hook_func_name)**</font>："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d83818",
   "metadata": {},
   "source": [
    "#### II.1.2 register backward hooks\n",
    "backward hook只有一种，在backward method执行完后执行。下面1和2只对当前hook所reigster上的module有效。3和4是global hook，也就是installed for all modules。\n",
    "1. <font color=green>**register_full_backward_pre_hook(hook_func_name)**</font>\n",
    "2. <font color=green>**register_full_backward_hook(hook_func_name)**</font>：这个是原register_backward_hook()\n",
    "3. <font color=green>**register_module_full_backward_hook(hook_func_name)**</font>\n",
    "4. <font color=green>**register_module_full_backward_pre_hook(hook_func_name)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3387227",
   "metadata": {},
   "source": [
    "### II.2 用法举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c94ff382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.757489Z",
     "start_time": "2024-08-12T15:37:32.754549Z"
    }
   },
   "outputs": [],
   "source": [
    "### 例1：定义不同的hooks，看他们的工作方式\n",
    "\n",
    "##  新建module和input\n",
    "m = nn.Linear(3, 3)\n",
    "torch.manual_seed(1)\n",
    "x = torch.randn(2, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7339d548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.760998Z",
     "start_time": "2024-08-12T15:37:32.758757Z"
    }
   },
   "outputs": [],
   "source": [
    "## 定义foreward hook function\n",
    "#  1.用于在forward pass前，检查或者调整inputs\n",
    "def forward_pre_hook(m, inputs):  # 注，inputs都是wrapped成tuple类型的\n",
    "    input = inputs[0]\n",
    "    return input + 1.\n",
    "\n",
    "#  2.用于在forward pass后，检查inputs/outputs或者调整outputs\n",
    "def forward_hook(m, inputs, output):# inputs都wrapped成tuple，output按原类型传\n",
    "    # 按ResNet的方式计算residual\n",
    "    return output + inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e0d2a3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.764451Z",
     "start_time": "2024-08-12T15:37:32.762002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3552,  0.0191,  0.7350],\n",
      "        [-0.3277,  0.2751,  0.4299]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## register前的输出\n",
    "print(format(m(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8088d594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.767966Z",
     "start_time": "2024-08-12T15:37:32.765484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6156, -0.1592,  1.2549],\n",
      "        [-0.5881,  0.0968,  0.9498]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# register forward_pre_hook后会产生不同的output：\n",
    "forward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)\n",
    "print(format(m(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c011d73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.771406Z",
     "start_time": "2024-08-12T15:37:32.768919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0457, 1.1077, 2.3166],\n",
      "        [1.0332, 0.6449, 1.7837]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# register forward_hook后会产生另一种不同的output：\n",
    "forward_hook_handle = m.register_forward_hook(forward_hook)\n",
    "print(format(m(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7458c052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.774885Z",
     "start_time": "2024-08-12T15:37:32.772414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3552,  0.0191,  0.7350],\n",
      "        [-0.3277,  0.2751,  0.4299]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 去掉hooks之后，output与register hooks之前的值一致\n",
    "forward_pre_hook_handle.remove()\n",
    "forward_hook_handle.remove()\n",
    "\n",
    "print(format(m(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b12f135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.779165Z",
     "start_time": "2024-08-12T15:37:32.775881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1376,  0.0587, -0.1150],\n",
      "        [ 0.1376,  0.0587, -0.1150]])\n"
     ]
    }
   ],
   "source": [
    "## 定义backward hook function\n",
    "#  功能：检查grad_inputs/grad_outputs或者调整剩余bp流程中用的grad_inputs\n",
    "#  注，grad_inputs/grad_outputs都wrapped成tuple\n",
    "def backward_hook(m, grad_inputs, grad_outputs): \n",
    "    new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]\n",
    "    return new_grad_inputs\n",
    "\n",
    "## 没有register backward hooks时的输出：\n",
    "m(x).sum().backward()\n",
    "print(format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acdf62c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.783617Z",
     "start_time": "2024-08-12T15:37:32.780320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[42., 42., 42.],\n",
      "        [42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 在重做Backward Propagation前要先Clear gradients\n",
    "m.zero_grad()\n",
    "x.grad.zero_()\n",
    "\n",
    "# register了backward hooks之后的输出：\n",
    "backward_hook_handle = m.register_full_backward_hook(backward_hook)\n",
    "m(x).sum().backward()\n",
    "print(format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02629b0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.787511Z",
     "start_time": "2024-08-12T15:37:32.784594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1376,  0.0587, -0.1150],\n",
      "        [ 0.1376,  0.0587, -0.1150]])\n"
     ]
    }
   ],
   "source": [
    "# 删除backward hooks之后的输出：\n",
    "backward_hook_handle.remove()\n",
    "\n",
    "m.zero_grad()\n",
    "x.grad.zero_()\n",
    "m(x).sum().backward()\n",
    "print(format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f277a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.791826Z",
     "start_time": "2024-08-12T15:37:32.788474Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2：用forward_hook查看module中activation value的例子\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.conv = nn.Conv2d(3,8,2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4,4))\n",
    "        self.fc = nn.Linear(8*4*4 , 1)\n",
    "    def forward(self, x):\n",
    "          x = F.relu(self.conv(x))\n",
    "          x = self.pool(x)\n",
    "          x = x.view(x.shape[0] , -1)\n",
    "          x = self.fc(x)\n",
    "          return x\n",
    "\n",
    "# 定义hook，用来提取activation的结果，也就是feature\n",
    "features = {} \n",
    "def hook_func(model, input ,output):\n",
    "    features['feature'] = output.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cbda9bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.796495Z",
     "start_time": "2024-08-12T15:37:32.792785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "# 在pooling layer上register hook\n",
    "net.pool.register_forward_hook(hook_func)\n",
    "\n",
    "x= torch.randn(1,3,10,10)\n",
    "output = net(x)\n",
    "print(features['feature'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf137ed",
   "metadata": {},
   "source": [
    "### II.3 典型应用场景\n",
    "#### 场景1. 在模型训练过程中打印所需信息\n",
    "**1. 方法：** 给module加wrapper，在wrapper module上加hooks\\\n",
    "**2. 优点：** \\\n",
    "（1）方便debug，避免手动增加和删除print的麻烦 \\\n",
    "（2）不仅可以在自定义module，还可以在pytorch自带的module和第三方module上使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec4d0f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:32.801073Z",
     "start_time": "2024-08-12T15:37:32.797867Z"
    }
   },
   "outputs": [],
   "source": [
    "## 场景1：在ResNet18上用hooks来打印model信息\n",
    "\n",
    "#  给model加一个wrapper class\n",
    "class VerboseNet(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "        # register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            # 虽然所有nn.Mudule的实例都继承了__name__属性，但有的module可能没赋值\n",
    "            # 这一步可以明确让layer都有对应的name\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer, _, output: print(f\"new --->: {layer.__name__}: {output.shape}\")\n",
    "            )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2506626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:33.512685Z",
     "start_time": "2024-08-12T15:37:32.802050Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roark/anaconda3/envs/231n/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/roark/anaconda3/envs/231n/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new --->: conv1: torch.Size([10, 64, 14, 14])\n",
      "new --->: bn1: torch.Size([10, 64, 14, 14])\n",
      "new --->: relu: torch.Size([10, 64, 14, 14])\n",
      "new --->: maxpool: torch.Size([10, 64, 7, 7])\n",
      "new --->: layer1: torch.Size([10, 64, 7, 7])\n",
      "new --->: layer2: torch.Size([10, 128, 4, 4])\n",
      "new --->: layer3: torch.Size([10, 256, 2, 2])\n",
      "new --->: layer4: torch.Size([10, 512, 1, 1])\n",
      "new --->: avgpool: torch.Size([10, 512, 1, 1])\n",
      "new --->: fc: torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "verbose_resnet = VerboseNet(resnet18(weights=ResNet18_Weights.DEFAULT))\n",
    "dummy_input = torch.ones(10, 3, 28, 28)\n",
    "\n",
    "# 这里用了assignment是为了不让jupyter打印函数的output\n",
    "_ = verbose_resnet(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b830b",
   "metadata": {},
   "source": [
    "#### 场景2. Feature extraction\n",
    "**1. 方法：** 给module加wrapper，在wrapper module上加hooks\\\n",
    "**2. 优点：** 在一个预训练好的模型上做transfer learning时，可能想查看该模型摸些layers上得到的feature。hook可以在不改变模型本身的条件下实现这一需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e02ff083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:33.519381Z",
     "start_time": "2024-08-12T15:37:33.514689Z"
    }
   },
   "outputs": [],
   "source": [
    "## 场景2：Feature extraction\n",
    "\n",
    "class FeatureExt(nn.Module):\n",
    "    def __init__(self, model, layers):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self._features = {layer: torch.empty(0) for layer in layers}\n",
    "        \n",
    "        for layer_i in layers:\n",
    "            # 把named_modules的sub-module list构造成dict，用layer_i索引\n",
    "            # [说明见下一个cell]\n",
    "            layer = dict([*self.model.named_modules()])[layer_i]\n",
    "            \n",
    "            # layer_i索引出来对应layer后，给该layer加上hook:\n",
    "            layer.register_forward_hook(self.save_outputs_hook(layer_i))\n",
    "         \n",
    "    # 定义hook function：\n",
    "    def save_outputs_hook(self, layer_i):\n",
    "        def fn(_, __, output):  # 下划线长度不同，因为arguments name不能相同\n",
    "            # 将该layer的output提取出来，存入feature dict\n",
    "            self._features[layer_i] = output\n",
    "        return fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _ = self.model(x)\n",
    "        return self._features  # 返回dict存放了指定layers的activation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5432737c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:33.818310Z",
     "start_time": "2024-08-12T15:37:33.520781Z"
    }
   },
   "outputs": [],
   "source": [
    "res18 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "## 对resnet.named_modules的说明：\n",
    "#  1. resnet18.named_modules是一个generator\n",
    "#     每次yield返回的是tuple，形如('layer_name', module)\n",
    "#  2. 加'*'来把generator解包成单独的tuples：*res18.named_modules()\n",
    "#  3. 将dict()用到list of tuples上，可以转换成key-value pair，便于用key索引\n",
    "#     这里要先将解包后的tuples打包到一个list里面，所以是用 dict([])\n",
    "\n",
    "#  ----- 打印sample layers 来查看网络的结构  -----\n",
    "# print([*res18.named_modules()][0]) # nest结构的顶层，描绘整个结构\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][5]) # nest结构的第2层，'layer1'\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][6]) # nest结构的第3层，'layer1'的sub-module\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][7]) # nest结构的第4层，..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "576976ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:33.842330Z",
     "start_time": "2024-08-12T15:37:33.820378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4 -> torch.Size([10, 512, 1, 1])\n",
      "avgpool -> torch.Size([10, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "res18_feats = FeatureExt(res18, layers=['layer4', 'avgpool'])\n",
    "\n",
    "fests = res18_feats(dummy_input)\n",
    "for name, output in fests.items():\n",
    "    print(name, '->', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c735ef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:57:53.554794Z",
     "start_time": "2024-08-12T15:57:53.535196Z"
    }
   },
   "outputs": [],
   "source": [
    "#### 3. 实现gradient clipping\n",
    "# 这部分代码由perplexity辅助生成\n",
    "\n",
    "def clip_grad(model, floor, cap):\n",
    "    for parameter in model.parameters():\n",
    "        parameter.register_hook(lambda grad: grad.clamp_(floor, cap))\n",
    "    return model\n",
    "\n",
    "class AffineReluAffine(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Example usage        \n",
    "if __name__ == \"__main__\":\n",
    "    # Create the model\n",
    "    model = AffineReluAffine(10, 20, 5)\n",
    "    \n",
    "    # Apply gradient clipping with floor = -1.0 and cap = 1.0\n",
    "    clip_grad(model, floor=-1.0, cap=1.0)\n",
    "    \n",
    "    # Create a random input tensor\n",
    "    input_tensor = torch.randn(1, 10)\n",
    "    output = model(input_tensor)\n",
    "    \n",
    "    # Assume some loss function and compute loss\n",
    "    loss = output.sum()  # Example loss\n",
    "    loss.backward()  # Compute gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aaff52",
   "metadata": {},
   "source": [
    "## III. hooks for saved tensors\n",
    "\n",
    "**典型应用：** 改变saved tensor pack/unpack方式，<font color=blue>将forward pass中要保存的tensor存到cpu或者disk上，节省GPU memory</font>\n",
    "\n",
    "**方法：** \n",
    "1. 定义两个hook function，一个实现pack，另一个unpack \n",
    "   1. 定义pack_func(tensor): 只接受1个tensor作为argument，返回任意python类型。 \n",
    "   2. 定义unpack_hook_func(output_of_pack_func)：只接受pack_func的返回值作为input argument，返回一个tensor用于后续的backward pass。该返回值的value要跟pack_func中的input的value相同，以达到原本想要从forward向backward传递信息的目的。\n",
    "2. register 上述pack/unpack hooks \n",
    "\n",
    "**基本原则：** \n",
    "1. uppack_func(pack_func(tensor)) = tensor\n",
    "2. pack_func的input不能做in-place modify\n",
    "3. pack_func(tensor)的output可以是tenosr或者任意python type object \n",
    "4. pack_func和unpack_func单独作用于每个saved tensor\n",
    "\n",
    "**执行过程：** \n",
    "1. 每次forward pass执行过程中，pack func在对应operation存储信息的时候被调用，其output会代替原本module中定义的pack func输出的output tensor而被存储。\n",
    "2. 在backward pass中按照chainrule执行到对应operation的backward method的之前，unpack func会被调用，它用pack func的output作为唯一的input来计算一个new tensor。这个new tensor会作为backward method的input之一而被使用。\n",
    "\n",
    "**涉及的3个tensor object的关系：**\n",
    "- pack(x)将x打包成另一个object，unpack输出的x同样也是新的tensor。如果pack(x)=x，那么他们虽然是三个不同的object，比如tensor，但share底层data memory。如果pack(x)不等于x，则x和unpack输出的x会share底层data memory。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51d40a61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:33.886999Z",
     "start_time": "2024-08-12T15:37:33.850705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packing: tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n",
      "packing: tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"222pt\" height=\"282pt\"\n",
       " viewBox=\"0.00 0.00 222.00 282.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 278)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-278 218,-278 218,4 -4,4\"/>\n",
       "<!-- 133530340792240 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>133530340792240</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"133.5,-31 79.5,-31 79.5,0 133.5,0 133.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 133530322175600 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>133530322175600</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 62,-86 62,-67 151,-67 151,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 133530322175600&#45;&gt;133530340792240 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>133530322175600&#45;&gt;133530340792240</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-66.79C106.5,-60.07 106.5,-50.4 106.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-41.19 106.5,-31.19 103,-41.19 110,-41.19\"/>\n",
       "</g>\n",
       "<!-- 133530322189760 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>133530322189760</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 133530322189760&#45;&gt;133530322175600 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>133530322189760&#45;&gt;133530322175600</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.5,-121.98C67.69,-114.23 80.01,-102.58 89.97,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.48,-95.59 97.34,-86.17 87.67,-90.5 92.48,-95.59\"/>\n",
       "</g>\n",
       "<!-- 133530333005280 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>133530333005280</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-207 23.5,-207 23.5,-177 77.5,-177 77.5,-207\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">a</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 133530333005280&#45;&gt;133530322189760 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>133530333005280&#45;&gt;133530322189760</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.84C50.5,-169.21 50.5,-159.7 50.5,-151.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.27 50.5,-141.27 47,-151.27 54,-151.27\"/>\n",
       "</g>\n",
       "<!-- 133530322177616 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>133530322177616</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-141 119,-141 119,-122 208,-122 208,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 133530322177616&#45;&gt;133530322175600 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>133530322177616&#45;&gt;133530322175600</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.34,-121.98C146,-114.23 133.47,-102.58 123.32,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.53,-90.42 115.82,-86.17 120.76,-95.54 125.53,-90.42\"/>\n",
       "</g>\n",
       "<!-- 133530322186208 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>133530322186208</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-201.5 113,-201.5 113,-182.5 214,-182.5 214,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 133530322186208&#45;&gt;133530322177616 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>133530322186208&#45;&gt;133530322177616</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-182.37C163.5,-174.25 163.5,-161.81 163.5,-151.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-151.17 163.5,-141.17 160,-151.17 167,-151.17\"/>\n",
       "</g>\n",
       "<!-- 133530322065808 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>133530322065808</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-274 136.5,-274 136.5,-243 190.5,-243 190.5,-274\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 133530322065808&#45;&gt;133530322186208 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>133530322065808&#45;&gt;133530322186208</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-242.86C163.5,-233.68 163.5,-221.75 163.5,-211.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-211.82 163.5,-201.82 160,-211.82 167,-211.82\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x79731c58b610>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 例1：saved_tensor_hooks工作过程\n",
    "#  定义hook func\n",
    "def pack(x):\n",
    "    print('packing:', x)\n",
    "    return x\n",
    "def unpack(x):\n",
    "    print('unpacking:', x)\n",
    "    return x\n",
    "\n",
    "a = torch.ones(5, requires_grad=True)\n",
    "b = torch.ones(5, requires_grad=True) * 2\n",
    "\n",
    "# register pack/unpack hooks\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = a * b\n",
    "\n",
    "# 从print结果可以看到：\n",
    "# 1.第1个packing的tensor是a\n",
    "# 2.第2个packing的tensor是b，但b不是leaf，它是torch.ones(...)*2的output\n",
    "torchviz.make_dot(y, params={'a':a, 'b':b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c9e909a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:33.892497Z",
     "start_time": "2024-08-12T15:37:33.888574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpacking: tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n",
      "unpacking: tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b39d50ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:33.900123Z",
     "start_time": "2024-08-12T15:37:33.893883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([0.8033, 0.1748, 0.0890], requires_grad=True)\n",
      "packing: tensor([3.2131, 0.6993, 0.3559])\n",
      "unpacking: tensor([0.8033, 0.1748, 0.0890])\n"
     ]
    }
   ],
   "source": [
    "## 例2.1：自定义pack和unpack规则：改变tensor大小后恢复\n",
    "#  pack/unpack func满足“unpack(pack(x)) = x”的规则即可\n",
    "\n",
    "# ---> 例2.1和例2.2没有实际意义，只是展示pack和unpack的自定义能力\n",
    "\n",
    "def pack(x):\n",
    "    print('packing:', x * 4)\n",
    "    return x * 4\n",
    "def unpack(x):\n",
    "    print('unpacking:', x / 4)\n",
    "    return x / 4\n",
    "# pack/unpack func满足“unpack(pack(x)) = x”的规则\n",
    "\n",
    "torch.manual_seed(3)\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print('x =', x)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = x ** 2\n",
    "    \n",
    "y.sum().backward()\n",
    "assert(x.grad.equal(x * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39ecda1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:33.905671Z",
     "start_time": "2024-08-12T15:37:33.901337Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2.2：自定义pack和unpack规则：保存index of a list\n",
    "\n",
    "storage = []\n",
    "\n",
    "def pack(x):\n",
    "    storage.append(x)\n",
    "    return len(storage) - 1\n",
    "\n",
    "def unpack(ind):\n",
    "    return storage[ind]\n",
    "\n",
    "torch.manual_seed(3)\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = x ** 2\n",
    "    \n",
    "y.sum().backward()\n",
    "assert(x.grad.equal(x * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adbf8fc",
   "metadata": {},
   "source": [
    "#### 用pack/unpack将tensor存到GPU之外的地方\n",
    "1. 这是GPU memory和time的trade off。官方样例中，用A100GPU测试，把ResNet152(batch size=256)存到cpu上，可以将gpu内存使用量从48G降低到5G,但是耗时增加6倍\n",
    "2. 一种折中是只把部分layer的tensor传到cpu或者其他位置。方法是，define a special nn.Module，用来wraps module and save its tensors to cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa8b94c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:33.912932Z",
     "start_time": "2024-08-12T15:37:33.906838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 例3：saving tensor to cpu\n",
    "\n",
    "#  1.人工手写\n",
    "def pack(x):\n",
    "    return (x.device, x.cpu())\n",
    "\n",
    "def unpack(package):\n",
    "    device, x = package\n",
    "    return x.to(device)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = x ** 2\n",
    "    \n",
    "y.sum().backward()\n",
    "# assert(x.grad.equal(x * 2))\n",
    "torch.allclose(x.grad, (2 * x))  # x.grad与2x值的差异在默认区间内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d37fbb5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:34.059903Z",
     "start_time": "2024-08-12T15:37:33.914356Z"
    }
   },
   "outputs": [],
   "source": [
    "#  2.pytorch已经实现了上述功能\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(5))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.autograd.graph.save_on_cpu(pin_memory=True):\n",
    "            return self.w * x\n",
    "\n",
    "x = torch.randn(5)\n",
    "model = Model()\n",
    "loss = model(x).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bad29cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:34.067167Z",
     "start_time": "2024-08-12T15:37:34.061513Z"
    }
   },
   "outputs": [],
   "source": [
    "#  3.module wrapper\n",
    "\n",
    "class SaveToCpu(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        with torch.autograd.graph.save_on_cpu(pin_memory=True):\n",
    "            return self.module(*args, **kwargs)\n",
    "        \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 100), \n",
    "    nn.ReLU(), \n",
    "    SaveToCpu(nn.Linear(100, 100)), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(100, 10),\n",
    ")\n",
    "\n",
    "x = torch.randn(10)\n",
    "loss = model(x).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98459c50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:34.080109Z",
     "start_time": "2024-08-12T15:37:34.068411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Double access failed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27394/2057767111.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensor = torch.load(name)\n"
     ]
    }
   ],
   "source": [
    "## 例4：saving tensor to disk\n",
    "\n",
    "## 错误的方式\n",
    "\n",
    "import uuid\n",
    "import os\n",
    "import tempfile\n",
    "tmp_dir_obj = tempfile.TemporaryDirectory()\n",
    "tmp_dir = tmp_dir_obj.name\n",
    "\n",
    "def pack_hook(tensor):\n",
    "    name = os.path.join(tmp_dir, str(uuid.uuid4()))\n",
    "    torch.save(tensor, name)\n",
    "    return name\n",
    "\n",
    "def unpack_hook(name):\n",
    "    tensor = torch.load(name)\n",
    "    os.remove(name)  # 如果这里remove，那么unpack就不能被call第二次\n",
    "    return tensor\n",
    "\n",
    "x = torch.ones(5, requires_grad=True)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "    y = x.pow(2)\n",
    "print(y.grad_fn._saved_self)      # 每执行一次就会unpack一次\n",
    "try:\n",
    "    print(y.grad_fn._saved_self)  # 第二次会失败\n",
    "    print(\"Double access succeeded!\")\n",
    "except:\n",
    "    print(\"Double access failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b639dba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:34.085437Z",
     "start_time": "2024-08-12T15:37:34.081435Z"
    }
   },
   "outputs": [],
   "source": [
    "## 正确的方式：利用pytorch自动释放saved data的机制\n",
    "#  pytorch自动释放不再需要的object，即这里的SelfDeletingTempFile object\n",
    "\n",
    "class SelfDeletingTempFile():\n",
    "    def __init__(self):\n",
    "        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n",
    "\n",
    "    def __del__(self):\n",
    "        os.remove(self.name)\n",
    "\n",
    "def pack_hook(tensor):\n",
    "    temp_file = SelfDeletingTempFile()\n",
    "    torch.save(tensor, temp_file.name)\n",
    "    return temp_file\n",
    "\n",
    "def unpack_hook(temp_file):\n",
    "    return torch.load(temp_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a96fd760",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:34.112712Z",
     "start_time": "2024-08-12T15:37:34.086855Z"
    }
   },
   "outputs": [],
   "source": [
    "# 只转存size >= 1000的tensor\n",
    "SAVE_ON_DISK_THRESHOLD = 1000\n",
    "\n",
    "def pack_hook(x):\n",
    "    if x.numel() < SAVE_ON_DISK_THRESHOLD:\n",
    "        return x\n",
    "    temp_file = SelfDeletingTempFile()\n",
    "    torch.save(tensor, temp_file.name)\n",
    "    return temp_file\n",
    "\n",
    "def unpack_hook(tensor_or_sctf):\n",
    "    if isinstance(tensor_or_sctf, torch.Tensor):\n",
    "        return tensor_or_sctf\n",
    "    return torch.load(tensor_or_sctf.name)\n",
    "\n",
    "class SaveToDisk(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "            return self.module(*args, **kwargs)\n",
    "\n",
    "net = nn.DataParallel(SaveToDisk(Model()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687083bb",
   "metadata": {},
   "source": [
    "上面例子方式定义的Hook是thread-local的，当与DataParallel一起用的时候要用上例中的方式，不能定义成下面的方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49e97f0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T15:37:34.116814Z",
     "start_time": "2024-08-12T15:37:34.114428Z"
    }
   },
   "outputs": [],
   "source": [
    "# net = nn.DataParallel(model)\n",
    "# with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "#     output = net(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
