{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a5963b",
   "metadata": {},
   "source": [
    "# Hooks\n",
    "**1. 什么是hook？**\n",
    "hook是一个特殊的<font color=blue>函数</font>，它在autograd.Function的forward或backward method调用的前后被执行。hooks可以用来在常规module的forward/backward pass中执行任意的code，或者用来调整inputs/outputs而不用改变module本身的forward() method.\\\n",
    "**2. hook的使用对象？**\n",
    "hook可以<font color=blue>用于**tensor或module**</font>，称为register on a tensor or nn.Module。典型使用场景：\\\n",
    "· <font color=green>用于tensor的时候主要用来控制从forward向backward传递信息过程中的pack/unpack information。</font>\\\n",
    "· <font color=green>用于module中的时候，可以用于模型可视化，debug，gradient check等。</font> \\\n",
    "**3. hooks的类型？**\n",
    "使用hook，要先register到使用hook的位置。按照使用位置，pytorch提供了两大类hooks：forward hook和backward hook。 \\\n",
    "**4. hooks的作用范围？**\n",
    "所有hooks都可以返回更新后的value，这些value也会被用到hooks所在pass的后续计算中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd58d23",
   "metadata": {},
   "source": [
    "## 1. Hooks for tensors\n",
    "用于tensor的hook只有backward hook。\\\n",
    "**signature of hook function：**<font color=red>hook_func_name(grad) -> Tensor or None </font>\\\n",
    "**method for register a backward hook**：<font color=red>tensor_name.register_hook(hook_func_name)</font> \\\n",
    "**典型用途：**\n",
    "1. <font color=green>改变tensor的梯度计算方式。</font>某个tensor x的x.grad用hook改变后，DAG上位于x前面，依赖于x的tensor的梯度也按chainrule改变。如果不用tensor hook，要等bp结束后手动改变x和每一个前序tensor的梯度。\n",
    "2. <font color=green>查看intermediate tensor的grad，此时不占额外内存。</font>不用tensor hook的话就要用intermediate_tensor.remain_grad()，会占用内存。\n",
    "3. <font color=green>自定义saved tensor的pack/unpack方式</font>\n",
    "\n",
    "**backward hooks execution：** [详见Pytorch Doc:autograd mechnism]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7029e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.107101Z",
     "start_time": "2023-10-26T04:04:56.874201Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchviz\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd175d",
   "metadata": {},
   "source": [
    "### 1.1 直接改变tensor/tensor.grad value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1957327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.112250Z",
     "start_time": "2023-10-26T04:04:58.109405Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例1：一个简单的例子\n",
    "# 定义hook function\n",
    "def func(grad):\n",
    "    return grad * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291c2df1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.148178Z",
     "start_time": "2023-10-26T04:04:58.113663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9064, -0.9064, -0.9064, -0.9064, -0.9064])\n",
      "tensor([-1.8891,  0.5235,  0.4592])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 1.without hook\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "y.retain_grad()\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03f98c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.156902Z",
     "start_time": "2023-10-26T04:04:58.150589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8128, -1.8128, -1.8128, -1.8128, -1.8128])\n",
      "tensor([-3.7783,  1.0471,  0.9184])\n",
      "tensor([-3.7783,  1.0471,  0.9184])\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# 2.with hook for loss\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "y.retain_grad()\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "#  register hook\n",
    "loss.register_hook(func)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# 因为loss的gradient翻倍，导致前向传递，w和b的梯度也翻倍\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "print(y.grad)\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a733af9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.164077Z",
     "start_time": "2023-10-26T04:04:58.158381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8128, -1.8128, -1.8128, -1.8128, -1.8128])\n",
      "tensor([-1.8891,  0.5235,  0.4592])\n",
      "tensor([-1.8891,  0.5235,  0.4592])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 3.with hook for w\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "y.retain_grad()\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "#  register hook for w\n",
    "w.register_hook(func)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# 因为w的gradient翻倍，前向传递不影响loss,y和b\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "print(y.grad)\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d36f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.168243Z",
     "start_time": "2023-10-26T04:04:58.165498Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2：Gradient clipping\n",
    "def grad_clipper(model, val):\n",
    "    for parameter in model.parameters():\n",
    "        parameter.register_hook(lambda grad: grad.climp_(floor, cap))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0750a030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.178830Z",
     "start_time": "2023-10-26T04:04:58.169453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "bias_grad in linear layer: tensor([-0.2000, -0.2000, -0.2000, -0.2000, -0.2000])\n"
     ]
    }
   ],
   "source": [
    "## 例3：用于module中的tensor objects\n",
    "#  要求：\n",
    "#    1. 将linear层中bias的梯度改为0\n",
    "#    2. conv layer从downstream拿到的gradient大小都不小于0\n",
    "\n",
    "# ----------------  原模型  ----------------\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = lambda x: x.view(-1)\n",
    "        self.fc = nn.Linear(160, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv(x))         \n",
    "        return self.fc(self.flatten(x))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "x = torch.randn(1, 3, 8, 8)\n",
    "net = TestNet()\n",
    "out = net(x)\n",
    "loss = (1 - out).mean()\n",
    "loss.backward()\n",
    "\n",
    "print(out.shape)\n",
    "# bias_grad in linear layer should be -1/out.shape\n",
    "print('bias_grad in linear layer:', net.fc.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed48a508",
   "metadata": {},
   "source": [
    "### 1.2 查看intermediate tensor或其grad信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "357834bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.184034Z",
     "start_time": "2023-10-26T04:04:58.179936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8891,  0.5235,  0.4592])\n"
     ]
    }
   ],
   "source": [
    "## 例2：打印intermediate output，也就是Feature value\n",
    "#  定义hook\n",
    "def func(grad):\n",
    "    print(grad)\n",
    "    return grad\n",
    "\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "#  register hook for y，y在这里是intermediate tensor\n",
    "#  如果不用hook，就要设置y.retain_grad()才能在bp结束后查看，会占用内存\n",
    "y.register_hook(func)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd1605",
   "metadata": {},
   "source": [
    "#### module中给tensor加入backward hook后，Backward pass的执行顺序\n",
    "1. 从root开始按照chainrule执行backward pass\n",
    "2. 遇到hook后，对制定tensor的grad执行相应的ops，如果同一位置有多个hook，按他们在module中出现的顺序执行操作，而不是想chainrule那样反向操作。如下例：\\\n",
    "fc layer backward method     -> \\\n",
    "flatten layer                -> \\\n",
    "register for clamp           -> \\\n",
    "register for print shape     -> \\\n",
    "register for check gradient  -> \\\n",
    "relu layer backward method   -> \\\n",
    "conv layer backward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e03686",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.192447Z",
     "start_time": "2023-10-26T04:04:58.185132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of output of flatten layer: torch.Size([160])\n",
      "size of output of relu layer: torch.Size([1, 10, 4, 4])\n",
      "any grad < 0? False\n",
      "bias_grad in linear layer: tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# ----------------  加入hook  ----------------\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = lambda x: x.view(-1)\n",
    "        self.fc = nn.Linear(160, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv(x))\n",
    "        \n",
    "        # 设置hook：让x.grad >= 0\n",
    "        # 这里处理的 x 是relu layer的output\n",
    "        # relu的梯度只有0和1，所以处理relu之后的x即可\n",
    "        x.register_hook(lambda grad: torch.clamp(grad, min=0))\n",
    "        \n",
    "        x.register_hook(lambda grad: print('size of output of relu layer:', grad.shape))\n",
    "        # 再加一个hook确认有没有x.grad是负值\n",
    "        # 放在这里改变的是relu运算后的x.grad\n",
    "        x.register_hook(lambda grad: print('any grad < 0?', \\\n",
    "                                           bool((grad < 0).any())))\n",
    "        x = self.flatten(x)\n",
    "        # 这里处理的 x 是flatten layer的output\n",
    "        x.register_hook(lambda grad: print('size of output of flatten layer:', grad.shape))\n",
    "\n",
    "        y = self.fc(x)\n",
    "        return y    \n",
    "    \n",
    "torch.manual_seed(2)\n",
    "x = torch.randn(1, 3, 8, 8)\n",
    "net = TestNet()\n",
    "\n",
    "# 在model外部，给参数tenosr设置hook: 这里将linear层中bias的梯度改为0\n",
    "# 这种方式可以在不改变module定义的条件下设置hook\n",
    "for name, param in net.named_parameters():\n",
    "    if 'fc' in name and 'bias' in name:\n",
    "        param.register_hook(lambda grad: torch.zeros(grad.shape))\n",
    "\n",
    "out = net(x)\n",
    "loss = (1 - out).mean()\n",
    "loss.backward()\n",
    "\n",
    "# 确认bias的grad成功改成了全0\n",
    "print('bias_grad in linear layer:', net.fc.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aaff52",
   "metadata": {},
   "source": [
    "### 1.3 hooks for saved tensors：\n",
    "**功能：** 改变saved tensor pack/unpack方式 \\\n",
    "**方法：** \n",
    "1. 定义两个hook function，一个实现pack，另一个unpack \\\n",
    "（1）定义pack_func(tensor): 只接受1个tensor作为argument，返回任意python类型。 \\\n",
    "（2）定义unpack_hook_func(output_of_pack_func)：只接受pack_func的返回值作为input argument，返回一个tensor用于后续的backward pass。该返回值的value要跟pack_func中的input的value相同，以达到原本想要从forward向backward传递信息的目的。\n",
    "（3）即使x = unpack(pack(x)),但pack会将x打包成另一个object，unpack输出的x同样也是新的tensor，如果pack(x)=x，那么他们虽然是三个不同的tensor，但share memory；如果pack(x)不等于x，则x和unpack输出的xshare memory。\n",
    "2. register 上述pack/unpack hooks \n",
    "\n",
    "**基本原则：** \n",
    "1. uppack_func(pack_func(tensor)) = tensor\n",
    "2. pack_func的input不能做in-place modify\n",
    "3. pack_func(tensor)的output可以是tenosr或者任意python type object \n",
    "4. pack_func和unpack_func单独作用于每个saved tensor\n",
    "\n",
    "**执行过程：** \n",
    "1. 每次forward pass执行过程中，pack func在对应operation存储信息的时候被调用，其output会代替原本module中定义的pack func输出的output tensor而被存储。\n",
    "2. 在backward pass中按照chainrule执行到对应operation的backward method的之前，unpack func会被调用，它用pack func的output作为唯一的input来计算一个new tensor。这个new tensor会作为backward method的input之一而被使用。\n",
    "\n",
    "**典型应用场景：** <font color=blue>将forward pass中要保存的tensor存到cpu或者disk上，节省GPU memory</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51d40a61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.229496Z",
     "start_time": "2023-10-26T04:04:58.195113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packing: tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
      "packing: tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"222pt\" height=\"282pt\"\n",
       " viewBox=\"0.00 0.00 222.00 282.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 278)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-278 218,-278 218,4 -4,4\"/>\n",
       "<!-- 140015580631008 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140015580631008</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"133.5,-31 79.5,-31 79.5,0 133.5,0 133.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 140012081989088 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140012081989088</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 62,-86 62,-67 151,-67 151,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140012081989088&#45;&gt;140015580631008 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140012081989088&#45;&gt;140015580631008</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-66.79C106.5,-60.07 106.5,-50.4 106.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-41.19 106.5,-31.19 103,-41.19 110,-41.19\"/>\n",
       "</g>\n",
       "<!-- 140012081997584 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140012081997584</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140012081997584&#45;&gt;140012081989088 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140012081997584&#45;&gt;140012081989088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.5,-121.98C67.69,-114.23 80.01,-102.58 89.97,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.48,-95.59 97.34,-86.17 87.67,-90.5 92.48,-95.59\"/>\n",
       "</g>\n",
       "<!-- 140012082027616 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140012082027616</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-207 23.5,-207 23.5,-177 77.5,-177 77.5,-207\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">a</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 140012082027616&#45;&gt;140012081997584 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140012082027616&#45;&gt;140012081997584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.84C50.5,-169.21 50.5,-159.7 50.5,-151.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.27 50.5,-141.27 47,-151.27 54,-151.27\"/>\n",
       "</g>\n",
       "<!-- 140012081990240 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140012081990240</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-141 119,-141 119,-122 208,-122 208,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140012081990240&#45;&gt;140012081989088 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140012081990240&#45;&gt;140012081989088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.34,-121.98C146,-114.23 133.47,-102.58 123.32,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.53,-90.42 115.82,-86.17 120.76,-95.54 125.53,-90.42\"/>\n",
       "</g>\n",
       "<!-- 140012081998736 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140012081998736</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-201.5 113,-201.5 113,-182.5 214,-182.5 214,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140012081998736&#45;&gt;140012081990240 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140012081998736&#45;&gt;140012081990240</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-182.37C163.5,-174.25 163.5,-161.81 163.5,-151.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-151.17 163.5,-141.17 160,-151.17 167,-151.17\"/>\n",
       "</g>\n",
       "<!-- 140012082027296 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140012082027296</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-274 136.5,-274 136.5,-243 190.5,-243 190.5,-274\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 140012082027296&#45;&gt;140012081998736 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140012082027296&#45;&gt;140012081998736</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-242.86C163.5,-233.68 163.5,-221.75 163.5,-211.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-211.82 163.5,-201.82 160,-211.82 167,-211.82\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f571a694cd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 例1：saved_tensor_hooks工作过程\n",
    "#  定义hook func\n",
    "def pack(x):\n",
    "    print('packing:', x)\n",
    "    return x\n",
    "def unpack(x):\n",
    "    print('unpacking:', x)\n",
    "    return x\n",
    "\n",
    "a = torch.ones(5, requires_grad=True)\n",
    "b = torch.ones(5, requires_grad=True) * 2\n",
    "\n",
    "# register pack/unpack hooks\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = a * b\n",
    "\n",
    "# 从图上可以看到：\n",
    "# 1.第1个packing的tensor是a\n",
    "# 2.第2个packing的tensor是b，但b不是leaf，它是torch.ones(...)*2的output\n",
    "torchviz.make_dot(y, params={'a':a, 'b':b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd11c307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.233898Z",
     "start_time": "2023-10-26T04:04:58.230791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpacking: tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
      "unpacking: tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dce8526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.239740Z",
     "start_time": "2023-10-26T04:04:58.235058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([0.8033, 0.1748, 0.0890], requires_grad=True)\n",
      "packing: tensor([3.2131, 0.6993, 0.3559])\n",
      "unpacking: tensor([0.8033, 0.1748, 0.0890])\n"
     ]
    }
   ],
   "source": [
    "## 例2.1：自定义pack和unpack规则：改变tensor大小后恢复\n",
    "#  pack/unpack func满足“unpack(pack(x)) = x”的规则即可\n",
    "\n",
    "# ---> 例2.1和例2.2没有实际意义，只是展示pack和unpack的自定义能力\n",
    "\n",
    "def pack(x):\n",
    "    print('packing:', x * 4)\n",
    "    return x * 4\n",
    "def unpack(x):\n",
    "    print('unpacking:', x / 4)\n",
    "    return x / 4\n",
    "# pack/unpack func满足“unpack(pack(x)) = x”的规则\n",
    "\n",
    "torch.manual_seed(3)\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print('x =', x)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = x ** 2\n",
    "    \n",
    "y.sum().backward()\n",
    "assert(x.grad.equal(x * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b661fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.244298Z",
     "start_time": "2023-10-26T04:04:58.240894Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2.2：自定义pack和unpack规则：保存index of a list\n",
    "\n",
    "storage = []\n",
    "\n",
    "def pack(x):\n",
    "    storage.append(x)\n",
    "    return len(storage) - 1\n",
    "\n",
    "def unpack(ind):\n",
    "    return storage[ind]\n",
    "\n",
    "torch.manual_seed(3)\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = x ** 2\n",
    "    \n",
    "y.sum().backward()\n",
    "assert(x.grad.equal(x * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f73667",
   "metadata": {},
   "source": [
    "#### 用pack/unpack将tensor存到GPU之外的地方\n",
    "1. 这是GPU memory和time的trade off。官方样例中，用A100GPU测试，把ResNet152(batch size=256)存到cpu上，可以将gpu内存使用量从48G降低到5G,但是耗时增加6倍\n",
    "2. 一种折中是只把部分layer的tensor传到cpu或者其他位置。方法是，define a special nn.Module，用来wraps module and save its tensors to cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e89e14e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.250023Z",
     "start_time": "2023-10-26T04:04:58.245417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 例3：saving tensor to cpu\n",
    "\n",
    "#  1.人工手写\n",
    "def pack(x):\n",
    "    return (x.device, x.cpu())\n",
    "\n",
    "def unpack(package):\n",
    "    device, x = package\n",
    "    return x.to(device)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = x ** 2\n",
    "    \n",
    "y.sum().backward()\n",
    "# assert(x.grad.equal(x * 2))\n",
    "torch.allclose(x.grad, (2 * x))  # x.grad与2x值的差异在默认区间内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b216b84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.450220Z",
     "start_time": "2023-10-26T04:04:58.251221Z"
    }
   },
   "outputs": [],
   "source": [
    "#  2.pytorch已经实现了上述功能\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(5))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.autograd.graph.save_on_cpu(pin_memory=True):\n",
    "            return self.w * x\n",
    "\n",
    "x = torch.randn(5)\n",
    "model = Model()\n",
    "loss = model(x).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e39997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.456938Z",
     "start_time": "2023-10-26T04:04:58.451836Z"
    }
   },
   "outputs": [],
   "source": [
    "#  3.module wrapper\n",
    "\n",
    "class SaveToCpu(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        with torch.autograd.graph.save_on_cpu(pin_memory=True):\n",
    "            return self.module(*args, **kwargs)\n",
    "        \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 100), \n",
    "    nn.ReLU(), \n",
    "    SaveToCpu(nn.Linear(100, 100)), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(100, 10),\n",
    ")\n",
    "\n",
    "x = torch.randn(10)\n",
    "loss = model(x).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5117ada1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.472371Z",
     "start_time": "2023-10-26T04:04:58.458119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Double access failed!\n"
     ]
    }
   ],
   "source": [
    "## 例4：saving tensor to disk\n",
    "\n",
    "## 错误的方式\n",
    "\n",
    "import uuid\n",
    "import os\n",
    "import tempfile\n",
    "tmp_dir_obj = tempfile.TemporaryDirectory()\n",
    "tmp_dir = tmp_dir_obj.name\n",
    "\n",
    "def pack_hook(tensor):\n",
    "    name = os.path.join(tmp_dir, str(uuid.uuid4()))\n",
    "    torch.save(tensor, name)\n",
    "    return name\n",
    "\n",
    "def unpack_hook(name):\n",
    "    tensor = torch.load(name)\n",
    "    os.remove(name)  # 如果这里remove，那么unpack就不能被call第二次\n",
    "    return tensor\n",
    "\n",
    "x = torch.ones(5, requires_grad=True)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "    y = x.pow(2)\n",
    "print(y.grad_fn._saved_self)      # 每执行一次就会unpack一次\n",
    "try:\n",
    "    print(y.grad_fn._saved_self)  # 第二次会失败\n",
    "    print(\"Double access succeeded!\")\n",
    "except:\n",
    "    print(\"Double access failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb0c2175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.476673Z",
     "start_time": "2023-10-26T04:04:58.473529Z"
    }
   },
   "outputs": [],
   "source": [
    "## 正确的方式：利用pytorch自动释放saved data的机制\n",
    "#  pytorch自动释放不再需要的object，即这里的SelfDeletingTempFile object\n",
    "\n",
    "class SelfDeletingTempFile():\n",
    "    def __init__(self):\n",
    "        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n",
    "\n",
    "    def __del__(self):\n",
    "        os.remove(self.name)\n",
    "\n",
    "def pack_hook(tensor):\n",
    "    temp_file = SelfDeletingTempFile()\n",
    "    torch.save(tensor, temp_file.name)\n",
    "    return temp_file\n",
    "\n",
    "def unpack_hook(temp_file):\n",
    "    return torch.load(temp_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e043e202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.499858Z",
     "start_time": "2023-10-26T04:04:58.477807Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only save on disk tensors that have size >= 1000\n",
    "SAVE_ON_DISK_THRESHOLD = 1000\n",
    "\n",
    "def pack_hook(x):\n",
    "    if x.numel() < SAVE_ON_DISK_THRESHOLD:\n",
    "        return x\n",
    "    temp_file = SelfDeletingTempFile()\n",
    "    torch.save(tensor, temp_file.name)\n",
    "    return temp_file\n",
    "\n",
    "def unpack_hook(tensor_or_sctf):\n",
    "    if isinstance(tensor_or_sctf, torch.Tensor):\n",
    "        return tensor_or_sctf\n",
    "    return torch.load(tensor_or_sctf.name)\n",
    "\n",
    "class SaveToDisk(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "            return self.module(*args, **kwargs)\n",
    "\n",
    "net = nn.DataParallel(SaveToDisk(Model()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc33edc",
   "metadata": {},
   "source": [
    "## 2. Hooks for nn.Module objects\n",
    "用于nn.Module的hook有forward和backward hook。\\\n",
    "**signature of hook function：** \n",
    "1. for backward hook: <font color=red>hook_func_name(module, grad_input, grad_output) -> Tensor or None </font>\n",
    "2. for foreward hook: <font color=red>hook_func_name(module, input, output) -> None </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de807d1",
   "metadata": {},
   "source": [
    "### 2.1 register forward hooks\n",
    "forward hook在forward pass中被调用，具体又有两种执行位置，**pre_hook**在forward method之前执行；**hook**在forward method执行完之后执行。\\\n",
    "下面1和2只对当前hook所reigster上的module有效。3和4是global hook，也就是installed for all modules。\n",
    "1. <font color=green>**register_forward_pre_hook(hook_func_name)**</font>\n",
    "2. <font color=green>**register_forward_hook(hook_func_name)**</font>\n",
    "3. <font color=green>**register_module_forward_pre_hook(hook_func_name)**</font>\n",
    "4. <font color=green>**register_module_forward_hook(hook_func_name)**</font>："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d83818",
   "metadata": {},
   "source": [
    "### 2.2 register backward hooks\n",
    "backward hook只有一种，在backward method执行完后执行。下面1和2只对当前hook所reigster上的module有效。3和4是global hook，也就是installed for all modules。\n",
    "1. <font color=green>**register_full_backward_pre_hook(hook_func_name)**</font>\n",
    "2. <font color=green>**register_full_backward_hook(hook_func_name)**</font>：这个是原register_backward_hook()\n",
    "3. <font color=green>**register_module_full_backward_hook(hook_func_name)**</font>\n",
    "4. <font color=green>**register_module_full_backward_pre_hook(hook_func_name)**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c94ff382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.503995Z",
     "start_time": "2023-10-26T04:04:58.501252Z"
    }
   },
   "outputs": [],
   "source": [
    "### 例1：定义不同的hooks，看他们的工作方式\n",
    "\n",
    "##  新建module和input\n",
    "m = nn.Linear(3, 3)\n",
    "torch.manual_seed(1)\n",
    "x = torch.randn(2, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7339d548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.510484Z",
     "start_time": "2023-10-26T04:04:58.505139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "register前：\n",
      "tensor([[-0.1644,  0.2539, -0.0903],\n",
      "        [-0.5065,  0.0724, -0.1563]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "register pre hook后，output：\n",
      "tensor([[ 0.3663,  1.1868, -0.1191],\n",
      "        [ 0.0242,  1.0053, -0.1852]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "register两个forward hook后，output: \n",
      "tensor([[2.0277, 2.4538, 0.9425],\n",
      "        [1.6455, 1.5534, 0.6487]], grad_fn=<AddBackward0>)\n",
      "\n",
      "去掉hooks: \n",
      "tensor([[-0.1644,  0.2539, -0.0903],\n",
      "        [-0.5065,  0.0724, -0.1563]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## 定义foreward hook function\n",
    "#  1.用于在forward pass前，检查或者调整inputs\n",
    "def forward_pre_hook(m, inputs):  # 注，inputs都是wrapped成tuple类型的\n",
    "    input = inputs[0]\n",
    "    return input + 1.\n",
    "\n",
    "#  2.用于在forward pass后，检查inputs/outputs或者调整outputs\n",
    "def forward_hook(m, inputs, output):# inputs都wrapped成tuple，output按原类型传\n",
    "    # 按ResNet的方式计算residual\n",
    "    return output + inputs[0]\n",
    "\n",
    "## register forward hooks：\n",
    "#  Run input through module before and after adding hooks.\n",
    "print('register前：\\n{}\\n'.format(m(x)))\n",
    "\n",
    "# input调整后会产生不同的output：\n",
    "forward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)\n",
    "print('register pre hook后，output：\\n{}\\n'.format(m(x)))\n",
    "\n",
    "# 调整后的output：\n",
    "forward_hook_handle = m.register_forward_hook(forward_hook)\n",
    "print('register两个forward hook后，output: \\n{}\\n'.format(m(x)))\n",
    "\n",
    "# 去掉hooks之后，output与adding hooks之前的值一致\n",
    "forward_pre_hook_handle.remove()\n",
    "forward_hook_handle.remove()\n",
    "print('去掉hooks: \\n{}'.format(m(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b12f135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.516357Z",
     "start_time": "2023-10-26T04:04:58.511770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "register前：\n",
      "tensor([[0.4999, 0.7264, 0.2085],\n",
      "        [0.4999, 0.7264, 0.2085]])\n",
      "\n",
      "register后：\n",
      "tensor([[42., 42., 42.],\n",
      "        [42., 42., 42.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 定义backward hook function\n",
    "#  功能：检查grad_inputs/grad_outputs或者调整剩余bp流程中用的grad_inputs\n",
    "def backward_hook(m, grad_inputs, grad_outputs): # 注，grad_inputs/grad_outputs都wrapped成tuple\n",
    "    new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]\n",
    "    return new_grad_inputs\n",
    "\n",
    "## register backward hooks:\n",
    "m(x).sum().backward()\n",
    "print('register前：\\n{}\\n'.format(x.grad))\n",
    "\n",
    "# Clear gradients before running backward pass again.\n",
    "m.zero_grad()\n",
    "x.grad.zero_()\n",
    "\n",
    "m.register_full_backward_hook(backward_hook)\n",
    "m(x).sum().backward()\n",
    "print('register后：\\n{}\\n'.format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f277a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.521032Z",
     "start_time": "2023-10-26T04:04:58.517501Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2：用forward_hook查看module中activation value的例子\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.conv = nn.Conv2d(3,8,2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4,4))\n",
    "        self.fc = nn.Linear(8*4*4 , 1)\n",
    "    def forward(self, x):\n",
    "          x = F.relu(self.conv(x))\n",
    "          x = self.pool(x)\n",
    "          x = x.view(x.shape[0] , -1)\n",
    "          x = self.fc(x)\n",
    "          return x\n",
    "\n",
    "# 定义hook，用来提取activation的结果，也就是feature\n",
    "features = {} \n",
    "def hook_func(model, input ,output):\n",
    "    features['feature'] = output.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cbda9bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.525964Z",
     "start_time": "2023-10-26T04:04:58.522169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "# 在pooling layer上register hook\n",
    "net.pool.register_forward_hook(hook_func)\n",
    "\n",
    "x= torch.randn(1,3,10,10)\n",
    "output = net(x)\n",
    "print(features['feature'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf137ed",
   "metadata": {},
   "source": [
    "### 2.3 典型应用场景和应用方式\n",
    "#### 场景1. 在模型训练过程中打印所需信息\n",
    "**1. 方法：** 给module加wrapper，在wrapper module上加hooks\\\n",
    "**2. 优点：** \\\n",
    "（1）方便debug，避免手动增加和删除print的麻烦 \\\n",
    "（2）不仅可以在自定义module，还可以在pytorch自带的module和第三方module上使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec4d0f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.530590Z",
     "start_time": "2023-10-26T04:04:58.527284Z"
    }
   },
   "outputs": [],
   "source": [
    "## 场景1：在ResNet18上用hooks来打印model信息\n",
    "\n",
    "#  给model加一个wrapper class\n",
    "class VerboseNet(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "        # register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            # 虽然所有nn.Mudule的实例都继承了__name__属性，但有的module可能没赋值\n",
    "            # 这一步可以明确让layer都有对应的name\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer, _, output: print(f\"new --->: {layer.__name__}: {output.shape}\")\n",
    "            )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2506626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.866670Z",
     "start_time": "2023-10-26T04:04:58.532022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new --->: conv1: torch.Size([10, 64, 14, 14])\n",
      "new --->: bn1: torch.Size([10, 64, 14, 14])\n",
      "new --->: relu: torch.Size([10, 64, 14, 14])\n",
      "new --->: maxpool: torch.Size([10, 64, 7, 7])\n",
      "new --->: layer1: torch.Size([10, 64, 7, 7])\n",
      "new --->: layer2: torch.Size([10, 128, 4, 4])\n",
      "new --->: layer3: torch.Size([10, 256, 2, 2])\n",
      "new --->: layer4: torch.Size([10, 512, 1, 1])\n",
      "new --->: avgpool: torch.Size([10, 512, 1, 1])\n",
      "new --->: fc: torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "verbose_resnet = VerboseNet(resnet18(weights=ResNet18_Weights.DEFAULT))\n",
    "dummy_input = torch.ones(10, 3, 28, 28)\n",
    "\n",
    "# 这里用了assignment是为了不让jupyter打印函数的output\n",
    "_ = verbose_resnet(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b830b",
   "metadata": {},
   "source": [
    "#### 场景2. Feature extraction\n",
    "**1. 方法：** 给module加wrapper，在wrapper module上加hooks\\\n",
    "**2. 优点：** 在一个预训练好的模型上做transfer learning时，可能想查看该模型摸些layers上得到的feature。hook可以在不改变模型本身的条件下实现这一需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e02ff083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:58.873236Z",
     "start_time": "2023-10-26T04:04:58.867967Z"
    }
   },
   "outputs": [],
   "source": [
    "## 场景2：Feature extraction\n",
    "\n",
    "class FeatureExt(nn.Module):\n",
    "    def __init__(self, model, layers):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self._features = {layer: torch.empty(0) for layer in layers}\n",
    "        \n",
    "        for layer_i in layers:\n",
    "            # 把named_modules的sub-module list构造成dict，用layer_i索引\n",
    "            # [说明见下一个cell]\n",
    "            layer = dict([*self.model.named_modules()])[layer_i]\n",
    "            \n",
    "            # layer_i索引出来对应layer后，给该layer加上hook:\n",
    "            layer.register_forward_hook(self.save_outputs_hook(layer_i))\n",
    "         \n",
    "    # 定义hook function：\n",
    "    def save_outputs_hook(self, layer_i):\n",
    "        def fn(_, __, output):  # 下划线长度不同，因为arguments name不能相同\n",
    "            # 将该layer的output提取出来，存入feature dict\n",
    "            self._features[layer_i] = output\n",
    "        return fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _ = self.model(x)\n",
    "        return self._features  # 返回dict存放了指定layers的activation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5432737c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:59.086065Z",
     "start_time": "2023-10-26T04:04:58.876392Z"
    }
   },
   "outputs": [],
   "source": [
    "res18 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "## 对resnet.named_modules的说明：\n",
    "#  1. resnet18.named_modules是一个generator\n",
    "#     每次yield返回的是tuple，形如('layer_name', module)\n",
    "#  2. 加'*'来把generator解包成单独的tuples：*res18.named_modules()\n",
    "#  3. 将dict()用到list of tuples上，可以转换成key-value pair，便于用key索引\n",
    "#     这里要先将解包后的tuples打包到一个list里面，所以是用 dict([])\n",
    "\n",
    "#  ----- 打印sample layers 来查看网络的结构  -----\n",
    "# print([*res18.named_modules()][0]) # nest结构的顶层，描绘整个结构\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][5]) # nest结构的第2层，'layer1'\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][6]) # nest结构的第3层，'layer1'的sub-module\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][7]) # nest结构的第4层，..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "576976ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T04:04:59.109117Z",
     "start_time": "2023-10-26T04:04:59.088309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4 -> torch.Size([10, 512, 1, 1])\n",
      "avgpool -> torch.Size([10, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "res18_feats = FeatureExt(res18, layers=['layer4', 'avgpool'])\n",
    "\n",
    "fests = res18_feats(dummy_input)\n",
    "for name, output in fests.items():\n",
    "    print(name, '->', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a5d79d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
