{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23724491",
   "metadata": {},
   "source": [
    "# Build model\n",
    "<font color=blue>[包含tutorial的Build model和developer note的Modules]</font>\n",
    "## 1. 什么是pytorch中的module,pytorch提供了哪些module类型？\n",
    "· module是构建神经网络的基础模块。pytorch提供了一个modules库，也支持自定义modules。用他们可以很容易地构建多层神经网络。具体实现来看，<font color=green>**namespace**</font> **torch.nn**提供了layers, containers和utilities三种主要的module类型，以及tensor类型的nn.Parameter作为modules parameter。\n",
    "1. <font color=lightblue>**Layers：**</font>NN通过layers对数据进行操作。pytorch用modules来表达这些layers,比如conv, affine, pooling, normalization, transformer和loss functions等\n",
    "2. <font color=lightblue>**containers：**</font>有3类container，nn.Module，nn.Sequential和holders of submodules。\\\n",
    "(1)**torch.nn.Module**。它是所有NN modules的base class，pytorch中所有的module都是**nn.Module**的子类\\\n",
    "(2)**torch.nn.Sequential**：以序列形式将1个或多个module顺序排列，体现了module的nestable\\\n",
    "(3)holders of submodules,其中：**nn.ModuleList，nn.ModuleDict**分别是以list和dictionary类型存储的module序列。**nn.ParamterList和nn.ParameterDict**分别是以list和dictionary形式存储的参数。\n",
    "3. <font color=lightblue>**utilities：**</font>把一些数据处理的函数以modules的形式表达。<font color=red>【具体待使用后描述？？？】</font>\n",
    "\n",
    "## 2. module的特点\n",
    "1. module和autograd system一起工作：modules使optimizer update参数非常方便<font color=red>【理解？？？】</font>\n",
    "2. pytorch中的module可以nest：每个神经网络模型自身都是一个module，该module又由其他modules(layers)构成。这种nest structure可以很方便的构造复杂的网络架构。<font color=red>【理解？？？】</font>\n",
    "3. **nn.Module**的子类会自动track参数，可以用两个method来查看：parameters()和named_parameters()\n",
    "4. 很容易与Transform配合使用：modules的save和restore都很直接，在CPU/GPU之间移动，做prune，quantize和其他很多操作都很方便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ea8140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.614462Z",
     "start_time": "2023-10-21T15:11:28.253828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn           # for torch.nn.Module\n",
    "import torch.nn.functional as F # for the activation function\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0e4f0",
   "metadata": {},
   "source": [
    "## 3. 定义一个NN\n",
    "1. 自定义model也得定义为**nn.Module**的子类，每个子类必须定义\\__init__和forward()两个method。\n",
    "2. 模型对input data的操作都放在forward()中。即，forward()用来指定要执行的computation，用的operation是nn.autograd.Function的子类的实例。这些子类可以是pytorch定义好的，也可以是自定义的。\n",
    "3. **用nn.Mudule来实例化module时，只implement forward() method不用implement backward() method**，因为：\\\n",
    "(1)<font color=blue>用nn.autograd.Function来实例化（自定义）Function时，要同时implement forward() and backward() methods</font> \\\n",
    "(2)<font color=blue>autograd system会用Function中的backward来自动处理module中用到的function的backward pass。</font>\n",
    "4. 如果module中要定义parameters，就要在\\__init\\__()中register。方式是在\\__init\\__()中将parameter定义为nn.Parameter的实例。此时，这些parameters就是parameters registered by the module。这也是autograd system运行需要的。\n",
    "5. Parameter class是torch.Tensor的子类，但他们可以被assigned as attributes of a Module。一旦实例化后，这些parameters就会被加到lists of the module's parameters，之后可以通过module.parameters()和model.namedparameters()来iterate throgh。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be88166",
   "metadata": {},
   "source": [
    "### 3.1 自定义一个简单的Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2951b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.620622Z",
     "start_time": "2023-10-21T15:11:29.616866Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):  # 必须是nn.Module的子类\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        # registering parameters: 参数定义成nn.Parameter的实例\n",
    "        # 此时autograd会自动tracking并让optimizer在迭代时update\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "  # implement forward() method\n",
    "    def forward(self, input):\n",
    "        return input @ self.weight + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63c55dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.638278Z",
     "start_time": "2023-10-21T15:11:29.622103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8944, -4.5328,  2.0422], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyLinear(4, 3)          \n",
    "sample_input = torch.randn(4)  \n",
    "\n",
    "# model is callable, calling invoke forward function\n",
    "model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e2b36d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.645085Z",
     "start_time": "2023-10-21T15:11:29.640362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1299,  1.0752,  0.8186],\n",
      "        [-0.8760, -0.7114,  0.6546],\n",
      "        [-0.2214, -1.3293,  1.1233],\n",
      "        [ 0.4691,  2.7896, -0.7205]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.1940,  0.2507,  1.3890], requires_grad=True)\n",
      "\n",
      "\n",
      "('weight', Parameter containing:\n",
      "tensor([[-0.1299,  1.0752,  0.8186],\n",
      "        [-0.8760, -0.7114,  0.6546],\n",
      "        [-0.2214, -1.3293,  1.1233],\n",
      "        [ 0.4691,  2.7896, -0.7205]], requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([-1.1940,  0.2507,  1.3890], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "## 遍历parameters()\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "## 遍历parameters named_parameters()\n",
    "#  这里weights和bias是parameter的name\n",
    "for parameter in model.named_parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f145b3d",
   "metadata": {},
   "source": [
    "### 3.2 将modules作为模型的基础模块(building blocks)\n",
    "· modules contain other modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297ab23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T09:10:48.862543Z",
     "start_time": "2023-10-20T09:10:48.859143Z"
    }
   },
   "source": [
    "#### i. 用nn.Sequential定义一个简单的module\n",
    "· Sequential会自动将上一层的输出传给下一层作为输入。但只在输入和输出都是单变量的情况时有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b22cb2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.651419Z",
     "start_time": "2023-10-21T15:11:29.646613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1015], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Sequential本身也是nn.Module的子类，所以实例化得到的也是module\n",
    "net = nn.Sequential(\n",
    "    MyLinear(4, 3),\n",
    "    nn.ReLU(),\n",
    "    MyLinear(3, 1)\n",
    ")\n",
    "\n",
    "simple_input = torch.randn(4)\n",
    "net(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87baec6",
   "metadata": {},
   "source": [
    "####  ii. 自定义module\n",
    "· 除了上面例子中非常简单的案例，通常都不会直接用Sequential来定义module，更多还是直接自定义module的方式.\\\n",
    "· 在__init__()中定义的submodule对应NN中的layer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9106542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.656436Z",
     "start_time": "2023-10-21T15:11:29.652774Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = MyLinear(4, 3)\n",
    "        self.layer1 = MyLinear(3, 1)  # 定义了两个submodule\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = F.relu(x)                 # relu不是submodule\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e718a358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.661474Z",
     "start_time": "2023-10-21T15:11:29.657844Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = MyLinear(4, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = MyLinear(3, 1)  # 定义了两个submodule\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.relu(x)                 # relu不是submodule\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f029af",
   "metadata": {},
   "source": [
    "**module的Immediate children可以用children() or named_children()来iterated through** \\\n",
    "上例中的children(也就是submodule)不包括rely层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ed2044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.666324Z",
     "start_time": "2023-10-21T15:11:29.662891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('layer0', MyLinear())\n",
      "('layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "for child in net.named_children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42b29f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.670363Z",
     "start_time": "2023-10-21T15:11:29.667671Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对比前面例子中直接用tensor operation定义的module\n",
    "# 此时module中没有child\n",
    "for child in model.children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0886481a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.676860Z",
     "start_time": "2023-10-21T15:11:29.673348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('layer0', MyLinear())\n",
      "('relu', ReLU())\n",
      "('layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "# 也可以把relu处理成module\n",
    "net2 = Net2()\n",
    "for child in net2.named_children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd1733",
   "metadata": {},
   "source": [
    "**modules() and named_modules() recursively iterate through a module and its child modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2152ffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.682678Z",
     "start_time": "2023-10-21T15:11:29.678249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "('', BigNet(\n",
      "  (l1): MyLinear()\n",
      "  (net): Net(\n",
      "    (layer0): MyLinear()\n",
      "    (layer1): MyLinear()\n",
      "  )\n",
      "))\n",
      "----------------------------------------------------\n",
      "('l1', MyLinear())\n",
      "----------------------------------------------------\n",
      "('net', Net(\n",
      "  (layer0): MyLinear()\n",
      "  (layer1): MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('net.layer0', MyLinear())\n",
      "----------------------------------------------------\n",
      "('net.layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "class BigNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = MyLinear(5, 4)\n",
    "        self.net = Net()\n",
    "    def forward(self, x):\n",
    "        return self.net(self.l1(x))\n",
    "\n",
    "big_net = BigNet()\n",
    "for module in big_net.named_modules():\n",
    "    print('-' * 52)\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0e3a1",
   "metadata": {},
   "source": [
    "#### iii. dynamically define submodule\n",
    "· 用ModuleList或者ModuleDict \\\n",
    "· calls to parameters() and named_parameters() will recursively include child parameters, allowing for convenient optimization of all parameters within the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "551de685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.689068Z",
     "start_time": "2023-10-21T15:11:29.684135Z"
    }
   },
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList(\n",
    "            [MyLinear(4, 4) for _ in range(num_layers)])\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU()\n",
    "        })\n",
    "        self.final = MyLinear(4, 1)\n",
    "        \n",
    "    def forward(self, x, act):\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "        x = self.activations[act](x)\n",
    "        # x = self.final(x)\n",
    "        return x\n",
    "\n",
    "dynamic_net = DynamicNet(3)\n",
    "sample_input = torch.randn(4)\n",
    "output = dynamic_net(sample_input, 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af90dd5",
   "metadata": {},
   "source": [
    "** · child module由__init__()中排列的module sequence决定，不由forward()实际执行的computation决定**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f75c3991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.693766Z",
     "start_time": "2023-10-21T15:11:29.690378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "('', DynamicNet(\n",
      "  (linears): ModuleList(\n",
      "    (0-2): 3 x MyLinear()\n",
      "  )\n",
      "  (activations): ModuleDict(\n",
      "    (relu): ReLU()\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (final): MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('linears', ModuleList(\n",
      "  (0-2): 3 x MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('linears.0', MyLinear())\n",
      "----------------------------------------------------\n",
      "('linears.1', MyLinear())\n",
      "----------------------------------------------------\n",
      "('linears.2', MyLinear())\n",
      "----------------------------------------------------\n",
      "('activations', ModuleDict(\n",
      "  (relu): ReLU()\n",
      "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "))\n",
      "----------------------------------------------------\n",
      "('activations.relu', ReLU())\n",
      "----------------------------------------------------\n",
      "('activations.lrelu', LeakyReLU(negative_slope=0.01))\n",
      "----------------------------------------------------\n",
      "('final', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "for module in dynamic_net.named_modules():\n",
    "    print('-'*52)\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f225eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:29.700462Z",
     "start_time": "2023-10-21T15:11:29.695157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "('linears.0.weight', Parameter containing:\n",
      "tensor([[-0.3525,  0.1409,  1.3865,  2.8742],\n",
      "        [ 0.6708, -0.5166,  0.1755, -0.5795],\n",
      "        [ 0.9394,  0.7073, -1.8115, -0.5176],\n",
      "        [ 0.8274, -0.8999, -1.3487, -0.5634]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.0.bias', Parameter containing:\n",
      "tensor([-0.7690, -0.3337, -0.3788, -0.2609], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.1.weight', Parameter containing:\n",
      "tensor([[ 1.7216e+00, -5.9480e-01,  1.2232e+00, -1.1701e-01],\n",
      "        [-2.1815e-01, -3.4154e-01, -2.9448e-01, -4.9559e-04],\n",
      "        [-1.6292e+00, -3.5020e-01,  1.8335e+00,  7.3476e-01],\n",
      "        [ 1.6334e-01, -1.6820e+00, -7.1722e-01, -6.4197e-01]],\n",
      "       requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.1.bias', Parameter containing:\n",
      "tensor([-0.7863,  1.3173, -0.2216, -0.6519], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.2.weight', Parameter containing:\n",
      "tensor([[-1.6903,  0.7280, -0.2277, -0.9928],\n",
      "        [-0.7582,  1.3335, -0.6271,  0.5493],\n",
      "        [ 1.5816, -0.5026, -1.0229,  0.2919],\n",
      "        [ 1.0124, -0.7070,  1.7507,  0.9754]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.2.bias', Parameter containing:\n",
      "tensor([-1.0204,  0.0363, -0.7635, -0.1105], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('final.weight', Parameter containing:\n",
      "tensor([[ 0.0224],\n",
      "        [ 0.2766],\n",
      "        [-1.3774],\n",
      "        [ 0.0123]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('final.bias', Parameter containing:\n",
      "tensor([0.6597], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for parameter in dynamic_net.named_parameters():\n",
    "    print('-'*68)\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a485448",
   "metadata": {},
   "source": [
    "#### vi. 移动参数的设备，改变参数精度，用.to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3ebc9d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:30.036852Z",
     "start_time": "2023-10-21T15:11:29.701924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 5.1280, 0.0000, 0.0000], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move all parameters to a CUDA device\n",
    "dynamic_net.to(device='cuda')\n",
    "\n",
    "# Change precision of all parameters\n",
    "dynamic_net.to(dtype=torch.float64)\n",
    "\n",
    "dynamic_net(torch.randn(4, device='cuda', dtype=torch.float64), 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555f4a6",
   "metadata": {},
   "source": [
    "#### v. module和submodule可以apply任意函数，包括自定义函数\n",
    "an arbitrary function can be applied to a module and its submodules recursively by using the apply() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56d8daee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:30.043690Z",
     "start_time": "2023-10-21T15:11:30.038595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicNet(\n",
       "  (linears): ModuleList(\n",
       "    (0-2): 3 x MyLinear()\n",
       "  )\n",
       "  (activations): ModuleDict(\n",
       "    (relu): ReLU()\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (final): MyLinear()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to initialize Linear weights.\n",
    "# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.\n",
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.fill_(0.0)\n",
    "\n",
    "# Apply the function recursively on the module and its submodules.\n",
    "dynamic_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f63538",
   "metadata": {},
   "source": [
    "## 4. 使用module训练NN\n",
    "**module有两种mode：trainning mode和evaluation mode**\n",
    "1. module默认处于training mode。用training()和eval()可以改变module所处mode。\n",
    "2. 如果module中有submodule在training mode和evaluation mode的时候输出不同，那么就应该在inference的时候将mode改为evaluation mode，比如batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85d6837a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.141585Z",
     "start_time": "2023-10-21T15:11:30.044971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer0): MyLinear()\n",
       "  (layer1): MyLinear()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新建network和optimizer\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, \n",
    "                            weight_decay=1e-2, momentum=0.9)\n",
    "\n",
    "# trainging the netword\n",
    "for _ in range(10000):\n",
    "    input = torch.randn(4)\n",
    "    output = net(input)\n",
    "    loss = torch.abs(output) # 用abs做loss，会让weights趋于0\n",
    "    \n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# training完成后，将module转到eval mode\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a99c86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.146933Z",
     "start_time": "2023-10-21T15:11:33.143319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0054],\n",
      "        [ 0.1011],\n",
      "        [-0.0995]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.layer1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31db1caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.160297Z",
     "start_time": "2023-10-21T15:11:33.148107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training mode output: tensor([1.3465, 3.2833, 0.5000, 0.6025])\n",
      "evaluation mode output: tensor([ 0.3465,  2.2833, -0.5000, -0.3975])\n"
     ]
    }
   ],
   "source": [
    "# 在training和evaluation mode下输出不同的例子\n",
    "class ModalModule(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.training:\n",
    "      # Add a constant only in training mode.\n",
    "      return x + 1.\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "m = ModalModule()\n",
    "x = torch.randn(4)\n",
    "print('training mode output: {}'.format(m(x)))\n",
    "\n",
    "m.eval()\n",
    "print('evaluation mode output: {}'.format(m(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47a97f",
   "metadata": {},
   "source": [
    "## 5. module state\n",
    "1. 如果要保存a trained model，可以存该module的state_dict，state_dict中保存了影响module运算的状态。state包括parameters和buffers。\\\n",
    "(1)**parameters**: learnable aspects of computation,存在state_dict中。\\\n",
    "(2)**buffers**: non-learnable aspects of computation. 有的module会存储参数之外的其他信息到state_dict，和参数不同的是，这些信息不需要learn，他们会被存在buffers中。\n",
    "2. 有两种buffers：Persistent buffers存在state_dict中，non-Persistent buffers不存在state_dict中。\\\n",
    "(1)Persistent buffers: 比如：serialized when saving and loading\n",
    "(2)non-Persistent buffers: 比如：left out of serialization\n",
    "3. Persistent buffers的特点：\\\n",
    "(1)如果state被存为state_dict的一部分，那么loading a serialized form of the module的时候，它就能被restore。 \\\n",
    "(2)这部分变量不会像parameters那样被optimizer处理，因而是non-leanable\n",
    "4. non-Persistent buffers的特点：\\\n",
    "不存为state_dict的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a3e8b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.169755Z",
     "start_time": "2023-10-21T15:11:33.161447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save the module\n",
    "torch.save(net.state_dict(), 'net.pt')\n",
    "\n",
    "## load the module\n",
    "#  1. 新建一个结构相同的module\n",
    "new_net = Net()\n",
    "#  2. load state\n",
    "new_net.load_state_dict(torch.load('net.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec5290bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.176099Z",
     "start_time": "2023-10-21T15:11:33.170893Z"
    }
   },
   "outputs": [],
   "source": [
    "## 使用buffers：module中要保存running mean\n",
    "#  将running mean的当前值存到state_dict用register_buffer()\n",
    "\n",
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('mean', torch.zeros(num_features))\n",
    "        # 此时，self.mean会被存到state_dict中\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 每次迭代时更新running mean的值\n",
    "        # 作为state_dict的一部分，当loading module的时候会被restore\n",
    "        self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n",
    "        return self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14a2fe3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.185298Z",
     "start_time": "2023-10-21T15:11:33.177403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))])\n",
      "tensor(True)\n",
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "m = RunningMean(4)\n",
    "for _ in range(10):\n",
    "    input = torch.randn(4)\n",
    "    m(input)\n",
    "\n",
    "print(m.state_dict())\n",
    "\n",
    "# Serialized form will contain the 'mean' tensor\n",
    "torch.save(m.state_dict(), 'mean.pt')\n",
    "\n",
    "m_loaded = RunningMean(4)\n",
    "m_loaded.load_state_dict(torch.load('mean.pt'))\n",
    "\n",
    "# 注意这里几种assert和print的差异\n",
    "assert(torch.all(m.mean == m_loaded.mean))\n",
    "print(torch.all(m.mean == m_loaded.mean))\n",
    "print(m.mean == m_loaded.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3629e24d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.192289Z",
     "start_time": "2023-10-21T15:11:33.186478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "## 将running mean存为non-Persistent buffers\n",
    "#  还是用register_buffer()，参数Persistent=False\n",
    "\n",
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('mean', torch.zeros(num_features), persistent=False)\n",
    "        # 此时，self.mean不会被存到state_dict中\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n",
    "        return self.mean\n",
    "\n",
    "torch.manual_seed(0)\n",
    "m2 = RunningMean(4)\n",
    "for _ in range(10):\n",
    "    input = torch.randn(4)\n",
    "    m2(input)\n",
    "\n",
    "print(m2.state_dict()) # 此时输出的state_dict是空的\n",
    "\n",
    "torch.save(m2.state_dict(), 'mean.pt')\n",
    "m2_loaded = RunningMean(4)\n",
    "m2_loaded.load_state_dict(torch.load('mean.pt'))\n",
    "print(torch.all(m2.mean == m2_loaded.mean)) # 输出False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6f7b2",
   "metadata": {},
   "source": [
    "#### 一个module的buffers可以用buffers()和named_buffers()来迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "082c83e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.195727Z",
     "start_time": "2023-10-21T15:11:33.193514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))\n"
     ]
    }
   ],
   "source": [
    "for buffer in m.named_buffers():\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20216c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.199213Z",
     "start_time": "2023-10-21T15:11:33.196864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))\n"
     ]
    }
   ],
   "source": [
    "for buffer in m2.named_buffers():\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f872834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:42:48.068015Z",
     "start_time": "2023-10-21T10:42:48.065554Z"
    }
   },
   "source": [
    "#### 两种buffers都受model-wide device/type changes所使用的.to() method影响\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84ffd956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.203104Z",
     "start_time": "2023-10-21T15:11:33.200405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunningMean()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.to(device='cuda', dtype=torch.float64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80cf5f4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.216543Z",
     "start_time": "2023-10-21T15:11:33.204407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('param1', tensor([-0.0404,  0.2881])), ('param2', tensor([-0.0075, -0.9145, -1.0886])), ('buffer1', tensor([ 1.3232,  0.0371, -0.2849, -0.1334])), ('param_list.0', tensor([-0.2666,  0.1894])), ('param_list.1', tensor([-0.2190,  2.0576])), ('param_list.2', tensor([-0.0354,  0.0627])), ('param_dict.bar', tensor([ 0.1753, -0.9315, -1.5055, -0.6610])), ('param_dict.foo', tensor([-0.7663,  1.0993,  2.7565])), ('linear.weight', tensor([[ 0.0197, -0.0610],\n",
      "        [ 0.1431,  0.4496],\n",
      "        [ 0.6698,  0.4491]])), ('linear.bias', tensor([ 0.6713, -0.0511, -0.6352]))])\n"
     ]
    }
   ],
   "source": [
    "## 一个综合例子\n",
    "class StatefulModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 用nn.Parameter实例化的参数会自动将tensor register为module parameter\n",
    "        self.param1 = nn.Parameter(torch.randn(2))\n",
    "\n",
    "        # 另一种将tensor register为module parameter的方式：用register_parameter() method\n",
    "        self.register_parameter('param2', nn.Parameter(torch.randn(3)))\n",
    "\n",
    "        # 将attribute： \"param3\" 定义为一个parameter，但不做初始化。\n",
    "        # 它的值'None'不会出现在state_dict中    \n",
    "        self.register_parameter('param3', None)\n",
    "\n",
    "        # Registers a list of parameters：没有name\n",
    "        self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])\n",
    "\n",
    "        # Registers a dictionary of parameters：有name\n",
    "        self.param_dict = nn.ParameterDict({\n",
    "            'foo': nn.Parameter(torch.randn(3)),\n",
    "            'bar': nn.Parameter(torch.randn(4))\n",
    "        })\n",
    "\n",
    "        # Registers a persistent buffer\n",
    "        self.register_buffer('buffer1', torch.randn(4), persistent=True)\n",
    "\n",
    "        # Registers a non-persistent buffer\n",
    "        self.register_buffer('buffer2', torch.randn(5), persistent=False)\n",
    "\n",
    "        # 将attribute：\"buffer3\" 定义为一个buffer，但不做初始化\n",
    "        # 它的值'None'也不会出现在state_dict中    \n",
    "        self.register_buffer('buffer3', None)\n",
    "\n",
    "        # 添加一个submodule就会将其parameters自动register为module的parameters\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "\n",
    "m = StatefulModule()\n",
    "\n",
    "# Save and load state_dict.\n",
    "torch.save(m.state_dict(), 'state.pt')\n",
    "m_loaded = StatefulModule()\n",
    "m_loaded.load_state_dict(torch.load('state.pt'))\n",
    "\n",
    "# state_dict中没有non-persistent buffer和reserved attributes \"param3\"与\"buffer3\"\n",
    "print(m_loaded.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405a4ab",
   "metadata": {},
   "source": [
    "## 6. module初始化\n",
    "1. 默认情况下，torch.nn提供的module中的parameter和浮点数buffer会在module实例化的时候初始化为存在CPU上的32位浮点数值。\n",
    "2. 如果要改变默认的初始化设置，可以在module实例化的时候设置对应的arguments或者直接用skip_init()method，之后自定义初始化方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62b1949b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.222630Z",
     "start_time": "2023-10-21T15:11:33.220087Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将module直接初始化到GPU上，参数类型为16位浮点数\n",
    "m = nn.Linear(5, 3, device='cuda', dtype=torch.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ae1ae7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.226330Z",
     "start_time": "2023-10-21T15:11:33.223831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# 除参数外，上述初始化方式也适用于floating-point buffers registered for the module\n",
    "m = nn.BatchNorm2d(3, dtype=torch.half)\n",
    "print(m.running_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69ec708b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.244052Z",
     "start_time": "2023-10-21T15:11:33.227487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3930, -0.1617,  0.4533, -0.0858,  0.7788],\n",
       "        [ 0.0826,  0.2477,  0.6222, -0.6544, -0.3411],\n",
       "        [ 0.1306,  0.3920,  0.5422,  0.7263, -0.0882]], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例：自定义参数初始化为正交矩阵\n",
    "m = torch.nn.utils.skip_init(nn.Linear, 5, 3)\n",
    "nn.init.orthogonal_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13a7e1",
   "metadata": {},
   "source": [
    "#### 自定义module的时候，建议按照torch.nn所遵守的规则那样：\n",
    "1. 提供一个device constructor kwarg，可以应用在任意的parameter和buffers registered by the module上\n",
    "2. 提供一个dtype constructor kwarg，可以应用在任意的parameter和floating-point buffers registered by the module上\n",
    "3. 只用初始化函数（比如：torch.nn.init package提供的函数）来初始化module constructor中的parameters和buffers。注意，此时要使用skip_init()。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28517f31",
   "metadata": {},
   "source": [
    "## 7. module hooks\n",
    "1. 什么是hooks\\\n",
    "为了更具体地控制training的过程，pytorch提供了hooks可以用来在forward和backward过程中进行任意的computation，甚至可以用来改变原pass如何执行。常见的使用场景包括：debugging，visualization activation，检查梯度等。hooks可以用用在pytorch或者第三方提供的modules上。\n",
    "\n",
    "2. 两种类型的hooks\\\n",
    "(1)forward hooks: 在forward pass中被调用，可以通过<font color=green>**register_forward_pre_hook()**</font>和<font color=green>**register_forward_hook()**</font>两种函数来install。他们分别会在调用forward function的前一步和后一步被调用。\\\n",
    "· 如果要在所有module中都调用hook，可以用对应的<font color=green>**register_module_forward_pre_hook()**</font>和<font color=green>**register_module_forward_hook()**</font>两种method。\\\n",
    "(2)backward hooks:在backward pass中被调用，可以通过<font color=green>**register_full_backward_pre_hook()**</font>和<font color=green>**register_full_backward_hook()**</font>两种函数来install。前者可以用来获取outputs的梯度，后者可以获取inputs和outputs的梯度。\n",
    "· 他们也有对应的global版本，也就是installed for all modules的版本，<font color=green>**register_module_full_backward_hook()**</font>和<font color=green>**register_module_full_backward_pre_hook()**</font>。\n",
    "\n",
    "3. 特点：所有hooks都可以返回更新后的value，这些value也会被用到hooks所在pass的后续计算中。因此，hooks可以用来在常规module的forward/backward过程中执行任意的code，或者用来调整inputs/outputs而不用改变module本身的forward() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "341ffc81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.248349Z",
     "start_time": "2023-10-21T15:11:33.245243Z"
    }
   },
   "outputs": [],
   "source": [
    "## 定义几个hooks\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 用于在forward pass前，检查或者调整inputs\n",
    "# 注意，inputs都是wrapped成tuple类型的\n",
    "def forward_pre_hook(m, inputs):\n",
    "    input = inputs[0]\n",
    "    return input + 1.\n",
    "\n",
    "# 用于在forward pass后，检查inputs/outputs或者调整outputs\n",
    "# 注意，inputs都是wrapped成tuple类型的，output都是are passed as-is\n",
    "def forward_hook(m, inputs, output):\n",
    "    # 按ResNet的方式计算residual\n",
    "    return output + inputs[0]\n",
    "\n",
    "# 检查grad_inputs/grad_outputs或者调整剩余bp流程中用的grad_inputs\n",
    "# 注意，grad_inputs/grad_outputs都是wrapped成tuple类型的\n",
    "def backward_hook(m, grad_inputs, grad_outputs):\n",
    "    new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]\n",
    "    return new_grad_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2fe2141",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.256600Z",
     "start_time": "2023-10-21T15:11:33.249530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output with no forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n",
      "        [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward0>)\n",
      "output with forward pre hook: tensor([[-0.5752, -0.7421,  0.4942],\n",
      "        [-0.0736,  0.5461,  0.0838]], grad_fn=<AddmmBackward0>)\n",
      "output with both forward hooks: tensor([[-1.0980,  0.6396,  0.4666],\n",
      "        [ 0.3634,  0.6538,  1.0256]], grad_fn=<AddBackward0>)\n",
      "output after removing forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n",
      "        [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward0>)\n",
      "x.grad with no backwards hook: tensor([[ 0.4497, -0.5046,  0.3146],\n",
      "        [ 0.4497, -0.5046,  0.3146]])\n",
      "x.grad with backwards hook: tensor([[42., 42., 42.],\n",
      "        [42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 新建module和input\n",
    "m = nn.Linear(3, 3)\n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "\n",
    "# forward hooks：\n",
    "# Run input through module before and after adding hooks.\n",
    "print('output with no forward hooks: {}'.format(m(x)))\n",
    "\n",
    "# input调整后会产生不同的output：\n",
    "forward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)\n",
    "print('output with forward pre hook: {}'.format(m(x)))\n",
    "\n",
    "# 调整后的output：\n",
    "forward_hook_handle = m.register_forward_hook(forward_hook)\n",
    "print('output with both forward hooks: {}'.format(m(x)))\n",
    "\n",
    "# 去掉hooks之后，output与adding hooks之前的值一致\n",
    "forward_pre_hook_handle.remove()\n",
    "forward_hook_handle.remove()\n",
    "print('output after removing forward hooks: {}'.format(m(x)))\n",
    "\n",
    "# backward hooks:\n",
    "m(x).sum().backward()\n",
    "print('x.grad with no backwards hook: {}'.format(x.grad))\n",
    "\n",
    "# Clear gradients before running backward pass again.\n",
    "m.zero_grad()\n",
    "x.grad.zero_()\n",
    "\n",
    "m.register_full_backward_hook(backward_hook)\n",
    "m(x).sum().backward()\n",
    "print('x.grad with backwards hook: {}'.format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24875cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74cfdaed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.260507Z",
     "start_time": "2023-10-21T15:11:33.257576Z"
    }
   },
   "outputs": [],
   "source": [
    "# 自定义NN\n",
    "class RKNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15acc066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.269016Z",
     "start_time": "2023-10-21T15:11:33.261450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RKNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 创建自定义NN的实例\n",
    "model = RKNet().to(device)  # model要建在gpu上\n",
    "print(model)                # 打印model的structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb7a088f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.287366Z",
     "start_time": "2023-10-21T15:11:33.269953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict class:tensor([8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "scores = model(X)\n",
    "\n",
    "prob = nn.Softmax(dim=1)(scores) # dim决定softmax求解的维度\n",
    "y_pred = prob.argmax(1)\n",
    "print(f'predict class:{y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337005f",
   "metadata": {},
   "source": [
    "## 典型layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d35d19",
   "metadata": {},
   "source": [
    "### nn.Flatten\n",
    "1. 参数：torch.nn.Flatten(start_dim=1, end_dim=-1)\n",
    "2. 压缩[start_dim, end_dim]范围的dims\n",
    "2. 默认将输入的data压成2维数据，保留原第一维，压缩剩下的维度，比如输出(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d741eb78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.291641Z",
     "start_time": "2023-10-21T15:11:33.288451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 784])\n",
      "torch.Size([84, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "flatten2 = nn.Flatten(0, 1)  # 压缩[0, 1]范围的dims\n",
    "flat_image2 = flatten2(input_image)\n",
    "print(flat_image2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15422dc0",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "1. affine layer\n",
    "2. 参数：torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "   · in_features (int) – size of each input sample\n",
    "   · out_features (int) – size of each output sample\n",
    "   · bias (bool)取False时, 就不会learn bias. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2975059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.295465Z",
     "start_time": "2023-10-21T15:11:33.292585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=6)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107dfd54",
   "metadata": {},
   "source": [
    "### nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9664ae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.300905Z",
     "start_time": "2023-10-21T15:11:33.296829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU:\n",
      " tensor([[-0.2382, -0.6698,  0.5672, -0.0279, -0.0255,  0.7921],\n",
      "        [-0.3899, -0.3592,  0.6236,  0.2598,  0.1601,  0.8789],\n",
      "        [-0.4697, -0.1639,  0.6639, -0.3950,  0.0037,  0.8036]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "After ReLU:\n",
      " tensor([[0.0000, 0.0000, 0.5672, 0.0000, 0.0000, 0.7921],\n",
      "        [0.0000, 0.0000, 0.6236, 0.2598, 0.1601, 0.8789],\n",
      "        [0.0000, 0.0000, 0.6639, 0.0000, 0.0037, 0.8036]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU:\\n {hidden1}\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU:\\n {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c6442",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "1. an ordered container of modules.\n",
    "2. 数据会按照Sequential中定义的layer顺序做处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88abdeb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.305227Z",
     "start_time": "2023-10-21T15:11:33.302312Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "scores = seq_modules(input_image)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d1ff4e",
   "metadata": {},
   "source": [
    "## 模型参数\n",
    "1. NN中的一些layers有参数，比如有的layers在training后都有weights和bias\n",
    "2. 把model定义为nn.Module的子类后，nn.Module能自动track所有model object中定义的fields，而参数可以通过model的parameters()和named_parameters()两种method来获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f185b9d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T15:11:33.314882Z",
     "start_time": "2023-10-21T15:11:33.306763Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: RKNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0114,  0.0017,  0.0213,  ..., -0.0045,  0.0074,  0.0287],\n",
      "        [-0.0328, -0.0175,  0.0052,  ...,  0.0321,  0.0299,  0.0017]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0037,  0.0140], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0329,  0.0100, -0.0161,  ..., -0.0220, -0.0111, -0.0066],\n",
      "        [-0.0438, -0.0121,  0.0077,  ...,  0.0383, -0.0045,  0.0278]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0114, -0.0320], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0288,  0.0433, -0.0141,  ..., -0.0400,  0.0347,  0.0229],\n",
      "        [-0.0161,  0.0252, -0.0200,  ..., -0.0269,  0.0360,  0.0205]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0057, -0.0082], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
