{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9e538e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:33:32.658712Z",
     "start_time": "2024-08-04T15:33:31.332953Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c1f78",
   "metadata": {},
   "source": [
    "# Arithmetic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571590e",
   "metadata": {},
   "source": [
    "## I. matrix product和inner product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e81cce",
   "metadata": {},
   "source": [
    "### I.1 matrix product\n",
    "1. **torch.mm()**: 最基础的矩阵乘法，不做broadcasting\n",
    "   - torch.mv()做matrix vector乘法\n",
    "2. **torch.mul()**: elementwise multiplication with broadcasting\n",
    "   - x.mul(x)相当于x * x\n",
    "3. **torch.matmul()**: 可以用符号@表示，带broadcasting的tensor乘法\n",
    "   - 如果两个参数都是1维，做dot product\n",
    "   - 如果两个参数都是1维，做基础矩阵乘法，等价于torch.mm()\n",
    "   - 如果第1个是matrix，第2个是vector，做matrix vector乘法，等价于torch.mv()\n",
    "   - 如果第1个是vector, 第2个是matrix，按照broadcast机制做矩阵乘法\n",
    "   - 如果其中一个维度高于2维，另一个至少有1维，做batched matrix products\n",
    "     - 另一个2维或以上，这里2维是matrix dims，按照broadcast机制扩展维度，然后与高维的tensor做batched matrix products。\n",
    "       - 如：t1的维度是(j,1,n,m)则(n,m)是matrix dims。t2是(k, m, p),则(m, p)是matrix dims.broadcast在(j,1)和(k)上做，得到$$\n",
    "       (j\\times k\\times n\\times m)\\times(j\\times k\\times m\\times p)=(j\\times k\\times n\\times p)$$\n",
    "     - 另一个1维，\n",
    "       - 如：t1的维度是(j,1,n,m)则(n,m)是matrix dims。t2是(k, m),则(m)是vector dim.broadcast在(j,1)和(k)上做，得到$$\n",
    "       (j\\times k\\times n\\times m)\\times(j\\times k\\times m)=(j\\times k\\times n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e890e4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:33:32.668990Z",
     "start_time": "2024-08-04T15:33:32.661077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[1, 1, 1, 1],\n",
       "           [1, 1, 1, 1]]],\n",
       " \n",
       " \n",
       "         [[[1, 1, 1, 1],\n",
       "           [1, 1, 1, 1]]]], dtype=torch.int32),\n",
       " tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11]], dtype=torch.int32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(16, dtype=torch.int).view(2, 1, 2, 4)\n",
    "b = torch.arange(12, dtype=torch.int).view(4, 3)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e8ce58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:33:32.677838Z",
     "start_time": "2024-08-04T15:33:32.670328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 2, 3]),\n",
       " tensor([[[[18, 22, 26],\n",
       "           [18, 22, 26]]],\n",
       " \n",
       " \n",
       "         [[[18, 22, 26],\n",
       "           [18, 22, 26]]]], dtype=torch.int32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a, b).shape, torch.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acba9379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:33:32.684787Z",
     "start_time": "2024-08-04T15:33:32.679662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 2]),\n",
       " tensor([[[6, 6]],\n",
       " \n",
       "         [[6, 6]]], dtype=torch.int32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.arange(4, dtype=torch.int)\n",
    "torch.matmul(a, c).shape, torch.matmul(a, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0dea56",
   "metadata": {},
   "source": [
    "### I.2 broadcast\n",
    "规则：按照<font color=blue>**last to first**</font>的顺序比较每个dim的值，\n",
    " - 所有dim值相同的话，直接运算；\n",
    " - 不相同的dim上，其中一个operand的dim值是1；\n",
    " - 或者，dimension在其中一个tensor上不存在。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97213e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:33:32.698550Z",
     "start_time": "2024-08-04T15:33:32.686245Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1998, 0.3044],\n",
      "         [0.2155, 0.6754],\n",
      "         [0.1749, 0.6393]],\n",
      "\n",
      "        [[0.1998, 0.3044],\n",
      "         [0.2155, 0.6754],\n",
      "         [0.1749, 0.6393]]])\n",
      "tensor([[[0.5262, 0.5262],\n",
      "         [0.9608, 0.9608],\n",
      "         [0.6138, 0.6138]],\n",
      "\n",
      "        [[0.5262, 0.5262],\n",
      "         [0.9608, 0.9608],\n",
      "         [0.6138, 0.6138]]])\n",
      "tensor([[[0.2755, 0.0411],\n",
      "         [0.2755, 0.0411],\n",
      "         [0.2755, 0.0411]],\n",
      "\n",
      "        [[0.2755, 0.0411],\n",
      "         [0.2755, 0.0411],\n",
      "         [0.2755, 0.0411]]])\n"
     ]
    }
   ],
   "source": [
    "a =     torch.ones(2, 3, 2)\n",
    "b = a * torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e7588b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:33:32.704899Z",
     "start_time": "2024-08-04T15:33:32.699753Z"
    }
   },
   "outputs": [],
   "source": [
    "# 典型错误\n",
    "a =     torch.ones(4, 3, 2)\n",
    "# b = a * torch.rand(4, 3)    # dimensions must match last-to-first\n",
    "# c = a * torch.rand(   2, 3) # both 3rd & 2nd dims different\n",
    "# d = a * torch.rand((0, ))   # can't broadcast with an empty tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1ac51",
   "metadata": {},
   "source": [
    "### I.3 inner product\n",
    "#### inner和matrix product执行乘法规则时的区别：\n",
    " - inner的两个矩阵的第二维length要一样:$$(n,k)\\times(m,k)=(n,m)$$\n",
    " - matrix乘法的两个矩阵是第一个的len(dim2)=第二个的len(dim1):$$(n,k)\\times(k,m)=(n,m)$$\n",
    " - 此外，矩阵乘法有broadcast，inner product不涉及broadcast的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c21e4c",
   "metadata": {},
   "source": [
    "1. torch.dot()：只支持两个1D tensor的inner product\n",
    "2. torch.inner(): \n",
    "   - 如果参数是两个scalar，做乘法\n",
    "   - 如果参数都不是scalar，那么两个参数的最后一维的length必须相同。\n",
    "     - 如果是两个1D tensor，和dot一样\n",
    "     - 如果参数中至少1个的维度>=2，他们在最后一维上做inner product，<font color=blue>**前面的维度不需要broadcast规则**</font> $$\n",
    "     (j,n,k)\\times(l,m,k)=(j, l, n,m) \\\\\n",
    "     (j,n,k)\\times(l,k)=(j, l, n)\n",
    "     $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b63e2f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:33:32.711551Z",
     "start_time": "2024-08-04T15:33:32.706236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14],\n",
       "         [15, 16, 17],\n",
       "         [18, 19, 20],\n",
       "         [21, 22, 23]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2, 3, dtype=torch.int)\n",
    "b = torch.arange(24, dtype=torch.int).view(2, 4, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8dda3d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:33:32.716519Z",
     "start_time": "2024-08-04T15:33:32.712955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 4]),\n",
       " tensor([[[ 3, 12, 21, 30],\n",
       "          [39, 48, 57, 66]],\n",
       " \n",
       "         [[ 3, 12, 21, 30],\n",
       "          [39, 48, 57, 66]]], dtype=torch.int32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.inner(a, b).shape, torch.inner(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6c7b6f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:33:32.721059Z",
     "start_time": "2024-08-04T15:33:32.717739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 2],\n",
       "        [2, 2, 2]], dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.inner(a, torch.tensor(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
