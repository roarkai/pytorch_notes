{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207a7305",
   "metadata": {},
   "source": [
    "# Extending Pytorch\n",
    "## 1. Extending torch.autograd\n",
    "### 1.1 用法理解\n",
    "1. 这里的扩展指的是以自定义autograd.Function的方式增加autograd处理的operation\n",
    "2. **使用场景：**\\\n",
    "(1)想在模型中使用不可导函数\\\n",
    "(2)虽然依赖non-Pytorch library(比如用numpy)来implement operation，但仍然想让operation可以chain with其他pytorch library提供的operations，并且使用autograd engine来做forward和backward。\\\n",
    "(3)为了提升内存利用率或者performance，使用了c++ extension来写operation，也可以wrap成Function来应用autograd engine。\\\n",
    "(4)为了减少内存占用，想要减少number of buffers saved for backward pass，也可以用自动以函数来combine ops together。\n",
    "3. **不要使用自定义Function的场景：**\\\n",
    "(1)pytorch函数库已经有想要执行的运算，而且已经可以record backward Graph，就没有必要自己写。 \\\n",
    "(2)如果只是想要maintain state，比如：trainable parameters，可以用自定义的module，而不需要自定义Function。 \\\n",
    "(3)如果想要改变pytorch库中函数在backward pass中计算gradient的方式，以实现其他的side effect，可以用registering a tensor或者registering a Module hook的方式实现，也没有必要额外定义函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff45e5",
   "metadata": {},
   "source": [
    "### 1.2 4. 实现方式\n",
    "**step1: 定义Function的子类，并在其中implement三个methods：forward(), setup_context(), backward()。**\n",
    "1. forward()：\\\n",
    "(1)任意python的数据类型都可以作为forward的input。\\\n",
    "(2)如果用tensor作为argument，并且该tensor要track history(设置了requires_grad=True)，那么在调用forward之前，会被转换成不需要track history的tensor，他们在forward中的应用也会计入graph。如果用list or dict of tensor，那么上述规则不会自动发生。\\\n",
    "(3)可以返回single tensor作为单一output，或者a tuple of tensors作为multiple outputs。\n",
    "2. setup_context()：只处理信息保存，不能做computation。\n",
    "3. backward(),即vjp()：定义梯度计算公式。\\\n",
    "(1)它收到的inputs数量应该和函数outputs数量相同，对应的就是这些outputs各自的梯度。一定不能对这些inputs做in-place modification。\\\n",
    "(2)它返回的output tensors数量应该和函数的inputs数量相同。对应的就是各个inputs的梯度。<font color=blue>如果有的inputs不需要计算梯度，或者他们不是tensor类型</font>，那么也要用None作为它的梯度返回值。\\\n",
    "(3)如果forward()中有optional argument，那么backward要预留返回值的位置，此时返回的gradient数量可能超过实际input arguments数量，但只要将他们都取值为None就行。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae4643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T13:05:42.848006Z",
     "start_time": "2023-10-22T13:05:42.836286Z"
    }
   },
   "source": [
    "**step2: 合理使用ctx，确保新定义的函数可以使用autograd engine。** \n",
    "1. **save_for_backward()**: 存放backward中要使用的tensor，non-tensor直接存在ctx上作为attribute。<font color=red>如果既不是input，又不是output的tensor要在ctx中存放供backward使用，则整个函数可能无法支持double backward。</font>\n",
    "2. <font color=green>**mark_dirty()**</font>: 如果有input被modified in-place，要用mark_dirty做标记\n",
    "3. <font color=green>**mark_non_differetiable()**</font>: 如果有的output无法求梯度（不可微），要用该method来做标记。默认所有可微的outputs tensor都会被设置为requires_grad=True。一旦mark后，就不会被设置为requires_grad=True。\n",
    "4. <font color=green>**set_materialize_grads()**</font>: 默认设置为True。当函数input的梯度与backward的arguments(dout)无关时，可以设参数为False优化梯度计算。用于告诉autograd engine not to materialize grad tensors given to backward function。也就是设置False后, backward的参数中的None object in Python or “undefined tensor” in C++将不会在调用backward之前被转变成<font colro=red>**全零tensor**</font>, 此时需要手动handle这些objects，as if they were tensors filled with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d8622",
   "metadata": {},
   "source": [
    "**step3: 如果函数不支持double backward，要对backward method用decorate once_differentiable()来明确声明。** \\\n",
    "**step4: 用torch.autograd.gradcheck()来检查backward的定义方式是否正确**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a2f6df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:14.290951Z",
     "start_time": "2023-10-23T05:57:13.128486Z"
    }
   },
   "outputs": [],
   "source": [
    "# 例1：自定义线性函数 y = x @ w.T + b\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input, weight, bias):\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    # inputs是传给forward的所有inputs构成的tuple\n",
    "    # output是forward()的output\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        input, weight, bias = inputs\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "\n",
    "    # 本例中的函数只有单一output, 所以backward只输出1个gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # unpack saved_tensors\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        \n",
    "        # inputs中所有元素的梯度都初始化为None，又additional trailing Nones会被忽略\n",
    "        # 所以当函数有optional inputs的时候，return statement可以很简单\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # 这里的条件判断optional，可以提升效率\n",
    "        if ctx.needs_input_grad[0]:   \n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        # 给不需要gradient的那些input也返回gradient并不会报错\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1678689",
   "metadata": {},
   "source": [
    "**使用自定义Function的两种常见方式：**\n",
    "1. 直接apply成一个函数\n",
    "2. wrap成一个新的函数：此时支持default args and keyword args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c75f07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:14.295171Z",
     "start_time": "2023-10-23T05:57:14.292578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Option 1: 直接apply一个函数\n",
    "linear = LinearFunction.apply\n",
    "\n",
    "# Option 2: wrap in a function\n",
    "def linear(input, weight, bias=None):\n",
    "    return LinearFunction.apply(input, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e745b817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:14.307626Z",
     "start_time": "2023-10-23T05:57:14.296275Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2，使用non-tensor argument: y = const * x\n",
    "class MulConstant(Function):\n",
    "    @staticmethod\n",
    "    def forward(tensor, constant):\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        # ctx是一个context object，用于为backward存储所需信息\n",
    "        tensor, constant = inputs\n",
    "        ctx.constant = constant   # constant直接存为ctx的attribute\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # 所有input都要有梯度返回值，non-Tensor arguments的梯度返回值为None\n",
    "        return grad_output * ctx.constant, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468a4dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:14.316312Z",
     "start_time": "2023-10-23T05:57:14.309297Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例3，对上例做优化：constant的梯度是None，与backward()的argument无关\n",
    "class MulConstant(Function):\n",
    "    @staticmethod\n",
    "    def forward(tensor, constant):\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        tensor, constant = inputs\n",
    "        # constant的梯度返回值是None，与backward的argument无关\n",
    "        # 所以设置set_materialize_grads(False)\n",
    "        ctx.set_materialize_grads(False)\n",
    "        ctx.constant = constant\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # 设置set_materialize_grads(False)后，\n",
    "        # 要手动处理grad_output为None的情形，本例中直接return None就行\n",
    "        if grad_output is None:\n",
    "            return None, None\n",
    "\n",
    "        return grad_output * ctx.constant, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade927a",
   "metadata": {},
   "source": [
    "**如果需要保存forward()中计算出来的中间值tensor**\n",
    "1. 有两种处理方式：\\\n",
    "(1)将其处理成forward的outputs。 <font color=blue>要计算高阶导数的时候，要用这种方式，同时还要将该tensor存到ctx.save_for_backward()中。</font> \\\n",
    "说明：\\\n",
    "backward的input，如grad_output, 也可能requires_grad=True。如果backward中用的operation是可微的，可以进一步计算高阶导数。但是ctx中存储的tensor本身并不会有gradients flowing back for them. 如果需要gradient flowing back到这些tensor上，就要将他们处理成output of the custom Function，同时存在ctx.save_for_backward()中. 这样，tensor就既能被backward()使用，又能有gradient flowing back to it。\\\n",
    "(2)联用forward和setup_context\n",
    "2. 如果计算图要通过该tensor，就要为他定义gradient的计算公式。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9059aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:14.322417Z",
     "start_time": "2023-10-23T05:57:14.317424Z"
    }
   },
   "outputs": [],
   "source": [
    "## 处理成forward的outputs\n",
    "# 例4：支持高阶导数\n",
    "class MyCube(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        # 要存dx在backward中使用，为此，在forward中将其作为返回值\n",
    "        dx = 3 * x ** 2\n",
    "        result = x ** 3\n",
    "        return result, dx\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x, = inputs\n",
    "        result, dx = output\n",
    "        ctx.save_for_backward(x, dx)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, grad_dx):\n",
    "        x, dx = ctx.saved_tensors\n",
    "        # In order for the autograd.Function to work with higher-order\n",
    "        # gradients, we must add the gradient contribution of `dx`,\n",
    "        # which is grad_dx * 6 * x.\n",
    "        result = grad_output * dx + grad_dx * 6 * x\n",
    "        return result\n",
    "\n",
    "# Wrap MyCube in a function以便确定唯一的output值\n",
    "def my_cube(x):\n",
    "    result, dx = MyCube.apply(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24ff75f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:14.703890Z",
     "start_time": "2023-10-23T05:57:14.323511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 例5：check gradient\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradcheck的输入要处理成a tuple of tensors\n",
    "input = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\n",
    "test = gradcheck(linear, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fac734c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:14.710231Z",
     "start_time": "2023-10-23T05:57:14.705102Z"
    }
   },
   "outputs": [],
   "source": [
    "# 例6：将forward和setup_context合并到forward中（not recommended）\n",
    "class LinearFunction(Function):\n",
    "    @staticmethod\n",
    "    # ctx is the first argument to forward\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        # The forward pass can use ctx.\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa43a7",
   "metadata": {},
   "source": [
    "## 2. Extending torch.nn\n",
    "nn输出两种类型的interface：modules和他们的functional version。\\\n",
    "extend nn可以用上述两种方式，但建议：\\\n",
    "(1)如果layer中有parameters或者buffers，用module。如：conv, affine. \\\n",
    "(2)如果没有，用function，比如activation function, pooling, relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08485b08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:14.764856Z",
     "start_time": "2023-10-23T05:57:14.711157Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # 1. nn.Parameter is a special kind of Tensor, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. \n",
    "        # 2. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # 3. nn.Parameters require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.empty(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        nn.init.uniform_(self.weight, -0.1, 0.1)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return LinearFunction.apply(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. \n",
    "        return 'input_features={}, output_features={}, bias={}'.format(\n",
    "            self.input_features, self.output_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7379a",
   "metadata": {},
   "source": [
    "## 3. Extending torch\n",
    "1. 目标：创建与tensor类似的自定义python type，既可以是与tensor所实现的操作相似但不相同的数据类型，也可以是tensor的子类型。在自定义的class中定义相应的同名methods，让pytorch中torch namespace里面原本接受tensor operands(操作数)的函数也能处理该自定义的数据类型。\n",
    "2. pytorch中提供的机制：如果自定义的python type class中定义了名为\\_\\_torch_function\\_\\_()的method，那么当自定义类型的实例作为参数传给torch namespace的函数的时候，pytorch就会invoke这个\\_\\_torch_function\\_\\_()。\\\n",
    "通过这种方式，可以自定义torch namespace中各种函数的implementation。当以自定义类型为参数来调用torch中的函数时，\\_\\_torch_function\\_\\_()实际上会调用这里自定义的implementation。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45941258",
   "metadata": {},
   "source": [
    "### 3.1 自定义与tensor类似的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "335ec6b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:14.772170Z",
     "start_time": "2023-10-23T05:57:14.765951Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例1：自定义一个2D scalar tensor\n",
    "#  1. n*n matrix，每个元素都是一个tensor scalar\n",
    "#  2. 对角线上的n个元素的值相同，由arguments决定，其他都是0\n",
    "\n",
    "class ScalarTensor(object):\n",
    "    def __init__(self, N, value):\n",
    "        self._N = N\n",
    "        self._value = value\n",
    "    def __repr__(self):\n",
    "        return f\"ScalarTensor(N={self._N}, value={self._value})\"\n",
    "    def tensor(self):\n",
    "        return self._value * torch.eye(self._N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f2fa57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:14.778386Z",
     "start_time": "2023-10-23T05:57:14.774404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalarTensor(N=3, value=2)\n",
      "tensor([[2., 0., 0.],\n",
      "        [0., 2., 0.],\n",
      "        [0., 0., 2.]])\n"
     ]
    }
   ],
   "source": [
    "d = ScalarTensor(3, 2)\n",
    "print(d)\n",
    "print(d.tensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74adf36f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:15.302211Z",
     "start_time": "2023-10-23T05:57:14.779478Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#  此时没有定义__torch_function__(),不支持torch operation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor"
     ]
    }
   ],
   "source": [
    "#  此时没有定义__torch_function__(),不支持torch operation\n",
    "torch.mean(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43208639",
   "metadata": {},
   "source": [
    "### 让自定义类型支持自定义torch operation\n",
    "1. 定义__torch_function__()，按照规则定义好4个参数 \\\n",
    "(1)func: reference to the torch API function that is being overridden \\\n",
    "(2)types: 说明__torch_function__ 支持的type list\\\n",
    "(3)args: 传递给function的arguments tuple \\\n",
    "(4)kwargs: 传递给function的keyword arguments dict\n",
    "2. 定义该类型数据上与torch operation匹配的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd1d81f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:25.709676Z",
     "start_time": "2023-10-23T05:57:25.705633Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2：让自定义类型支持自定义torch operation: torch.mean()\n",
    "\n",
    "# 1.指定该数据类型implement的operation，函数名要与torch namespace中的匹配\n",
    "HANDLED_FUNCTIONS = {}\n",
    "\n",
    "class ScalarTensor(object):\n",
    "    def __init__(self, N, value):\n",
    "        self._N = N\n",
    "        self._value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n",
    "\n",
    "    def tensor(self):\n",
    "        return self._value * torch.eye(self._N)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        if func not in HANDLED_FUNCTIONS or not all(\n",
    "            issubclass(t, (torch.Tensor, ScalarTensor))\n",
    "            for t in types\n",
    "        ):\n",
    "            return NotImplemented\n",
    "        return HANDLED_FUNCTIONS[func](*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca554bb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:28.250507Z",
     "start_time": "2023-10-23T05:57:28.247640Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2.定义operation的implementation，这里是torch.mean\n",
    "import functools\n",
    "\n",
    "# 定义一个decorator\n",
    "def implements(torch_function):\n",
    "    \"\"\"Register a torch function override for ScalarTensor\"\"\"\n",
    "    def decorator(func):\n",
    "        functools.update_wrapper(func, torch_function)\n",
    "        HANDLED_FUNCTIONS[torch_function] = func\n",
    "        return func\n",
    "    return decorator\n",
    "\n",
    "# 用decorator来实现具体函数的implementation\n",
    "@implements(torch.mean)\n",
    "def mean(input):\n",
    "    return float(input._value) / input._N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63800bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:30.060817Z",
     "start_time": "2023-10-23T05:57:30.056042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = ScalarTensor(5, 2)\n",
    "torch.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1af09377",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:31.334656Z",
     "start_time": "2023-10-23T05:57:31.331471Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. 函数operand有多个，其中有的是tensor，有的是自定义的类型\n",
    "def ensure_tensor(data):\n",
    "    # torch.tensor()通过深拷贝数据，构造一个新tensor\n",
    "    if isinstance(data, ScalarTensor):\n",
    "        return data.tensor()\n",
    "    # as_tensor可以将python对象转换为tensor\n",
    "    return torch.as_tensor(data)\n",
    "\n",
    "@implements(torch.add)\n",
    "def add(input, other):\n",
    "    # 如果输入都是ScalarTensor，加总两个底层tensor\n",
    "    try:\n",
    "        if input._N == other._N:\n",
    "            return ScalarTensor(input._N, input._value + other._value)\n",
    "        else:\n",
    "            raise ValueError(\"Shape mismatch!\")\n",
    "    # 如果两种输入中有ScalarTensor又有Tensor，要将scalarTensor转变成tensor\n",
    "    except AttributeError:\n",
    "        return torch.add(ensure_tensor(input), ensure_tensor(other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6c7df89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:32.118499Z",
     "start_time": "2023-10-23T05:57:32.115182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalarTensor(N=2, value=4)\n",
      "tensor([[3., 1.],\n",
      "        [1., 3.]])\n"
     ]
    }
   ],
   "source": [
    "s = ScalarTensor(2, 2)\n",
    "print(torch.add(s, s))\n",
    "\n",
    "t = torch.tensor([[1, 1,], [1, 1]])\n",
    "print(torch.add(s, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d707ceb",
   "metadata": {},
   "source": [
    "### 处理torch namespace中有，但是没有自定义函数implementation的情形\n",
    "1. 如果对自定义类型用torch namespace中的函数，但没有implement对应函数，会报错\n",
    "2. 一种不报错的处理方式是，判断自定义类型中有tensor method，调用该method把tensor传给torch原函数处理。<font color=red>注意这是因为自定义类型中一般都有tensor method。此时也要判断原生的运算规则是否符合自定义类型本身的需求。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b219e5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:57:33.832573Z",
     "start_time": "2023-10-23T05:57:33.819891Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no implementation found for 'torch.mul' on types that implement __torch_function__: [<class '__main__.ScalarTensor'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: no implementation found for 'torch.mul' on types that implement __torch_function__: [<class '__main__.ScalarTensor'>]"
     ]
    }
   ],
   "source": [
    "torch.mul(s, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a40d0b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:58:46.472417Z",
     "start_time": "2023-10-23T05:58:46.468360Z"
    }
   },
   "outputs": [],
   "source": [
    "# invoke tensor method生成tensor对象后，调用torch原有的函数\n",
    "class ScalarTensor(object):\n",
    "    def __init__(self, N, value):\n",
    "        self._N = N\n",
    "        self._value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n",
    "\n",
    "    def tensor(self):\n",
    "        return self._value * torch.eye(self._N)\n",
    "    \n",
    "    # 改变__torch_function__中的执行逻辑\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        if func not in HANDLED_FUNCTIONS or not all(\n",
    "                issubclass(t, (torch.Tensor, ScalarTensor))\n",
    "                for t in types\n",
    "            ):\n",
    "            args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]\n",
    "            return func(*args, **kwargs)\n",
    "        return HANDLED_FUNCTIONS[func](*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfa29e76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T05:58:47.400453Z",
     "start_time": "2023-10-23T05:58:47.396985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 0.],\n",
       "        [0., 4.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ScalarTensor(2, 2)\n",
    "torch.mul(s, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7410517",
   "metadata": {},
   "source": [
    "### 3.2 自定义tensor的子类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076e6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb7fb2f",
   "metadata": {},
   "source": [
    "### 3.3 自定义tensor wrapper类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f905ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
