{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71087c30",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/roarkai/pytorch_notes/blob/master/Autograd_mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640472d",
   "metadata": {
    "id": "1640472d"
   },
   "source": [
    "# Autograd\n",
    "\n",
    "## I. Autograd记录运算历史的方式\n",
    "1. Autograd是一个自动微分系统。从概念上看,在执行forward pass中的函数运算的同时，autograd会记录一个有向无环图(DAG),这个图上记录了所有执行过的operation。这个图上,leaf是input tensor,root是output tensor。在执行backward pass时，通过从root到leaf的方向tracing此图,就可以用链式法则自动计算梯度。\n",
    "\n",
    "2. **从具体的执行来看，autograd用Function objects graph来表达上述DAG。**\n",
    "\n",
    "  (1) tensor使用的function是**torch.autograd.Function**的实例。这些function class中都定义了forward和backward函数。用他们对tensor做运算时，会自动创建计算图。\n",
    "\n",
    "  (2) 可以apply()Function objects graph来计算评估图的结果。\n",
    "\n",
    "  (3) 计算Forward pass时,autograd在执行对应的function的同时，还会构建一个graph来表示这些将要计算梯度的function.每个torch.Tensor的.grad_fn属性都是进入此图的入口。完成Forward pass后,就可以在反向传播中evaluate the graph以计算梯度。\n",
    "\n",
    "3. **每个迭代都会从零开始重新创建图。** 这种设计是为了让forward pass中的运算过程可以使用任意Python control flow statements。因为一旦有了control flow statements，每次迭代中图的形状和大小就可能发生变化。每次迭代都重新创建图的好处是，在启动训练之前,不必编码所有可能的path。实际运行了哪些函数，最后就求哪些函数对应的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f231a993",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.315778Z",
     "start_time": "2023-10-14T15:59:06.108065Z"
    },
    "id": "f231a993"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)   # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)  # 要计算梯度\n",
    "b = torch.randn(3, requires_grad=True)     # 要计算梯度\n",
    "z = x @ w + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e680ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.321865Z",
     "start_time": "2023-10-14T15:59:07.318179Z"
    },
    "id": "35e680ea",
    "outputId": "0518f05c-c334-4352-d8c7-81f855dfea22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f4e84715660>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f4e847158d0>\n"
     ]
    }
   ],
   "source": [
    "# tensor的grad_fn属性中存放了ref to the BP function\n",
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ef5f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T07:31:14.170980Z",
     "start_time": "2023-10-09T07:31:14.151850Z"
    },
    "id": "f06ef5f0"
   },
   "source": [
    "### 计算梯度\n",
    "1. 只有计算图中leaf nodes(requires_grad设为True时)才有grad properties，其他nodes都没有。\n",
    "2. 一个计算图默认只能做一次BP。这是pytorch基于性能考虑设置的规则。如果要多次BP，要在backward call中设置参数retain_graph=True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da1af7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.354113Z",
     "start_time": "2023-10-14T15:59:07.323390Z"
    },
    "id": "8da1af7c",
    "outputId": "c3c5c844-0ffa-4042-c232-3ee1b02abd73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2144, 0.3061, 0.3074],\n",
      "        [0.2144, 0.3061, 0.3074],\n",
      "        [0.2144, 0.3061, 0.3074],\n",
      "        [0.2144, 0.3061, 0.3074],\n",
      "        [0.2144, 0.3061, 0.3074]])\n",
      "tensor([0.2144, 0.3061, 0.3074])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CBA_umhpLV-F",
   "metadata": {
    "id": "CBA_umhpLV-F"
   },
   "source": [
    "### backward所需的数据如何传递\n",
    "1. 自定义nn.autograd.Function时，在forward method中用save_for_backward()来存储backward中会用到的tensor data。这些数据在backward method中可以用saved_tensors() method提取。pytorch中已经定义好的函数也是通过这种方式向backward method传递data的。\n",
    "\n",
    "2. 在debug需要的时候，可以查看grad_fn对应的saved tensors。这些tensor以grad_fn的attribute形式存在，attribute name以'_saved'开头。\n",
    "3. pytorch在saving时，会将function中的saved tensor打包，在读取他们的时候再。unpack。但在读取他们的时候可能会因为要防止reference cycles，将tensor unpack成另一个tensor。此时读取出来的tensor和原来的tensor很可能不是同一个tensor。但是他们有共同的data storage。2.1版本中，只有tensor是其对应的grad_fn的output，才会这样。但这个implementation dettails在未来可能会变化。\n",
    "4. 可以通过hooks for saved tensors来控制pytorch如何packing/unpacking saved tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "huD3gpn7MT_h",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.360550Z",
     "start_time": "2023-10-14T15:59:07.356652Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huD3gpn7MT_h",
    "outputId": "afa4d7be-4456-4878-e6cf-0c05696be441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# save input of function\n",
    "x = torch.randn(5, requires_grad=True)\n",
    "y = x.pow(2)\n",
    "print(x.equal(y.grad_fn._saved_self)) #\n",
    "print(x is y.grad_fn._saved_self)     #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "oWgJkEvdNRKt",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.365714Z",
     "start_time": "2023-10-14T15:59:07.362068Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWgJkEvdNRKt",
    "outputId": "84949383-2a09-4cd3-f668-ccc7dd121c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# save output of function\n",
    "x = torch.randn(5, requires_grad=True)\n",
    "y = x.exp()\n",
    "print(x.equal(y.grad_fn._saved_result)) #\n",
    "print(x is y.grad_fn._saved_result)     #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eba1be",
   "metadata": {
    "id": "d7eba1be"
   },
   "source": [
    "## locally disabling gradient tracking/computation\n",
    "默认所有requires_grad=True的tensor都会被track computational history。但有的时候，不需要计算梯度，为了提高运算效率，可以disable gradient computation。\n",
    "\n",
    "· **不需要做gradient computation的典型场景：** \\\n",
    "  (1) model已经train好了，只做inference，不需要好用资源track history\\\n",
    "  (2) frozen参数来做finetune\n",
    "\n",
    "· **diable gradient tracking的两类方法：** \\\n",
    "  (1) disable entire block of code用context manager,比如：no-grad mode和inference mode\\\n",
    "  (2) 在更精细的范围内exclusion of sub-gragphs from gradient computation可以通过设置tensor的requires_grad值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Ltz7uu7SqoP",
   "metadata": {
    "id": "3Ltz7uu7SqoP"
   },
   "source": [
    "### 方法1：setting requires_grad\n",
    "1. tensor的requires_grad可以用来控制forward和backward中的gradient computation是否发生。requires_grad的默认是false，但wrapped in nn.Parameter的tensor的默认值是True。\n",
    "(1)Forward pass中，只有当operation中有tensor的requires_grad=True的时候才会被record到backward graph上。\n",
    "(2)Backward pass: 只有requires_grad=True的leaf tensor才会计算gradients并累积到tensor.grad上。所以，尽管每个tensor都设定了requires_grad的值，实际上只有leaf tensor的这个设定是有意义的。\n",
    "2. leaf tensor和non-leaf tensor的区别：\n",
    "(1)leaf: 没有grad_fn; 没有backward graph与之对应；requires_grad的值要设定，要么手动设定，要么nn.Parameter中自动设定。\n",
    "(2)non-leaf:有grad_fn; 有backward graph与之对应，因此他们的gradient只是作为intermediate result来计算requires_grad=True的leaf tensor的gradient；也因此，所有non-leaf tensor都自动设定了requires_grad=True，不需要手动设定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HGsWLS0Z0tXu",
   "metadata": {
    "id": "HGsWLS0Z0tXu"
   },
   "source": [
    "### 方法2：setting grad mode\n",
    "· pytorch中有3种grad model，他们决定了autograd处理gradient的方式：默认的是grad mode，此外还有no-grad mode和inference mode。他们都可以通过context manager和decorator来设置。\n",
    "1. **<font color=red>grad mode</font>:** 只有在该mode下，requires_grad的配置才能起作用，autograd才会record operations on the tensors。在另外两种mode下，requires_grad都会被overriden to be False。\n",
    "\n",
    "2. **<font color=red>no-grad mode</font>:** 让autograd暂时不记录operations（不将operations记录到backward graph上），退出该mode回到grad mode后再根据requires_grad的值是True还是False决定是否记录。\\\n",
    "  **· <font color=orange>适用场景：</font>** 只是暂时中断tensor operation的autograd，但tensor在当前operation中的运算结果还会被用于grad mode下的operation。\\\n",
    "  **· <font color=lightgreen>例子1, optimizer: </font>** 此时，weights' update不需要被autograd记录下来，但更新后的weights的值要用到下一次迭代中。而参与下一次迭代的forward运算时，这些weights是在grad mode下作计算的。\\\n",
    "  **· <font color=lightgreen>例子2, torch.nn.init</font>** 初始化operation是不需要计算梯度的。但是初始化完成后的training过程中，这些weights执行forward运算都是在grad mode中。\\\n",
    "\n",
    "3. **<font color=red>inference mode</font>:** 不是暂停让autograd记录tensor参与的operation，而是完全关闭autograd对operation的tracking。优点是tensor的运算速度会比no-grad mode更快。代价是inference mode下创建的tensor即使退出inference mode后，autograd也无法tracking它们参与的operations。\\\n",
    "  **· <font color=orange>适用场景：</font>** tensor参与的运算不需要记录到backward graph，并且这些运算中创建的tensor后续也不会用于其他需要被autograd记录的运算中去。\\\n",
    "  **· <font color=lightgreen>例子1, data processing: </font>** 不是模型的一部分，不做backward。\\\n",
    "  **· <font color=lightgreen>例子2, model evaluation：</font>** inference不做backward\\\n",
    "\n",
    "4. **<font color=red>evaluation mode</font>:** 这个不是disable autograd的机制，但容易和上述机制混淆。\\\n",
    "从功能上看，module.eval()和前面的no-grad和inference mode完全是不同的东西。理论上，model.eval()的作用取决于model中使用的modules，以及他们是否定义了trainning specific behavior。\\\n",
    "比如：model中用了torch.nn.Dropout或者torch.nn.Batchnorm，他们的特点是在trainning和validation/test时执行的operation不一样，这种情况下，就要手动在trainning和val/test阶段分别调用model.eval(), model.train()。\n",
    "tensor的detach method。\\\n",
    "· <font color=red>建议：不论有没有定义trainning specific behavior，都在trainning和val/test阶段分别调用model.eval(), model.train()。因为，model中使用的module即使在现在没有trainning specific behavior，未来可能会发生改变。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hoYnsrGSMvUA",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.369758Z",
     "start_time": "2023-10-14T15:59:07.367204Z"
    },
    "id": "hoYnsrGSMvUA"
   },
   "outputs": [],
   "source": [
    "## 不区分trainning和val/test (伪码)\n",
    "\n",
    "# for t in range(1000):\n",
    "#   y_pred_train = model(x_train)\n",
    "#   loss = loss(y_pred_train, y_train)\n",
    "#   loss.backward()\n",
    "#   opt.step()\n",
    "#   opt.zero_grad()\n",
    "\n",
    "#   with torch.no_grad():\n",
    "#     y_pred_val = model(x_val)\n",
    "#     loss = loss(y_pred_val, y_val)\n",
    "#     val_loss = sum(loss) / len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2jS8tw2M32F",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.373932Z",
     "start_time": "2023-10-14T15:59:07.371234Z"
    },
    "id": "e2jS8tw2M32F"
   },
   "outputs": [],
   "source": [
    "## 区分trainning和val/test (伪码)\n",
    "\n",
    "# for t in range(1000):\n",
    "#   model.train()\n",
    "#   y_pred_train = model(x_train)\n",
    "#   loss = loss(y_pred_train, y_train)\n",
    "#   loss.backward()\n",
    "#   opt.step()\n",
    "#   opt.zero_grad()\n",
    "\n",
    "#   model.eval()\n",
    "#   with torch.no_grad():\n",
    "#     y_pred_val = model(x_val)\n",
    "#     loss = loss(y_pred_val, y_val)\n",
    "#     val_loss = sum(loss) / len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c94307f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.379209Z",
     "start_time": "2023-10-14T15:59:07.375322Z"
    },
    "id": "c94307f3",
    "outputId": "7b2f40a0-fe88-4c46-c4c6-0e7e22343046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56a07ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.383909Z",
     "start_time": "2023-10-14T15:59:07.380635Z"
    },
    "id": "a56a07ad",
    "outputId": "14c2b6e5-0ad9-4778-fdab-2c63af7a43e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u2cXs8_kYLrI",
   "metadata": {
    "id": "u2cXs8_kYLrI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a683a70c",
   "metadata": {
    "id": "a683a70c"
   },
   "source": [
    "## directed acyclic graph(DAG)\n",
    "1. DAG由Function对象组成\n",
    "2. 叶节点是input tensor，根节点是output tensor，通过从root到leaves遍历graph，可以用chainrule自动计算gradients\n",
    "3. forward pass中，autograd会同时做两件事：\\\n",
    "(1)执行forward operation，计算tensor\\\n",
    "(2)将operation的gradient function存到DAG中\n",
    "4. 当.backward()被调用的时候，就会做backward pass。此时autograd会做3件事：\n",
    "(1)用每个.grad_fn计算梯度\\\n",
    "(2)将梯度累积到各自对应tensor的.grad属性中\\\n",
    "(3)使用chain rule，向leaf tensor传梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339eea6",
   "metadata": {
    "id": "7339eea6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5NE26ne7bcvO",
   "metadata": {
    "id": "5NE26ne7bcvO"
   },
   "source": [
    "# 自定义torch.autograd\n",
    "\n",
    "步骤：\\\n",
    "· step1: subclass torch.autograd.Function, implement forward(), setup_context(), and backward() methods. \\\n",
    "· step2: 在以ctx（context object）作为argument的method中设置好其内容。\\\n",
    "· step3: declare是否支持double backwards\\\n",
    "· step4: 做gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aPKDMrtZdyto",
   "metadata": {
    "id": "aPKDMrtZdyto"
   },
   "source": [
    "### step1：subclass Function\n",
    "1. forward\n",
    "2. ctx可以在forward中通过将ctx作为argument来对其设置。如果forward中没有设置ctx，就要用setup_context()。setup_context()只负责设置ctx，不做其他运算。\n",
    "3. backward定义了gradient formula。\\\n",
    "· backward() method的tensor arguments是upstream gradients，function的每个output对应1个upstream gradient所以backward函数的tensor arguments数量与本函数的output数量相同。\\\n",
    "· backward() method的输出是function inputs的gradient。所以，backward的output tensor数量就是function inputs的数量。如果有的input tensor不用求gradient，或者有的input不是tensor，也不用求gradient，那么它们对应的输出用python：None。如果forward()中有optional arguments，那么输出的数量可能超过function input的数量，此时，backward()输出的数量也会超过function inputs，但这些forward arguments对应在backward中的output都要取None。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OXh1vHUnjbTe",
   "metadata": {
    "id": "OXh1vHUnjbTe"
   },
   "source": [
    "### step2：use the function in ctx properly,确保Funtction在autograd engine中正确工作\n",
    "1. ctx.save_for_backward():存储backward pass中要用的tensor。\\\n",
    "· 如果要存储的data不是tensor，只能直接用ctx\\\n",
    "· 如果要存储的tensor既不是function的input也不是output，那么function就可能不支持double backward\n",
    "2. ctx.mark_dirty()：用来mark在backward pass中会发生in-place modify的function input。\n",
    "3. ctx.mark_none_differentiable()：用来标记function outputs是否需要求梯度。默认所有differentiable output tensors都设置为requires gradient。而不可导的那些则设置为不会被设置为requiures gradient。\n",
    "4. ctx.set_materialize_grads()：当output tensor的计算不依赖于input值时，可以通过not materialize grad tensors given to backward function来优化梯度的计算方式。默认设置是Ture。如果设置为False，python中的None不会在call backward之前被转化为全0的tensor，此时，要手动处理这些None objects。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruX3slVypfI7",
   "metadata": {
    "id": "ruX3slVypfI7"
   },
   "source": [
    "### step3：如果自定义的Function不支持double backward，要用decorating with once_differentiable()来声明\n",
    "### step4：用torch.autograd.gradcheck()验证梯度计算的正确性。方法是用backward function计算Jacobian matrix，将结果与使用数值计算的结果做比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gto7CDDRqE11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.391186Z",
     "start_time": "2023-10-14T15:59:07.386376Z"
    },
    "id": "gto7CDDRqE11"
   },
   "outputs": [],
   "source": [
    "## 例1：自定义LinearFunction\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "class LinearFunction(Function):\n",
    "  @staticmethod\n",
    "  def forward(input, weight, bias):\n",
    "    output = input @ weight.T\n",
    "    if bias is not None:\n",
    "      output += bias.unsqueeze(0).expand_as(output)\n",
    "    return output\n",
    "\n",
    "  @staticmethod\n",
    "  # inputs是所有传给forward()的inputs构成的tuple\n",
    "  # output是forward()的output\n",
    "  def setup_context(ctx, inputs, output):\n",
    "    input, weight, bias = inputs\n",
    "    ctx.save_for_backward(input, weight, bias)\n",
    "\n",
    "  @staticmethod\n",
    "  # grad_output是backward()唯一的输出\n",
    "  def backward(ctx, grad_output):\n",
    "    # unpack所有的saved_tensors\n",
    "    input, weight, bias = ctx.saved_tensors\n",
    "    # 初始化所有inputs的gradient为None，因为additional trailing Nones are ignored\n",
    "    # 因此，即使在Function有additional input的时候，return statement都可以很简单\n",
    "    grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "    # 增加的条件验证只是为了避免不必要的运算\n",
    "    if ctx.needs_input_grad[0]:\n",
    "      grad_input = grad_output.mm(weight)\n",
    "    if ctx.needs_input_grad[1]:\n",
    "      grad_weight = grad_output.t().mm(input)\n",
    "    if bias is not None and ctx.needs_input_grad[2]:\n",
    "      grad_bias = grad_output.sum(0)\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9165d055",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.395384Z",
     "start_time": "2023-10-14T15:59:07.392653Z"
    }
   },
   "outputs": [],
   "source": [
    "## 应用方式\n",
    "#  1. 最好wrapped into function形式\n",
    "def linear(input, weight, bias=None):\n",
    "    return LinearFunction.apply(input, weight, bias)\n",
    "\n",
    "#  2. 直接用\n",
    "linear = LinearFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1686be8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.400119Z",
     "start_time": "2023-10-14T15:59:07.396833Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2：自定义一个有non-tensor argument的函数\n",
    "class MulContent(Function):\n",
    "  @staticmethod\n",
    "  def forward(tensor, constant):\n",
    "    return tensor * constant\n",
    "\n",
    "  @staticmethod\n",
    "  # inputs是所有传给forward()的inputs构成的tuple\n",
    "  # output是forward()的output\n",
    "  def setup_context(ctx, inputs, output):\n",
    "    tensor, constant = inputs\n",
    "    ctx.constant = constant  # 以ctx.constant的形式存非tensor\n",
    "\n",
    "  @staticmethod\n",
    "  # grad_output是backward()唯一的输出\n",
    "  def backward(ctx, grad_output):    \n",
    "    return grad_output * ctx.constant, None # constant的梯度返回值用None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "221fe7a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.405317Z",
     "start_time": "2023-10-14T15:59:07.401656Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例3（优化上例）：calling set_materialize_grads(False):\n",
    "class MulContent(Function):\n",
    "  @staticmethod\n",
    "  def forward(tensor, constant):\n",
    "    return tensor * constant\n",
    "\n",
    "  @staticmethod\n",
    "  # inputs是所有传给forward()的inputs构成的tuple\n",
    "  # output是forward()的output\n",
    "  def setup_context(ctx, inputs, output):\n",
    "    tensor, constant = inputs\n",
    "    ctx.set_materialize_grads(False)\n",
    "    ctx.constant = constant  # 以ctx.constant的形式存非tensor\n",
    "\n",
    "  @staticmethod\n",
    "  # grad_output是backward()唯一的输出\n",
    "  def backward(ctx, grad_output):\n",
    "    # must handle None grad_output tensor\n",
    "    # 这样就可以skip不必要的计算，直接返回None\n",
    "    if grad_output is None:\n",
    "        return None, None\n",
    "    \n",
    "    return grad_output * ctx.constant, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b03773",
   "metadata": {},
   "source": [
    "如果需要save在forward中计算的intermediate tenosr，他们要么returned as output，要么combine forward和setup_context()。\n",
    "这意味着如果想要通过这些intermediate values来计算gradient，就要为这些intermediate values定义gradient formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62880a22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T15:59:07.410628Z",
     "start_time": "2023-10-14T15:59:07.406749Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例4：define gradient formula for intermediate values\n",
    "class MyCube(Function):\n",
    "  @staticmethod\n",
    "  def forward(x):\n",
    "    # 需要save x for backward，所以要将其return as output\n",
    "    dx = 3 * x ** 2\n",
    "    result = x ** 3\n",
    "    return result, dx\n",
    "\n",
    "  @staticmethod\n",
    "  # inputs是所有传给forward()的inputs构成的tuple\n",
    "  # output是forward()的output\n",
    "  def setup_context(ctx, inputs, output):\n",
    "    x, = inputs          # 因为inputs是tuple，所以只有1个元素的时候也要用'x,'\n",
    "    result, dx = output\n",
    "    ctx.save_for_backward(x, dx)\n",
    "\n",
    "  @staticmethod\n",
    "  # grad_output是backward()唯一的输出\n",
    "  def backward(ctx, grad_output): \n",
    "    x, dx = ctx.saved_tensors\n",
    "    result = grad_output * dx + grad_dx * 6 * x    \n",
    "    return result\n",
    "\n",
    "# wrap MyCube in a function\n",
    "def my_cube(x):\n",
    "    result, dx = MyCube.apply(x)\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
