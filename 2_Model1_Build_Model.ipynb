{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23724491",
   "metadata": {},
   "source": [
    "# Build model\n",
    "<font color=blue>[包含tutorial的Build model和developer note的Modules]</font>\n",
    "## 1. 什么是module，pytorch有哪些module类型？\n",
    " - module是构建神经网络的基础模块，pytorch用module来表达神经网络。\n",
    " - pytorch提供了一个modules库，也支持自定义modules。用他们可以很容易地构建多层神经网络。\n",
    "   - 具体实现来看，<font color=green>**namespace**</font> **torch.nn**提供了layers, containers和utilities三种主要的module类型，并使用**tensor类型的nn.Parameter**作为modules parameter。\n",
    "1. <font color=blue>**Layers：**</font>NN通过layers对数据进行操作。pytorch用modules来表达这些layers,比如conv, affine, pooling, normalization, transformer和loss functions等\n",
    "2. <font color=blue>**containers：**</font>有3类container，nn.Module，nn.Sequential和holders of submodules。\\\n",
    "(1)**torch.nn.Module**。它是所有NN modules的base class，pytorch中所有的module都是**nn.Module**的子类\\\n",
    "(2)**torch.nn.Sequential**：以序列形式将1个或多个module顺序排列，体现了module的nestable\\\n",
    "(3)holders of submodules,其中：**nn.ModuleList，nn.ModuleDict**分别是以list和dictionary类型存储的module序列。**nn.ParamterList和nn.ParameterDict**分别是以list和dictionary形式存储的参数。\n",
    "3. <font color=blue>**utilities：**</font>把一些数据处理的函数以modules的形式表达。\n",
    "\n",
    "## 2. module的特点\n",
    "1. module和autograd system一起工作：modules使optimizer update参数非常方便。因为module能在autograd system的管理下自动完成requires_grad=True的tensor的梯度计算，optimizer可以在此基础上自动工作。\n",
    "2. pytorch中的module可以nest：每个神经网络模型自身都是一个module，该module又由其他modules(layers)构成。这种nest structure可以很方便的构造复杂的网络架构。\n",
    "3. **nn.Module**的子类会自动track参数，可以用两个method来查看：parameters()和named_parameters()\n",
    "   - <font color=orange>module和autograd.Functional相比，可以自动维护module.parameters()中的参数。</font>\n",
    "4. 好用，而且也容易对module做各种调整：modules的save和restore都很直接，在CPU/GPU之间移动，做prune，quantize和其他很多操作都很方便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13ea8140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:30:05.887656Z",
     "start_time": "2024-08-05T15:30:05.881877Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn           # for torch.nn.Module\n",
    "import torch.nn.functional as F # for the activation function\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0e4f0",
   "metadata": {},
   "source": [
    "## 3. 自定义module实现不同的功能\n",
    "- 自定义model也得定义为**nn.Module**的子类，每个子类必须定义\\__init__和forward()两个method。\n",
    "   - 模型对input data的操作都放在forward()中。即，forward()用来指定要执行的computation，用的operation是nn.autograd.Function的子类的实例。这些子类可以是pytorch定义好的，也可以是自定义的。\n",
    "   - \\__init__()中要对operation涉及的参数做register，登记后，autograd system工作时就能track他们。\n",
    "     - register的方法是在\\__init\\__()中将parameter定义为nn.Parameter的实例。\n",
    "- Parameter class是torch.Tensor的子类，但他们可以被assigned as attributes of a Module。一旦实例化后，这些parameters就会被加到lists of the module's parameters，之后可以通过module.parameters()和model.namedparameters()来iterate throgh。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67326e23",
   "metadata": {},
   "source": [
    "- <font color=green>**用nn.Mudule来实例化module时，只implement forward() method不用implement backward() method**。</font>因为：\n",
    "   - <font color=blue>用nn.autograd.Function来实例化（自定义）Function时，要同时implement forward() and backward() methods</font> \n",
    "   - <font color=blue>autograd system会用Function中的backward来自动处理module中用到的function的backward pass。</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be88166",
   "metadata": {},
   "source": [
    "### 3.1 自定义一个简单的layer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2951b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.873804Z",
     "start_time": "2024-08-05T15:24:38.870074Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):  # 必须是nn.Module的子类\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        # registering parameters: 参数定义成nn.Parameter的实例\n",
    "        # 此时autograd会自动tracking并让optimizer在迭代时update\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "  # implement forward() method\n",
    "    def forward(self, input):\n",
    "        return input @ self.weight + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63c55dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.894019Z",
     "start_time": "2024-08-05T15:24:38.875152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6408, -1.0079, -0.5340], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyLinear(4, 3)          \n",
    "sample_input = torch.randn(4)  \n",
    "\n",
    "# model is callable, calling invoke forward function\n",
    "model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e2b36d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.900941Z",
     "start_time": "2024-08-05T15:24:38.896044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.7721,  0.8088,  0.4646],\n",
      "        [ 0.3230,  0.1650,  0.5115],\n",
      "        [ 0.2285, -0.7809, -0.5223],\n",
      "        [-0.4120, -0.9987,  1.0269]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5329, -0.1094,  0.2147], requires_grad=True)\n",
      "\n",
      "\n",
      "('weight', Parameter containing:\n",
      "tensor([[ 0.7721,  0.8088,  0.4646],\n",
      "        [ 0.3230,  0.1650,  0.5115],\n",
      "        [ 0.2285, -0.7809, -0.5223],\n",
      "        [-0.4120, -0.9987,  1.0269]], requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([-0.5329, -0.1094,  0.2147], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "## 遍历parameters()\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "## 遍历parameters named_parameters()\n",
    "#  这里weights和bias是parameter的name\n",
    "for parameter in model.named_parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f145b3d",
   "metadata": {},
   "source": [
    "### 3.2 将modules作为模型的基础模块(building blocks)\n",
    "· modules contain other modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297ab23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T09:10:48.862543Z",
     "start_time": "2023-10-20T09:10:48.859143Z"
    }
   },
   "source": [
    "#### i. 用nn.Sequential定义一个简单的module\n",
    "· Sequential会自动将上一层的输出传给下一层作为输入。<font color=red>但只在输入和输出都是单变量的情况时有效。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b22cb2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.907267Z",
     "start_time": "2024-08-05T15:24:38.902246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1253], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Sequential本身也是nn.Module的子类，所以实例化得到的也是module\n",
    "net = nn.Sequential(\n",
    "    MyLinear(4, 3),\n",
    "    nn.ReLU(),\n",
    "    MyLinear(3, 1)\n",
    ")\n",
    "\n",
    "simple_input = torch.randn(4)\n",
    "net(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87baec6",
   "metadata": {},
   "source": [
    "####  ii. 自定义module\n",
    "- 除了上面例子中非常简单的案例，通常都不建议直接用Sequential来定义module，最好自定义module.\n",
    "- 在__init__()中定义的submodule对应神经网络中的layer，用的也就是toch.nn中或者自定义的layer module。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9106542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.911888Z",
     "start_time": "2024-08-05T15:24:38.908418Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = MyLinear(4, 3)\n",
    "        self.layer1 = MyLinear(3, 1)  # 定义了两个submodule\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = F.relu(x)                 # relu不是submodule，是function\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e718a358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.916750Z",
     "start_time": "2024-08-05T15:24:38.912968Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = MyLinear(4, 3)\n",
    "        self.relu = nn.ReLU()         # 用的nn中的ReLU module\n",
    "        self.layer1 = MyLinear(3, 1)  # 定义了3个submodule\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.relu(x)              # 这里relu是submodule\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "# 可以这么处理，但不必要，因为ReLU没有参数，没必要用module，就function就好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f029af",
   "metadata": {},
   "source": [
    "- **可以用children() or named_children()来iterate遍历module的immediate children**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ed2044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.921393Z",
     "start_time": "2024-08-05T15:24:38.918004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('layer0', MyLinear())\n",
      "('layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "for child in net.named_children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42b29f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.925156Z",
     "start_time": "2024-08-05T15:24:38.922475Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对比前面例子中直接用tensor operation定义的module\n",
    "# 此时module中没有child\n",
    "for child in model.children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0886481a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.931258Z",
     "start_time": "2024-08-05T15:24:38.927985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('layer0', MyLinear())\n",
      "('relu', ReLU())\n",
      "('layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "# 也可以把relu处理成module\n",
    "net2 = Net2()\n",
    "for child in net2.named_children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd1733",
   "metadata": {},
   "source": [
    "- 如果不止想查看immediate children，想深入看所有nested的module，就用**modules() and named_modules() recursively iterate through a module and its child modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2152ffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.935996Z",
     "start_time": "2024-08-05T15:24:38.932590Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BigNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = MyLinear(5, 4)\n",
    "        self.net = Net()\n",
    "    def forward(self, x):\n",
    "        return self.net(self.l1(x))\n",
    "\n",
    "big_net = BigNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c6d8ac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.940318Z",
     "start_time": "2024-08-05T15:24:38.937310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLinear()\n",
      "Net(\n",
      "  (layer0): MyLinear()\n",
      "  (layer1): MyLinear()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 只看immediate children\n",
    "for child in big_net.children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55fab76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.944819Z",
     "start_time": "2024-08-05T15:24:38.941483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "('', BigNet(\n",
      "  (l1): MyLinear()\n",
      "  (net): Net(\n",
      "    (layer0): MyLinear()\n",
      "    (layer1): MyLinear()\n",
      "  )\n",
      "))\n",
      "----------------------------------------------------\n",
      "('l1', MyLinear())\n",
      "----------------------------------------------------\n",
      "('net', Net(\n",
      "  (layer0): MyLinear()\n",
      "  (layer1): MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('net.layer0', MyLinear())\n",
      "----------------------------------------------------\n",
      "('net.layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "# 看所有nested children\n",
    "for module in big_net.named_modules():\n",
    "    print('-' * 52)\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0e3a1",
   "metadata": {},
   "source": [
    "#### iii. dynamically define submodule\n",
    "· 用ModuleList或者ModuleDict \\\n",
    "· calls to parameters() and named_parameters() will recursively include child parameters, allowing for convenient optimization of all parameters within the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551de685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.950461Z",
     "start_time": "2024-08-05T15:24:38.946044Z"
    }
   },
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList(\n",
    "            [MyLinear(4, 4) for _ in range(num_layers)])\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU()\n",
    "        })\n",
    "        self.final = MyLinear(4, 1)\n",
    "        \n",
    "    def forward(self, x, act):\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "        x = self.activations[act](x)\n",
    "        # x = self.final(x)\n",
    "        return x\n",
    "\n",
    "dynamic_net = DynamicNet(3)\n",
    "sample_input = torch.randn(4)\n",
    "output = dynamic_net(sample_input, 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af90dd5",
   "metadata": {},
   "source": [
    "- **child module由__init__()中排列的module sequence决定，不由forward()实际执行的computation决定**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f75c3991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.954176Z",
     "start_time": "2024-08-05T15:24:38.951668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "('', DynamicNet(\n",
      "  (linears): ModuleList(\n",
      "    (0-2): 3 x MyLinear()\n",
      "  )\n",
      "  (activations): ModuleDict(\n",
      "    (relu): ReLU()\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (final): MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('linears', ModuleList(\n",
      "  (0-2): 3 x MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('linears.0', MyLinear())\n",
      "----------------------------------------------------\n",
      "('linears.1', MyLinear())\n",
      "----------------------------------------------------\n",
      "('linears.2', MyLinear())\n",
      "----------------------------------------------------\n",
      "('activations', ModuleDict(\n",
      "  (relu): ReLU()\n",
      "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "))\n",
      "----------------------------------------------------\n",
      "('activations.relu', ReLU())\n",
      "----------------------------------------------------\n",
      "('activations.lrelu', LeakyReLU(negative_slope=0.01))\n",
      "----------------------------------------------------\n",
      "('final', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "for module in dynamic_net.named_modules():\n",
    "    print('-'*52)\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f225eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:38.960185Z",
     "start_time": "2024-08-05T15:24:38.955366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "('linears.0.weight', Parameter containing:\n",
      "tensor([[ 1.8950,  2.5236,  0.7107, -1.8043],\n",
      "        [-0.5830,  1.3449,  0.0872,  0.8069],\n",
      "        [-0.9661,  2.2346, -1.2900, -0.5680],\n",
      "        [ 0.2744,  0.1565,  0.0752,  0.8783]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.0.bias', Parameter containing:\n",
      "tensor([-0.0155,  0.0646, -0.2567,  0.2446], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.1.weight', Parameter containing:\n",
      "tensor([[ 0.2363, -1.4645,  0.5775,  0.1964],\n",
      "        [-1.1402, -1.0531, -0.1585,  0.0966],\n",
      "        [ 0.5598, -0.5302,  0.7613, -0.1086],\n",
      "        [-0.9517,  0.2512, -0.3361, -0.6253]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.1.bias', Parameter containing:\n",
      "tensor([ 0.2948,  0.1851,  0.5885, -0.8602], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.2.weight', Parameter containing:\n",
      "tensor([[ 1.6807, -1.9788,  2.3324, -1.8978],\n",
      "        [ 2.3916,  0.5876, -0.4144, -0.1259],\n",
      "        [-1.4973, -0.0936,  0.7614, -1.8076],\n",
      "        [-0.3657, -1.5113, -0.2865,  0.2089]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.2.bias', Parameter containing:\n",
      "tensor([-0.1900,  0.3751,  0.5673, -0.1551], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('final.weight', Parameter containing:\n",
      "tensor([[ 0.6417],\n",
      "        [-0.2497],\n",
      "        [-0.2828],\n",
      "        [-2.6735]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('final.bias', Parameter containing:\n",
      "tensor([0.3235], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for parameter in dynamic_net.named_parameters():\n",
    "    print('-'*68)\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a485448",
   "metadata": {},
   "source": [
    "#### vi. 移动参数的设备，改变参数精度，用.to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3ebc9d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:39.298436Z",
     "start_time": "2024-08-05T15:24:38.961310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 5.4312, 0.0000], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move all parameters to a CUDA device\n",
    "dynamic_net.to(device='cuda')\n",
    "\n",
    "# Change precision of all parameters\n",
    "dynamic_net.to(dtype=torch.float64)\n",
    "\n",
    "dynamic_net(torch.randn(4, device='cuda', dtype=torch.float64), 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555f4a6",
   "metadata": {},
   "source": [
    "#### v. module和submodule可以apply任意函数，包括自定义函数\n",
    "an arbitrary function can be applied to a module and its submodules recursively by using the apply() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56d8daee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:39.303699Z",
     "start_time": "2024-08-05T15:24:39.299722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicNet(\n",
       "  (linears): ModuleList(\n",
       "    (0-2): 3 x MyLinear()\n",
       "  )\n",
       "  (activations): ModuleDict(\n",
       "    (relu): ReLU()\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (final): MyLinear()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to initialize Linear weights.\n",
    "# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.\n",
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.fill_(0.0)\n",
    "\n",
    "# Apply the function recursively on the module and its submodules.\n",
    "dynamic_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f63538",
   "metadata": {},
   "source": [
    "## 4. 使用module训练NN\n",
    "**module有两种mode：trainning mode和evaluation mode**\n",
    "1. module默认处于training mode。用training()和eval()可以改变module所处mode。\n",
    "2. 如果module中有submodule在training mode和evaluation mode的时候输出不同，那么就应该在inference的时候将mode改为evaluation mode，比如batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85d6837a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:42.998590Z",
     "start_time": "2024-08-05T15:24:39.304835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer0): MyLinear()\n",
       "  (layer1): MyLinear()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新建network和optimizer\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, \n",
    "                            weight_decay=1e-2, momentum=0.9)\n",
    "\n",
    "# trainging the netword\n",
    "for _ in range(10000):\n",
    "    input = torch.randn(4)\n",
    "    output = net(input)\n",
    "    loss = torch.abs(output) # 用abs做loss，会让weights趋于0\n",
    "    \n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# training完成后，将module转到eval mode\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a99c86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.004984Z",
     "start_time": "2024-08-05T15:24:43.000433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.6042],\n",
      "        [-0.6223],\n",
      "        [ 0.7621]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.layer1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31db1caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.019160Z",
     "start_time": "2024-08-05T15:24:43.006111Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training mode output: tensor([0.6138, 0.0766, 1.3739, 0.0727])\n",
      "evaluation mode output: tensor([-0.3862, -0.9234,  0.3739, -0.9273])\n"
     ]
    }
   ],
   "source": [
    "# 在training和evaluation mode下输出不同的例子\n",
    "class ModalModule(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.training:\n",
    "      # Add a constant only in training mode.\n",
    "      return x + 1.\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "m = ModalModule()\n",
    "x = torch.randn(4)\n",
    "print('training mode output: {}'.format(m(x)))\n",
    "\n",
    "m.eval()\n",
    "print('evaluation mode output: {}'.format(m(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47a97f",
   "metadata": {},
   "source": [
    "## 5. module state\n",
    "1. 如果要保存a trained model，可以存该module的state_dict，state_dict中保存了影响module运算的状态。state包括parameters和buffers。\\\n",
    "(1)**parameters**: learnable aspects of computation,存在state_dict中。\\\n",
    "(2)**buffers**: non-learnable aspects of computation. 有的module会存储参数之外的其他信息到state_dict，和参数不同的是，这些信息不需要learn，他们会被存在buffers中。\n",
    "2. 有两种buffers：Persistent buffers存在state_dict中，non-Persistent buffers不存在state_dict中。\\\n",
    "(1)Persistent buffers: 比如：serialized when saving and loading\n",
    "(2)non-Persistent buffers: 比如：left out of serialization\n",
    "3. Persistent buffers的特点：\\\n",
    "(1)如果state被存为state_dict的一部分，那么loading a serialized form of the module的时候，它就能被restore。 \\\n",
    "(2)这部分变量不会像parameters那样被optimizer处理，因而是non-leanable\n",
    "4. non-Persistent buffers的特点：\\\n",
    "不存为state_dict的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6a3e8b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.027697Z",
     "start_time": "2024-08-05T15:24:43.020339Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25855/2257276058.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  new_net.load_state_dict(torch.load('net.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save the module\n",
    "torch.save(net.state_dict(), 'net.pt')\n",
    "\n",
    "## load the module\n",
    "#  1. 新建一个结构相同的module\n",
    "new_net = Net()\n",
    "#  2. load state\n",
    "new_net.load_state_dict(torch.load('net.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec5290bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.032278Z",
     "start_time": "2024-08-05T15:24:43.028851Z"
    }
   },
   "outputs": [],
   "source": [
    "## 使用buffers：module中要保存running mean\n",
    "#  将running mean的当前值存到state_dict用register_buffer()\n",
    "\n",
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('mean', torch.zeros(num_features))\n",
    "        # 此时，self.mean会被存到state_dict中\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 每次迭代时更新running mean的值\n",
    "        # 作为state_dict的一部分，当loading module的时候会被restore\n",
    "        self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n",
    "        return self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14a2fe3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.041364Z",
     "start_time": "2024-08-05T15:24:43.033425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))])\n",
      "tensor(True)\n",
      "tensor([True, True, True, True])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25855/830558868.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  m_loaded.load_state_dict(torch.load('mean.pt'))\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "m = RunningMean(4)\n",
    "for _ in range(10):\n",
    "    input = torch.randn(4)\n",
    "    m(input)\n",
    "\n",
    "print(m.state_dict())\n",
    "\n",
    "# Serialized form will contain the 'mean' tensor\n",
    "torch.save(m.state_dict(), 'mean.pt')\n",
    "\n",
    "m_loaded = RunningMean(4)\n",
    "m_loaded.load_state_dict(torch.load('mean.pt'))\n",
    "\n",
    "# 注意这里几种assert和print的差异\n",
    "assert(torch.all(m.mean == m_loaded.mean))\n",
    "print(torch.all(m.mean == m_loaded.mean))\n",
    "print(m.mean == m_loaded.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3629e24d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.048676Z",
     "start_time": "2024-08-05T15:24:43.042900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "tensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25855/603031504.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  m2_loaded.load_state_dict(torch.load('mean.pt'))\n"
     ]
    }
   ],
   "source": [
    "## 将running mean存为non-Persistent buffers\n",
    "#  还是用register_buffer()，参数Persistent=False\n",
    "\n",
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('mean', torch.zeros(num_features), persistent=False)\n",
    "        # 此时，self.mean不会被存到state_dict中\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n",
    "        return self.mean\n",
    "\n",
    "torch.manual_seed(0)\n",
    "m2 = RunningMean(4)\n",
    "for _ in range(10):\n",
    "    input = torch.randn(4)\n",
    "    m2(input)\n",
    "\n",
    "print(m2.state_dict()) # 此时输出的state_dict是空的\n",
    "\n",
    "torch.save(m2.state_dict(), 'mean.pt')\n",
    "m2_loaded = RunningMean(4)\n",
    "m2_loaded.load_state_dict(torch.load('mean.pt'))\n",
    "print(torch.all(m2.mean == m2_loaded.mean)) # 输出False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6f7b2",
   "metadata": {},
   "source": [
    "#### 一个module的buffers可以用buffers()和named_buffers()来迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "082c83e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.052409Z",
     "start_time": "2024-08-05T15:24:43.049757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))\n"
     ]
    }
   ],
   "source": [
    "for buffer in m.named_buffers():\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20216c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.056022Z",
     "start_time": "2024-08-05T15:24:43.053461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))\n"
     ]
    }
   ],
   "source": [
    "for buffer in m2.named_buffers():\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f872834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:42:48.068015Z",
     "start_time": "2023-10-21T10:42:48.065554Z"
    }
   },
   "source": [
    "#### 两种buffers都受model-wide device/type changes所使用的.to() method影响\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84ffd956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.062294Z",
     "start_time": "2024-08-05T15:24:43.059091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunningMean()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.to(device='cuda', dtype=torch.float64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80cf5f4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.073715Z",
     "start_time": "2024-08-05T15:24:43.063254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('param1', tensor([-0.0404,  0.2881])), ('param2', tensor([-0.0075, -0.9145, -1.0886])), ('buffer1', tensor([ 1.3232,  0.0371, -0.2849, -0.1334])), ('param_list.0', tensor([-0.2666,  0.1894])), ('param_list.1', tensor([-0.2190,  2.0576])), ('param_list.2', tensor([-0.0354,  0.0627])), ('param_dict.bar', tensor([ 0.1753, -0.9315, -1.5055, -0.6610])), ('param_dict.foo', tensor([-0.7663,  1.0993,  2.7565])), ('linear.weight', tensor([[ 0.0197, -0.0610],\n",
      "        [ 0.1431,  0.4496],\n",
      "        [ 0.6698,  0.4491]])), ('linear.bias', tensor([ 0.6713, -0.0511, -0.6352]))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25855/1798499057.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  m_loaded.load_state_dict(torch.load('state.pt'))\n"
     ]
    }
   ],
   "source": [
    "## 一个综合例子\n",
    "class StatefulModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 用nn.Parameter实例化的参数会自动将tensor register为module parameter\n",
    "        self.param1 = nn.Parameter(torch.randn(2))\n",
    "\n",
    "        # 另一种将tensor register为module parameter的方式：用register_parameter() method\n",
    "        self.register_parameter('param2', nn.Parameter(torch.randn(3)))\n",
    "\n",
    "        # 将attribute： \"param3\" 定义为一个parameter，但不做初始化。\n",
    "        # 它的值'None'不会出现在state_dict中    \n",
    "        self.register_parameter('param3', None)\n",
    "\n",
    "        # Registers a list of parameters：没有name\n",
    "        self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])\n",
    "\n",
    "        # Registers a dictionary of parameters：有name\n",
    "        self.param_dict = nn.ParameterDict({\n",
    "            'foo': nn.Parameter(torch.randn(3)),\n",
    "            'bar': nn.Parameter(torch.randn(4))\n",
    "        })\n",
    "\n",
    "        # Registers a persistent buffer\n",
    "        self.register_buffer('buffer1', torch.randn(4), persistent=True)\n",
    "\n",
    "        # Registers a non-persistent buffer\n",
    "        self.register_buffer('buffer2', torch.randn(5), persistent=False)\n",
    "\n",
    "        # 将attribute：\"buffer3\" 定义为一个buffer，但不做初始化\n",
    "        # 它的值'None'也不会出现在state_dict中    \n",
    "        self.register_buffer('buffer3', None)\n",
    "\n",
    "        # 添加一个submodule就会将其parameters自动register为module的parameters\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "\n",
    "m = StatefulModule()\n",
    "\n",
    "# Save and load state_dict.\n",
    "torch.save(m.state_dict(), 'state.pt')\n",
    "m_loaded = StatefulModule()\n",
    "m_loaded.load_state_dict(torch.load('state.pt'))\n",
    "\n",
    "# state_dict中没有non-persistent buffer和reserved attributes \"param3\"与\"buffer3\"\n",
    "print(m_loaded.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405a4ab",
   "metadata": {},
   "source": [
    "## 6. module初始化\n",
    "1. 默认情况下，torch.nn提供的module中的parameter和浮点数buffer会在module实例化的时候初始化为存在CPU上的32位浮点数值。\n",
    "2. 如果要改变默认的初始化设置，可以在module实例化的时候设置对应的arguments或者直接用skip_init()method，之后自定义初始化方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62b1949b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.077528Z",
     "start_time": "2024-08-05T15:24:43.074740Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将module直接初始化到GPU上，参数类型为16位浮点数\n",
    "m = nn.Linear(5, 3, device='cuda', dtype=torch.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ae1ae7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.081628Z",
     "start_time": "2024-08-05T15:24:43.078600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# 除参数外，上述初始化方式也适用于floating-point buffers registered for the module\n",
    "m = nn.BatchNorm2d(3, dtype=torch.half)\n",
    "print(m.running_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69ec708b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.088674Z",
     "start_time": "2024-08-05T15:24:43.082682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3930, -0.1617,  0.4533, -0.0858,  0.7788],\n",
       "        [ 0.0826,  0.2477,  0.6222, -0.6544, -0.3411],\n",
       "        [ 0.1306,  0.3920,  0.5422,  0.7263, -0.0882]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例：自定义参数初始化为正交矩阵\n",
    "m = torch.nn.utils.skip_init(nn.Linear, 5, 3)\n",
    "nn.init.orthogonal_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13a7e1",
   "metadata": {},
   "source": [
    "#### 自定义module的时候，建议按照torch.nn所遵守的规则那样：\n",
    "1. 提供一个device constructor kwarg，可以应用在任意的parameter和buffers registered by the module上\n",
    "2. 提供一个dtype constructor kwarg，可以应用在任意的parameter和floating-point buffers registered by the module上\n",
    "3. 只用初始化函数（比如：torch.nn.init package提供的函数）来初始化module constructor中的parameters和buffers。注意，此时要使用skip_init()。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337005f",
   "metadata": {},
   "source": [
    "## 7. torch自带module中的典型layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d35d19",
   "metadata": {},
   "source": [
    "### nn.Flatten\n",
    "1. 参数：torch.nn.Flatten(start_dim=1, end_dim=-1)\n",
    "2. 压缩[start_dim, end_dim]范围的dims\n",
    "2. 默认将输入的data压成2维数据，保留原第一维，压缩剩下的维度，比如输出(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d741eb78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.093292Z",
     "start_time": "2024-08-05T15:24:43.089977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 784])\n",
      "torch.Size([84, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "flatten2 = nn.Flatten(0, 1)  # 压缩[0, 1]范围的dims\n",
    "flat_image2 = flatten2(input_image)\n",
    "print(flat_image2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15422dc0",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "1. affine layer\n",
    "2. 参数：torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "   · in_features (int) – size of each input sample\n",
    "   · out_features (int) – size of each output sample\n",
    "   · bias (bool)取False时, 就不会learn bias. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2975059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.097875Z",
     "start_time": "2024-08-05T15:24:43.094525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=6)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107dfd54",
   "metadata": {},
   "source": [
    "### nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9664ae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.102751Z",
     "start_time": "2024-08-05T15:24:43.099121Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU:\n",
      " tensor([[-0.1059, -0.3650, -0.2354,  0.1382,  0.0520,  0.0531],\n",
      "        [-0.2357, -0.4797, -0.2849, -0.0148,  0.2071,  0.2770],\n",
      "        [-0.0447, -0.2520, -0.0628,  0.0251,  0.4561,  0.1185]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "After ReLU:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.1382, 0.0520, 0.0531],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2071, 0.2770],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0251, 0.4561, 0.1185]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU:\\n {hidden1}\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU:\\n {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c6442",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "1. an ordered container of modules.\n",
    "2. 数据会按照Sequential中定义的layer顺序做处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88abdeb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T15:24:43.107011Z",
     "start_time": "2024-08-05T15:24:43.104038Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "scores = seq_modules(input_image)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
