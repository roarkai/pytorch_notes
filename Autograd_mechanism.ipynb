{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roarkai/pytorch_notes/blob/master/Autograd_mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1640472d",
      "metadata": {
        "id": "1640472d"
      },
      "source": [
        "# Autograd\n",
        "\n",
        "## I. Autograd记录运算历史的方式\n",
        "1. Autograd是一个自动微分系统。从概念上看,在执行forward pass中的函数运算的同时，autograd会记录一个有向无环图(DAG),这个图上记录了所有执行过的operation。这个图上,leaf是input tensor,root是output tensor。在执行backward pass时，通过从root到leaf的方向tracing此图,就可以用链式法则自动计算梯度。\n",
        "\n",
        "2. **从具体的执行来看，autograd用Function objects graph来表达上述DAG。**\n",
        "\n",
        "  (1) tensor使用的function是**torch.autograd.Function**的实例。这些function class中都定义了forward和backward函数。用他们对tensor做运算时，会自动创建计算图。\n",
        "\n",
        "  (2) 可以apply()Function objects graph来计算评估图的结果。\n",
        "\n",
        "  (3) 计算Forward pass时,autograd在执行对应的function的同时，还会构建一个graph来表示这些将要计算梯度的function.每个torch.Tensor的.grad_fn属性都是进入此图的入口。完成Forward pass后,就可以在反向传播中evaluate the graph以计算梯度。\n",
        "\n",
        "3. **每个迭代都会从零开始重新创建图。** 这种设计是为了让forward pass中的运算过程可以使用任意Python control flow statements。因为一旦有了control flow statements，每次迭代中图的形状和大小就可能发生变化。每次迭代都重新创建图的好处是，在启动训练之前,不必编码所有可能的path。实际运行了哪些函数，最后就求哪些函数对应的梯度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f231a993",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-11T12:15:55.801260Z",
          "start_time": "2023-10-11T12:15:54.577429Z"
        },
        "id": "f231a993"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)   # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)  # 要计算梯度\n",
        "b = torch.randn(3, requires_grad=True)     # 要计算梯度\n",
        "z = x @ w + b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e680ea",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-11T12:16:00.031494Z",
          "start_time": "2023-10-11T12:16:00.028014Z"
        },
        "id": "35e680ea",
        "outputId": "0518f05c-c334-4352-d8c7-81f855dfea22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7f597c1a00a0>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f597c1a0c40>\n"
          ]
        }
      ],
      "source": [
        "# tensor的grad_fn属性中存放了ref to the BP function\n",
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f06ef5f0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-09T07:31:14.170980Z",
          "start_time": "2023-10-09T07:31:14.151850Z"
        },
        "id": "f06ef5f0"
      },
      "source": [
        "### 计算梯度\n",
        "1. 只有计算图中leaf nodes(requires_grad设为True时)才有grad properties，其他nodes都没有。\n",
        "2. 一个计算图默认只能做一次BP。这是pytorch基于性能考虑设置的规则。如果要多次BP，要在backward call中设置参数retain_graph=True。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8da1af7c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-09T07:45:23.652641Z",
          "start_time": "2023-10-09T07:45:23.634992Z"
        },
        "id": "8da1af7c",
        "outputId": "c3c5c844-0ffa-4042-c232-3ee1b02abd73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0012, 0.2839, 0.0744],\n",
            "        [0.0012, 0.2839, 0.0744],\n",
            "        [0.0012, 0.2839, 0.0744],\n",
            "        [0.0012, 0.2839, 0.0744],\n",
            "        [0.0012, 0.2839, 0.0744]])\n",
            "tensor([0.0012, 0.2839, 0.0744])\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### backward所需的数据如何传递\n",
        "1. 自定义nn.autograd.Function时，在forward method中用save_for_backward()来存储backward中会用到的tensor data。这些数据在backward method中可以用saved_tensors() method提取。pytorch中已经定义好的函数也是通过这种方式向backward method传递data的。\n",
        "\n",
        "2. 在debug需要的时候，可以查看grad_fn对应的saved tensors。这些tensor以grad_fn的attribute形式存在，attribute name以'_saved'开头。\n",
        "3. pytorch在saving时，会将function中的saved tensor打包，在读取他们的时候再。unpack。但在读取他们的时候可能会因为要防止reference cycles，将tensor unpack成另一个tensor。此时读取出来的tensor和原来的tensor很可能不是同一个tensor。但是他们有共同的data storage。2.1版本中，只有tensor是其对应的grad_fn的output，才会这样。但这个implementation dettails在未来可能会变化。\n",
        "4. 可以通过hooks for saved tensors来控制pytorch如何packing/unpacking saved tensors."
      ],
      "metadata": {
        "id": "CBA_umhpLV-F"
      },
      "id": "CBA_umhpLV-F"
    },
    {
      "cell_type": "code",
      "source": [
        "# save input of function\n",
        "x = torch.randn(5, requires_grad=True)\n",
        "y = x.pow(2)\n",
        "print(x.equal(y.grad_fn._saved_self)) #\n",
        "print(x is y.grad_fn._saved_self)     #"
      ],
      "metadata": {
        "id": "huD3gpn7MT_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa4d7be-4456-4878-e6cf-0c05696be441"
      },
      "id": "huD3gpn7MT_h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save output of function\n",
        "x = torch.randn(5, requires_grad=True)\n",
        "y = x.exp()\n",
        "print(x.equal(y.grad_fn._saved_result)) #\n",
        "print(x is y.grad_fn._saved_result)     #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWgJkEvdNRKt",
        "outputId": "84949383-2a09-4cd3-f668-ccc7dd121c23"
      },
      "id": "oWgJkEvdNRKt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7eba1be",
      "metadata": {
        "id": "d7eba1be"
      },
      "source": [
        "## locally disabling gradient tracking/computation\n",
        "默认所有requires_grad=True的tensor都会被track computational history。但有的时候，不需要计算梯度，为了提高运算效率，可以disable gradient computation。\n",
        "\n",
        "· **不需要做gradient computation的典型场景：** \\\n",
        "  (1) model已经train好了，只做inference，不需要好用资源track history\\\n",
        "  (2) frozen参数来做finetune\n",
        "\n",
        "· **diable gradient tracking的两类方法：** \\\n",
        "  (1) disable entire block of code用context manager,比如：no-grad mode和inference mode\\\n",
        "  (2) 在更精细的范围内exclusion of sub-gragphs from gradient computation可以通过设置tensor的requires_grad值。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方法1：setting requires_grad\n",
        "1. tensor的requires_grad可以用来控制forward和backward中的gradient computation是否发生。requires_grad的默认是false，但wrapped in nn.Parameter的tensor的默认值是True。\n",
        "(1)Forward pass中，只有当operation中有tensor的requires_grad=True的时候才会被record到backward graph上。\n",
        "(2)Backward pass: 只有requires_grad=True的leaf tensor才会计算gradients并累积到tensor.grad上。所以，尽管每个tensor都设定了requires_grad的值，实际上只有leaf tensor的这个设定是有意义的。\n",
        "2. leaf tensor和non-leaf tensor的区别：\n",
        "(1)leaf: 没有grad_fn; 没有backward graph与之对应；requires_grad的值要设定，要么手动设定，要么nn.Parameter中自动设定。\n",
        "(2)non-leaf:有grad_fn; 有backward graph与之对应，因此他们的gradient只是作为intermediate result来计算requires_grad=True的leaf tensor的gradient；也因此，所有non-leaf tensor都自动设定了requires_grad=True，不需要手动设定。"
      ],
      "metadata": {
        "id": "3Ltz7uu7SqoP"
      },
      "id": "3Ltz7uu7SqoP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方法2：setting grad mode\n",
        "· pytorch中有3种grad model，他们决定了autograd处理gradient的方式：默认的是grad mode，此外还有no-grad mode和inference mode。他们都可以通过context manager和decorator来设置。\n",
        "1. **<font color=red>grad mode</font>:** 只有在该mode下，requires_grad的配置才能起作用，autograd才会record operations on the tensors。在另外两种mode下，requires_grad都会被overriden to be False。\n",
        "\n",
        "2. **<font color=red>no-grad mode</font>:** 让autograd暂时不记录operations（不将operations记录到backward graph上），退出该mode回到grad mode后再根据requires_grad的值是True还是False决定是否记录。\\\n",
        "  **· <font color=orange>适用场景：</font>** 只是暂时中断tensor operation的autograd，但tensor在当前operation中的运算结果还会被用于grad mode下的operation。\\\n",
        "  **· <font color=lightgreen>例子1, optimizer: </font>** 此时，weights' update不需要被autograd记录下来，但更新后的weights的值要用到下一次迭代中。而参与下一次迭代的forward运算时，这些weights是在grad mode下作计算的。\\\n",
        "  **· <font color=lightgreen>例子2, torch.nn.init</font>** 初始化operation是不需要计算梯度的。但是初始化完成后的training过程中，这些weights执行forward运算都是在grad mode中。\\\n",
        "\n",
        "3. **<font color=red>inference mode</font>:** 不是暂停让autograd记录tensor参与的operation，而是完全关闭autograd对operation的tracking。优点是tensor的运算速度会比no-grad mode更快。代价是inference mode下创建的tensor即使退出inference mode后，autograd也无法tracking它们参与的operations。\\\n",
        "  **· <font color=orange>适用场景：</font>** tensor参与的运算不需要记录到backward graph，并且这些运算中创建的tensor后续也不会用于其他需要被autograd记录的运算中去。\\\n",
        "  **· <font color=lightgreen>例子1, data processing: </font>** 不是模型的一部分，不做backward。\\\n",
        "  **· <font color=lightgreen>例子2, model evaluation：</font>** inference不做backward\\\n",
        "\n",
        "4. **<font color=red>evaluation mode</font>:** 这个不是disable autograd的机制，但容易和上述机制混淆。\\\n",
        "从功能上看，module.eval()和前面的no-grad和inference mode完全是不同的东西。理论上，model.eval()的作用取决于model中使用的modules，以及他们是否定义了trainning specific behavior。\\\n",
        "比如：model中用了torch.nn.Dropout或者torch.nn.Batchnorm，他们的特点是在trainning和validation/test时执行的operation不一样，这种情况下，就要手动在trainning和val/test阶段分别调用model.eval(), model.train()。\n",
        "tensor的detach method。\\\n",
        "· <font color=red>建议：不论有没有定义trainning specific behavior，都在trainning和val/test阶段分别调用model.eval(), model.train()。因为，model中使用的module即使在现在没有trainning specific behavior，未来可能会发生改变。</font>"
      ],
      "metadata": {
        "id": "HGsWLS0Z0tXu"
      },
      "id": "HGsWLS0Z0tXu"
    },
    {
      "cell_type": "code",
      "source": [
        "## 不区分trainning和val/test (伪码)\n",
        "\n",
        "# for t in range(1000):\n",
        "#   y_pred_train = model(x_train)\n",
        "#   loss = loss(y_pred_train, y_train)\n",
        "#   loss.backward()\n",
        "#   opt.step()\n",
        "#   opt.zero_grad()\n",
        "\n",
        "#   with torch.no_grad():\n",
        "#     y_pred_val = model(x_val)\n",
        "#     loss = loss(y_pred_val, y_val)\n",
        "#     val_loss = sum(loss) / len(y_val)"
      ],
      "metadata": {
        "id": "hoYnsrGSMvUA"
      },
      "id": "hoYnsrGSMvUA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 区分trainning和val/test (伪码)\n",
        "\n",
        "# for t in range(1000):\n",
        "#   model.train()\n",
        "#   y_pred_train = model(x_train)\n",
        "#   loss = loss(y_pred_train, y_train)\n",
        "#   loss.backward()\n",
        "#   opt.step()\n",
        "#   opt.zero_grad()\n",
        "\n",
        "#   model.eval()\n",
        "#   with torch.no_grad():\n",
        "#     y_pred_val = model(x_val)\n",
        "#     loss = loss(y_pred_val, y_val)\n",
        "#     val_loss = sum(loss) / len(y_val)"
      ],
      "metadata": {
        "id": "e2jS8tw2M32F"
      },
      "id": "e2jS8tw2M32F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c94307f3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-09T07:45:23.658986Z",
          "start_time": "2023-10-09T07:45:23.654967Z"
        },
        "id": "c94307f3",
        "outputId": "7b2f40a0-fe88-4c46-c4c6-0e7e22343046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a56a07ad",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-09T07:45:23.664770Z",
          "start_time": "2023-10-09T07:45:23.660748Z"
        },
        "id": "a56a07ad",
        "outputId": "14c2b6e5-0ad9-4778-fdab-2c63af7a43e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u2cXs8_kYLrI"
      },
      "id": "u2cXs8_kYLrI"
    },
    {
      "cell_type": "markdown",
      "id": "a683a70c",
      "metadata": {
        "id": "a683a70c"
      },
      "source": [
        "## directed acyclic graph(DAG)\n",
        "1. DAG由Function对象组成\n",
        "2. 叶节点是input tensor，根节点是output tensor，通过从root到leaves遍历graph，可以用chainrule自动计算gradients\n",
        "3. forward pass中，autograd会同时做两件事：\\\n",
        "(1)执行forward operation，计算tensor\\\n",
        "(2)将operation的gradient function存到DAG中\n",
        "4. 当.backward()被调用的时候，就会做backward pass。此时autograd会做3件事：\n",
        "(1)用每个.grad_fn计算梯度\\\n",
        "(2)将梯度累积到各自对应tensor的.grad属性中\\\n",
        "(3)使用chain rule，向leaf tensor传梯度"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7339eea6",
      "metadata": {
        "id": "7339eea6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自定义torch.autograd\n",
        "\n",
        "步骤：\\\n",
        "· step1: subclass torch.autograd.Function, implement forward(), setup_context(), and backward() methods. \\\n",
        "· step2: 在以ctx（context object）作为argument的method中设置好其内容。\\\n",
        "· step3: declare是否支持double backwards\\\n",
        "· step4: 做gradient check"
      ],
      "metadata": {
        "id": "5NE26ne7bcvO"
      },
      "id": "5NE26ne7bcvO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step1：subclass Function\n",
        "1. forward\n",
        "2. ctx可以在forward中通过将ctx作为argument来对其设置。如果forward中没有设置ctx，就要用setup_context()。setup_context()只负责设置ctx，不做其他运算。\n",
        "3. backward定义了gradient formula。\\\n",
        "· backward() method的tensor arguments是upstream gradients，function的每个output对应1个upstream gradient所以backward函数的tensor arguments数量与本函数的output数量相同。\\\n",
        "· backward() method的输出是function inputs的gradient。所以，backward的output tensor数量就是function inputs的数量。如果有的input tensor不用求gradient，或者有的input不是tensor，也不用求gradient，那么它们对应的输出用python：None。如果forward()中有optional arguments，那么输出的数量可能超过function input的数量，此时，backward()输出的数量也会超过function inputs，但这些forward arguments对应在backward中的output都要取None。"
      ],
      "metadata": {
        "id": "aPKDMrtZdyto"
      },
      "id": "aPKDMrtZdyto"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step2：use the function in ctx properly,确保Funtction在autograd engine中正确工作\n",
        "1. ctx.save_for_backward():存储backward pass中要用的tensor。\\\n",
        "· 如果要存储的data不是tensor，只能直接用ctx\\\n",
        "· 如果要存储的tensor既不是function的input也不是output，那么function就可能不支持double backward\n",
        "2. ctx.mark_dirty()：用来mark在backward pass中会发生in-place modify的function input。\n",
        "3. ctx.mark_none_differentiable()：用来标记function outputs是否需要求梯度。默认所有differentiable output tensors都设置为requires gradient。而不可导的那些则设置为不会被设置为requiures gradient。\n",
        "4. ctx.set_materialize_grads()：当output tensor的计算不依赖于input值时，可以通过not materialize grad tensors given to backward function来优化梯度的计算方式。默认设置是Ture。如果设置为False，python中的None不会在call backward之前被转化为全0的tensor，此时，要手动处理这些None objects。"
      ],
      "metadata": {
        "id": "OXh1vHUnjbTe"
      },
      "id": "OXh1vHUnjbTe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step3：如果自定义的Function不支持double backward，要用decorating with once_differentiable()来声明\n",
        "### step4：用torch.autograd.gradcheck()验证梯度计算的正确性。方法是用backward function计算Jacobian matrix，将结果与使用数值计算的结果做比较"
      ],
      "metadata": {
        "id": "ruX3slVypfI7"
      },
      "id": "ruX3slVypfI7"
    },
    {
      "cell_type": "code",
      "source": [
        "## 自定义LinearFunction\n",
        "import torch\n",
        "import torch.autograd.Function as Function\n",
        "class LinearFunction(Function):\n",
        "  @staticmethod\n",
        "  def forward(input, weight, bias):\n",
        "    output = input @ weight\n",
        "    if bias is not None:\n",
        "      output += bias.unsqueeze(0).expand_as(output)\n",
        "\n",
        "  @staticmethod\n",
        "  # inputs是所有传给forward()的inputs构成的tuple\n",
        "  # output是forward()的output\n",
        "  def setup_context(ctx, inputs, output):\n",
        "    input, weight, bias = inputs\n",
        "    ctx.save_for_backward(input, weight, bias)\n",
        "\n",
        "  @staticmethod\n",
        "  # grad_output是backward()唯一的输出\n",
        "  def backward(ctx, grad_output):\n",
        "    # unpack所有的saved_tensors\n",
        "    input, weight, bias = ctx.saved_tensors\n",
        "    # 初始化所有inputs的gradient为None，因为additional trailing Nones are ignored\n",
        "    # 因此，即使在Function有additional input的时候，return statement都可以很简单\n",
        "    grad_input = grad_weight = grad_bias = None\n",
        "\n",
        "    # 增加的条件验证只是为了避免不必要的运算，提高计算效率\n",
        "    if ctx.needs_input_grad[0]:\n",
        "      grad_input = grad_output.mm(weight)\n",
        "    if ctx.needs_input_grad[1]:\n",
        "      grad_weight = grad_output.t().mm(input)\n",
        "    if bias is not None and ctx.needs_input_grad[2]:\n",
        "      grad_bias = grad_output.sum(0)\n",
        "\n",
        "    return grad_input, grad_weight, grad_bias\n"
      ],
      "metadata": {
        "id": "gto7CDDRqE11"
      },
      "id": "gto7CDDRqE11",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:231n] *",
      "language": "python",
      "name": "conda-env-231n-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}