{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c18bb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.258757Z",
     "start_time": "2024-08-04T15:39:00.918467Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1151dcc0",
   "metadata": {},
   "source": [
    "# Storage and View\n",
    "1. 每个tensor都有对应的底层data放在memory上\n",
    "   - 用tensor.storage()查看底层data的信息\n",
    "   - tensor.data_ptr()返回tensor第一个元素的地址.可以用a.storage().data_ptr() == b.storage().data_ptr()来判断两个tensor底层数据是不是同一个。<font color=red>注意，这里只能用'=='，不能用'is'，因为这两个pointer本身是两个不同的pointer，只是他们的值相同</font>\n",
    "2. tensor有两种layout attribute，对应两种不同类型的tensor：dense和sparse tensor。\n",
    "   - dense tensor的底层data占用连续的存储空间，对应layout attribute是torch.stride。它用stride对象提供信息，决定如何从连续的data sequence上读取tensor element。\n",
    "   - sparse tensor则占用非连续空间，对应的layout attribute是torch.sparse。略<font color=red>[这里只关注dense tensor]</font>\n",
    "3. 每个dense tensor基于它的底层data有自己呈现data的方式，也就是view。不同的tensor可以共享相同的data memory，但是view不同。\n",
    "4. **常见的只改变view，不copy data的reshape method**：这些是可以通过改变stride信息来得到新view的method，不需要copy data。\n",
    "   1. <font color=blue>**tensor.view()**</font>：dense tensor如果是continuous tensor，可以用tensor.view() method改变tensor的view形态，得到的新tensor的data sequence的排序和原数据在memory中的排序一样，即不改变原tensor的continuity。<font color=green>如果要得到一个新shape的tensor，但是不知道是否能保证keep continuity，最好改用tensor.reshape()</font>\n",
    "   2. <font color=blue>**tensor.transpose()和tensor.movedim(source, destination)**</font>：\n",
    "      - 他们都是两种dense tensor(continuous和non-continuous)都可以用，得到的tensor的data sequence的排序和原数据在memory中的排序不同，因此此时continuity会被改变。\n",
    "      - transpose只能两个维度对调位置\n",
    "      - movedim更灵活，可以同时调整多个维度\n",
    "   3. <font color=blue>**tensor.squeeze(), tensor.unsqueeze()和tensor.expand()**</font>: 两种dense tensor(continuous和non-continuous)都可以用。\n",
    "      - <font color=blue>**tensor.expand()**</font>是扩展dim length=1的维度，扩展是指给重复的多个view。可以增加一个维度，相当于先做unsqueeze在dim0加一个dim length=1的维度，再扩展。\n",
    "   4. <font color=blue>**tensor.vsplit(), tensor.hsplit(), tensor.split(), tensor.chunk() and tensor.unflatten()**</font>\n",
    "      - hsplit和vsplit中，如果参数是int，要求对应dim的length要整除该int\n",
    "      - split最灵活，如果给的参数是int，对应dim的length如果不能整除，那么最后一个分出来的section的数量最少\n",
    "      - 三种\\*split method都可以通过设置参数为list，把原tensor切成大小不同的chunk，但是chunk method只能做平分，只是切出来最后一个chunk大小可能小一点。\n",
    "      - tensor.unflatten()不是切割，而是把指定维度的数据转成指定的shape。<font color=red>注，tensor.flatten()可能copy data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d86957",
   "metadata": {},
   "source": [
    "## 1. tensor的存储方式和特点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6945895",
   "metadata": {},
   "source": [
    "### 1.1 tensor.storage()\n",
    "输出tensor存在memory中的1D data sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fca0855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.267457Z",
     "start_time": "2024-08-04T15:39:02.260667Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26087/3405978919.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage(), x.is_contiguous()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "( 1\n",
       "  2\n",
       "  3\n",
       "  6\n",
       "  7\n",
       "  8\n",
       " [torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6],\n",
       " True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [6, 7, 8]])\n",
    "x.storage(), x.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7cadaa",
   "metadata": {},
   "source": [
    "### 1.2 dense和sparse tensor有不同的layout attribute\n",
    "1. 根据存储方式的不同，tensor有两种类型，dense tensor和sparse tensor，他们可以通过tensor.layout attribute来区分\\\n",
    "(1)**dense tensor** is stored linearly in a contiguous block of memory，也就是其存储占用了连续的memory block。\\\n",
    "(2)**sparse tensor**中大部分是0，只有很少比例的值是非零值，为了提高存储效率，pytorch为这类数据提供了特殊的存储方式。\n",
    "2. torch.layout attribute是表达tensor的memory layout特征的object。这个attribute有两种类型的取值：torch.stride和torch.sparse_coo，分别对应上面的两种tensor类型\\\n",
    "(1)如果<font color=blue>**layout=torch.strided**</font>，那么该tensor是dense tensor\\\n",
    "(2)如果<font color=blue>**layout=torch.sparse**</font>，那么该tensor是sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8a8811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.281371Z",
     "start_time": "2024-08-04T15:39:02.270836Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, (3, 1), False, (1, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 新建dense tensor，得到连续的存储空间\n",
    "#  连续的存储空间是指底层的data，改变view后的新tensor的元素可能不连续\n",
    "x = torch.rand((2, 3), layout=torch.strided, requires_grad=False)\n",
    "\n",
    "# 这里y是x做transpose后的View，这个view的element相对实际存储的data而言不连续\n",
    "y = x.T\n",
    "x.is_contiguous(), x.stride(), y.is_contiguous(), y.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86bed454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.288056Z",
     "start_time": "2024-08-04T15:39:02.282975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(indices=tensor([], size=(2, 0)),\n",
       "        values=tensor([], size=(0,)),\n",
       "        size=(100, 200), nnz=0, layout=torch.sparse_coo),\n",
       " False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 新建sparse tensor，存储空间本身就不连续\n",
    "z = torch.zeros((100, 200), layout=torch.sparse_coo, requires_grad=False)\n",
    "z, z.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f53313",
   "metadata": {},
   "source": [
    "### 1.3 tensor.stride()可以查看dense tensor读取底层data的方式\n",
    "1. dense tensor才有**tensor.stride()** method\n",
    "   - dense tensor不论是contiguous还是non-contiguous tensor都有stride。\n",
    "2. strided tensor的stride是一个int list,其第k-th元素的值表示tensor的k-th dimension上，一个元素到下个元素之间，用units of elements衡量的长度。\n",
    "3. 每个strided tensor都有一个与之关联的torch.Storage存放data. 这些tensors 则提供strided view of a storage。一个torch.Storage可以对应不同的strided tensors。这些tensors有相同的data，不同的tensor.view()\n",
    "   - <font color=blue>**Numpy strides()** returns (N bytes to Next Row, M bytes to Next Column)\n",
    "   - **Pytorch stride()** returns (N elements to Next Row, M elements to Next Column).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef9bd37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.293977Z",
     "start_time": "2024-08-04T15:39:02.289526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "5\n",
      "1\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5],\n",
    "                  [6, 7, 8, 9, 10]])\n",
    "\n",
    "print(x.stride())  # 不指定dim，A tuple of all strides is returned\n",
    "                   # 在tuple(5, 1)中，5在第0维，1在第1维对应traverse的维度\n",
    "print(x.stride(0)) # 指定dim=0，返回该维度上的stride值5\n",
    "                   # traverse along the 0th dim, 比如从'1'跳到'6',距离是5\n",
    "print(x.stride(-1))# 指定dim=1，返回该维度上的stride值1\n",
    "                   # traverse along the 1st dim, 比如从'7'跳到'8',距离是1\n",
    "\n",
    "## 改变view，stride也改变\n",
    "y = x.view(5, 2)\n",
    "print(y.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20998a20",
   "metadata": {},
   "source": [
    "### 1.4 dense tensor的底层data存储在连续位置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495f825",
   "metadata": {},
   "source": [
    "1. dense tensor可能是contiguous tensor或者non-contiguous tensor。虽然他们在存储空间中都占用了连续的存储单元，但non-contiguous tensor的元素排列顺序与其元素在存储空间的排列顺序不一致。\\\n",
    "**· <font color=blue>tensor.view()</font>** 会保持contiguous tensor的contiguity。也只有contiguous tensor有view() method\\\n",
    "**· <font color=blue>tensor.transpose()</font>** 会改变tensor的contiguity。contiguous和non-contiguous tensor都可以transpose。当base tensor是dense tensor时，transpose不copy data，所以output tensor也只是原underlying data的一个new view。\n",
    "2. 用is_contiguous()可以查看是否contiguous\n",
    "3. 用tensor.contiguous()可以把non-contiguous tensor转变成contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403313af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.301173Z",
     "start_time": "2024-08-04T15:39:02.295815Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2],\n",
      "        [1, 3]]) True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([[0, 1],[2, 3]])\n",
    "t = b.transpose(0, 1) \n",
    "print(t, b.data_ptr() == t.data_ptr()) # 同一data\n",
    "b.is_contiguous(), t.is_contiguous() # continuity变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fee84a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.306745Z",
     "start_time": "2024-08-04T15:39:02.302539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 用tensor.storage()可以获得该tensor在memory上的1D sequence\n",
    "x = torch.arange(0,6)\n",
    "\n",
    "## view返回的tensor中，data sequence的排序和原数据相同，都是0, 1, 2, 3, 4...\n",
    "y = x.view(2,3)\n",
    "y.stride(), y.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "580611c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.311821Z",
     "start_time": "2024-08-04T15:39:02.308189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.storage().data_ptr() == x.storage().data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa46269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.318627Z",
     "start_time": "2024-08-04T15:39:02.314713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: \n",
      " tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "(1, 3) False True\n"
     ]
    }
   ],
   "source": [
    "## transpose返回的tensor中，data sequence的排序和原数据在memory中的排序不同\n",
    "#  z中元素的排序是：0, 3, 1, 4...\n",
    "#  transpose不copy data，它仍然是原data的view，但是是non-contiguous ‘View’\n",
    "#  按新tensor的element顺序从memory中取值的时候不是连续取值\n",
    "z = y.T\n",
    "print('z: \\n', z)\n",
    "print(z.stride(), z.is_contiguous(), y.storage().data_ptr() == z.storage().data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "592de342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.323676Z",
     "start_time": "2024-08-04T15:39:02.320058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## transpose将non-contiguous转变成contiguous\n",
    "w = z.T\n",
    "w.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08c6c55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.329098Z",
     "start_time": "2024-08-04T15:39:02.324919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: \n",
      " tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]]) True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## 用tensor.contiguous()转变成contiguous tensor\n",
    "#  此时发生了copy，按照tensor view的排序方式复制了一个新的data\n",
    "v = z.contiguous()\n",
    "print('v: \\n', v, v.is_contiguous())\n",
    "print(v.storage().data_ptr() == z.storage().data_ptr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e777cf",
   "metadata": {},
   "source": [
    "## 2. 通过改变stride改变view的method\n",
    "### 2.1 用tensor.view()不改变torch.layout属性\n",
    "<font color=red>相当于numpy里面的reshape。</font>\n",
    "1. 只有contiguous dense tensor才有view method。\n",
    "2. dense tensor在计算机中以1D data sequence的形式存在congtiguous block of memory上。tensor.view()本质上是告诉计算机如何(根据其参数设置)stride over the 1D data sequence。所以，只提供了new view of the base tensor，不改变底层data。view的改变也就意味着tensor.stride()也会随之改变。\n",
    "3. tensor.view()返回的new tensor从memory上读取elements的顺序与其线性存储的顺序一致，<font color=red>只是改变了划分layer,column和row等unit单位的位置。</font>\n",
    "4. 因为View tensor与它的base tensor的underlying data是同一个data，即tensor.view()返回new tensor，但并不copy data，所以，其中一个tensor元素值改变，另一个也变。\n",
    "\n",
    "<font color=blue>**tensor.data_ptr()**</font>: 返回1st element的存储地址，即data pointer指向的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b9588b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.335491Z",
     "start_time": "2024-08-04T15:39:02.330494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原data pointer: 139062656\n",
      "原id: 140668228855552\n",
      "原stride和size: (1,) torch.Size([15]) \n",
      "\n",
      "新data pointer与原值相同: 139062656\n",
      "id变化，说明新建了一个tensor: 140668228856032\n",
      "stride和size变化: (5, 1) torch.Size([3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tensor.view()返回的新tensor不copy data，但是会改变stride和shape(size)\n",
    "a = torch.arange(15)\n",
    "print('原data pointer:', a.data_ptr()) # \n",
    "print('原id:', id(a))\n",
    "print('原stride和size:', a.stride(), a.size(), '\\n')\n",
    "\n",
    "# change view\n",
    "b = a.view(3, 5)\n",
    "print('新data pointer与原值相同:', b.data_ptr())\n",
    "print('id变化，说明新建了一个tensor:', id(b))                 # id变化\n",
    "print('stride和size变化:', b.stride(), b.size()) \n",
    "\n",
    "# data地址不变\n",
    "a.data_ptr() == b.data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be1765a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.340872Z",
     "start_time": "2024-08-04T15:39:02.336752Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      " tensor([0.5790, 0.9373, 0.9461, 0.0924, 0.1483, 0.3489, 0.3139, 0.5684, 0.3756,\n",
      "        0.8025, 0.0997, 0.8067])\n",
      "b: \n",
      " tensor([[[0.5790, 0.9373],\n",
      "         [0.9461, 0.0924]],\n",
      "\n",
      "        [[0.1483, 0.3489],\n",
      "         [0.3139, 0.5684]],\n",
      "\n",
      "        [[0.3756, 0.8025],\n",
      "         [0.0997, 0.8067]]])\n",
      "140668228850672 140668228855952 140668228856032\n"
     ]
    }
   ],
   "source": [
    "## tensor.view()不改变tensor layout in memory\n",
    "a = torch.rand(12)\n",
    "b = a.view(3, 2, 2)\n",
    "c = a.view(2, 3, 2)  # 不改变tensor layout in memory\n",
    "\n",
    "print('a: \\n', a)\n",
    "print('b: \\n', b) # b的元素排序和a完全相同，只是每个dim的截断位置改变\n",
    "\n",
    "print(id(a), id(b), id(c)) # id变化, 是不同的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926af611",
   "metadata": {},
   "source": [
    "### 2.2 用transpose和movedim会改变contiguity\n",
    "<font color=red>注：这里的view的layout是指view的shape，不是tensor的layout属性。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65335dd1",
   "metadata": {},
   "source": [
    "#### 2.2.1 tensor.transpose()\n",
    "1. 和view method不同的是，transpose改变了elements在view中的layout。含义是：如果把transpose之后的tensor排成一个vector，其顺序与input tensor不相同。\n",
    "2. 和view method相同的是，当tensor是dense tensor时，output只是input的一个view，不发生copy。改变其中一个的元素值，另一个也变\n",
    "3. <font color=blue>**tensor.adjoint(),tensor.mT和tensor.mH**</font>在tensor元素为实数的时候相当于tensor.transpose(-2,-1)。返回原tensor交换最后两个维度后的view。mT和mH的区别在于处理复数的时候一个可以实现和adjoin一样的功能，一个不能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f197ecf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.345137Z",
     "start_time": "2024-08-04T15:39:02.342103Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原shape: \n",
      " tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11]],\n",
      "\n",
      "         [[12, 13, 14, 15],\n",
      "          [16, 17, 18, 19],\n",
      "          [20, 21, 22, 23]]]])\n",
      "torch.Size([1, 2, 3, 4]) 140668228856832 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(1, 2, 3, 4)\n",
    "print(\"原shape:\", '\\n', a)\n",
    "print(a.size(), id(a), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d741c414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.350743Z",
     "start_time": "2024-08-04T15:39:02.346655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用transpose交换了2nd和3rd dim: \n",
      " tensor([[[[ 0,  1,  2,  3],\n",
      "          [12, 13, 14, 15]],\n",
      "\n",
      "         [[ 4,  5,  6,  7],\n",
      "          [16, 17, 18, 19]],\n",
      "\n",
      "         [[ 8,  9, 10, 11],\n",
      "          [20, 21, 22, 23]]]])\n",
      "torch.Size([1, 3, 2, 4]) 140668228858672 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
       "        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.transpose(1, 2)\n",
    "print(\"用transpose交换了2nd和3rd dim:\", '\\n', b)\n",
    "print(b.size(), id(b), '\\n')\n",
    "\n",
    "a.view(2, 12)\n",
    "# b.view(24) # 错，b这时不是contiguous，不能用view method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0de76531",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.355841Z",
     "start_time": "2024-08-04T15:39:02.352269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0,  4,  8],\n",
       "           [ 1,  5,  9],\n",
       "           [ 2,  6, 10],\n",
       "           [ 3,  7, 11]],\n",
       " \n",
       "          [[12, 16, 20],\n",
       "           [13, 17, 21],\n",
       "           [14, 18, 22],\n",
       "           [15, 19, 23]]]]),\n",
       " torch.Size([1, 2, 4, 3]),\n",
       " tensor(True))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.mT # 注意mT不是函数，mutate tail\n",
    "d = a.mH # 注意mH不是函数，mutate Head\n",
    "\n",
    "c, c.shape, torch.all(c == d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a9f35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T15:22:05.874792Z",
     "start_time": "2024-08-03T15:22:05.870360Z"
    }
   },
   "source": [
    "#### 2.2.2 movedim(source, destination)\n",
    "和transpose一样，它会改变tensor的view的layout，得到的是non-contiguous tensor，也就是不能按照原data sequence来从底层data读取element。但是不会copy data。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d7f0bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.360435Z",
     "start_time": "2024-08-04T15:39:02.357456Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(24).view(2, 3, 4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21299ba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.365000Z",
     "start_time": "2024-08-04T15:39:02.361716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  4,  8],\n",
       "         [12, 16, 20]],\n",
       "\n",
       "        [[ 1,  5,  9],\n",
       "         [13, 17, 21]],\n",
       "\n",
       "        [[ 2,  6, 10],\n",
       "         [14, 18, 22]],\n",
       "\n",
       "        [[ 3,  7, 11],\n",
       "         [15, 19, 23]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 当参数是int的时候，int代表dim，\n",
    "t.movedim(2, 0) # 把dim2移动到dim0的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdfb0aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.369755Z",
     "start_time": "2024-08-04T15:39:02.366236Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 2]),\n",
       " tensor([[[ 0, 12],\n",
       "          [ 1, 13],\n",
       "          [ 2, 14],\n",
       "          [ 3, 15]],\n",
       " \n",
       "         [[ 4, 16],\n",
       "          [ 5, 17],\n",
       "          [ 6, 18],\n",
       "          [ 7, 19]],\n",
       " \n",
       "         [[ 8, 20],\n",
       "          [ 9, 21],\n",
       "          [10, 22],\n",
       "          [11, 23]]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 当参数是tuple的时候，分别代表source和destination dims\n",
    "s = t.movedim((1, 2), (0, 1)) # 把dim1移动到dim0，再把dim2移动到dim1\n",
    "s.shape, s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c65ce",
   "metadata": {},
   "source": [
    "### 2.3 squeeze and unsqueeze\n",
    " - squeeze和unsqueeze只改变原数据的view，新生成的tensor与input tensor指向相同的memory\n",
    "#### 2.3.1 tensor.unsqueeze()\n",
    "1. pytorch默认是按照batch处理数据，以image为例，默认的input大小是(N, C, H, W)。如果想让model处理单个数据，就要把当个样本的大小从(C, H, W)改成(1, C, H, W)\n",
    "2. 常用于ease broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ff4f526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.374318Z",
     "start_time": "2024-08-04T15:39:02.370903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "增加一个长度为1的维度到新的第0维： torch.Size([1, 3, 226, 226])\n",
      "增加一个长度为1的维度到新的第1维： torch.Size([3, 1, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "# 用tensor.unsqueeze()来增加长度为1的新维度\n",
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0) # 在第0维增加一个维度\n",
    "print('增加一个长度为1的维度到新的第0维：',b.shape)\n",
    "\n",
    "c = a.unsqueeze(1)\n",
    "print('增加一个长度为1的维度到新的第1维：',c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80164246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.379248Z",
     "start_time": "2024-08-04T15:39:02.375540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before: torch.Size([3])\n",
      "shape after: torch.Size([3, 1])\n",
      "tensor([[[0., 0.],\n",
      "         [1., 1.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [1., 1.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [1., 1.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [1., 1.],\n",
      "         [2., 2.]]])\n"
     ]
    }
   ],
   "source": [
    "# unsqueeze常用于方便broadcast\n",
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.arange(3)     # a * b不能直接运算\n",
    "print('shape before:', b.shape)\n",
    "c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\n",
    "print('shape after:', c.shape)\n",
    "print(a * c)             # broadcasting works again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c727082",
   "metadata": {},
   "source": [
    "#### 2.3.2 tensor.squeeze()\n",
    "1. 用tensor.squeeze()来压缩长度为1的维度(dimensions of extent 1)\n",
    "2. 最好指定dim number，因为pytorch按batch处理数据，squeeze()会把第1维N压掉，导致模型出错 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f9c3cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.385025Z",
     "start_time": "2024-08-04T15:39:02.380587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[2., 2., 2.]],\n",
       " \n",
       "          [[2., 2., 2.]]]]),\n",
       " (6, 3, 3, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(1, 2, 1, 3)\n",
    "b = a.squeeze()   # 压缩所有维度为1的dims\n",
    "c = a.squeeze(0)  # 压缩第0维\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "\n",
    "## 改变b之后，a的值也变\n",
    "b += 1\n",
    "a, a.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013818d4",
   "metadata": {},
   "source": [
    "#### 2.3.3 tensor.expand(newshape)\n",
    "- 以repeat的方式扩展dim length=1的维度\n",
    "- 不扩展的维度可以在参数中的对应dim上用-1\n",
    "- 可以增加一维到dim0，假如原来的shape是(a, b...)，这里参数设为(n, a, b...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24d0e553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.389336Z",
     "start_time": "2024-08-04T15:39:02.386290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[1,2], [3,4], [5,6]]])\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea02468a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.394156Z",
     "start_time": "2024-08-04T15:39:02.390606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 2]),\n",
       " tensor([[[1, 2],\n",
       "          [3, 4],\n",
       "          [5, 6]],\n",
       " \n",
       "         [[1, 2],\n",
       "          [3, 4],\n",
       "          [5, 6]]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以repeat的方式扩展dim length=1的维度\n",
    "x.expand(2, 3, 2).shape, x.expand(2, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d9c2820",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.398753Z",
     "start_time": "2024-08-04T15:39:02.395361Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## -1表示不改变对应维度\n",
    "torch.all(x.expand(2, 3, 2) == x.expand(2, -1, -1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0623aa4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.403552Z",
     "start_time": "2024-08-04T15:39:02.399898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 2],\n",
       "          [3, 4],\n",
       "          [5, 6]]],\n",
       "\n",
       "\n",
       "        [[[1, 2],\n",
       "          [3, 4],\n",
       "          [5, 6]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 可以增加一维到dim0\n",
    "x.expand(2, 1, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7e225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb97b0b4",
   "metadata": {},
   "source": [
    "### 2.4 tensor.split(), tensor.chunk() and tensor.unflatten()\n",
    "1. split不同于numpy中的array_split，主要是参数的含义不同\n",
    "   - int参数\n",
    "     - tensor.split中指定的是每份的size是多大\n",
    "     - numpy.array_split中指定要分多少份，和tensor.chunk一样\n",
    "   - list或者tuple参数\n",
    "     - tensor.split中指定的是每份的length数量\n",
    "     - numpy.array_split中是给的分割点\n",
    "2. chunk灵活性不如split，只能做\"平分\"，不能分成指定的数量不均匀的chunk。\n",
    "   - 分割得到n份，前l % n份的size是l // n + 1，后面的是l // n。<font color=red>注，这里分法有点奇怪，是先每份给l // n,再把余数l % n分给前l % n份。</font>\n",
    "3. unflatten把指定维度改成新的shape。在指定dim上重新改shape的时候不需要改变原tensor的element sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d27f440f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.410932Z",
     "start_time": "2024-08-04T15:39:02.406587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [2, 3],\n",
       "         [4, 5],\n",
       "         [6, 7],\n",
       "         [8, 9]]),\n",
       " (tensor([[0, 1]]),\n",
       "  tensor([[2, 3],\n",
       "          [4, 5],\n",
       "          [6, 7],\n",
       "          [8, 9]])))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tensor.split\n",
    "a = torch.arange(10).reshape(5, 2)\n",
    "a, torch.split(a, [1, 4]) # tuple中元素sum要和a中dim0的length一样，不然报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e20f9fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.416884Z",
     "start_time": "2024-08-04T15:39:02.412203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk\n",
      " case1:  (tensor([0, 1]), tensor([2, 3]), tensor([4, 5]), tensor([6]))\n",
      "case2:  (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]))\n",
      "split\n",
      " case1:  (tensor([0, 1, 2, 3]), tensor([4, 5, 6]))\n",
      "case2:  (tensor([0, 1, 2, 3]), tensor([4, 5, 6, 7]), tensor([8]))\n"
     ]
    }
   ],
   "source": [
    "## tensor.chunk(int, dim=0): int指的是dim要分成多少份\n",
    "print('chunk\\n case1: ', torch.arange(7).chunk(4))\n",
    "print('case2: ', torch.arange(9).chunk(4)) # 得到的chunk数量可能不等于参数\n",
    "\n",
    "#  对比split\n",
    "print('split\\n case1: ', torch.arange(7).split(4))\n",
    "print('case2: ', torch.arange(9).split(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "febfa0bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T15:39:02.422728Z",
     "start_time": "2024-08-04T15:39:02.418161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1],\n",
       "          [ 2,  3],\n",
       "          [ 4,  5],\n",
       "          [ 6,  7]],\n",
       " \n",
       "         [[ 8,  9],\n",
       "          [10, 11],\n",
       "          [12, 13],\n",
       "          [14, 15]]]),\n",
       " torch.Size([2, 2, 2, 1, 2]),\n",
       " tensor([[[[[ 0,  1]],\n",
       " \n",
       "           [[ 2,  3]]],\n",
       " \n",
       " \n",
       "          [[[ 4,  5]],\n",
       " \n",
       "           [[ 6,  7]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[ 8,  9]],\n",
       " \n",
       "           [[10, 11]]],\n",
       " \n",
       " \n",
       "          [[[12, 13]],\n",
       " \n",
       "           [[14, 15]]]]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## unflatten把指定维度改成新的shape\n",
    "c = torch.arange(16).view(2, 4, 2)\n",
    "new_c = torch.unflatten(c, 1, (2, 2, 1)) # 把c的dim1转变成shape:(2, 2)\n",
    "c, new_c.shape, new_c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
