{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a5963b",
   "metadata": {},
   "source": [
    "# Hooks\n",
    "**1. 什么是hook？**\n",
    "hook是一个特殊的<font color=blue>函数</font>，它在autograd.Function的forward或backward method调用的前后被执行。hooks可以用来在常规module的forward/backward pass中执行任意的code，或者用来调整inputs/outputs而不用改变module本身的forward() method.\\\n",
    "**2. hook的使用对象？**\n",
    "hook可以<font color=blue>用于**tensor或module**</font>，称为register on a tensor or nn.Module。典型使用场景：\\\n",
    "· <font color=green>用于tensor的时候主要用来控制从forward向backward传递信息过程中的pack/unpack information。</font>\\\n",
    "· <font color=green>用于module中的时候，可以用于模型可视化，debug，gradient check等。</font> \\\n",
    "**3. hooks的类型？**\n",
    "使用hook，要先register到使用hook的位置。按照使用位置，pytorch提供了两大类hooks：forward hook和backward hook。 \\\n",
    "**4. hooks的作用范围？**\n",
    "所有hooks都可以返回更新后的value，这些value也会被用到hooks所在pass的后续计算中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd58d23",
   "metadata": {},
   "source": [
    "## 1. Hooks for tensors\n",
    "用于tensor的hook只有backward hook。\\\n",
    "**signature of hook function：**<font color=red>hook_func_name(grad) -> Tensor or None </font>\\\n",
    "**method for register a backward hook**：<font color=red>tensor_name.register_hook(hook_func_name)</font> \\\n",
    "**典型用途：**\n",
    "1. <font color=green>改变tensor的梯度计算方式。</font>某个tensor x的x.grad用hook改变后，DAG上位于x前面，依赖于x的tensor的梯度也按chainrule改变。如果不用tensor hook，要等bp结束后手动改变x和每一个前序tensor的梯度。\n",
    "2. <font color=green>查看intermediate tensor的grad，此时不占额外内存。</font>不用tensor hook的话就要用intermediate_tensor.remain_grad()，会占用内存。\n",
    "3. <font color=green>自定义saved tensor的pack/unpack方式</font>\n",
    "\n",
    "**backward hooks execution：** [详见Pytorch Doc:autograd mechnism]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7029e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:43:14.460418Z",
     "start_time": "2024-08-11T00:43:14.456401Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchviz\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd175d",
   "metadata": {},
   "source": [
    "### 1.1 直接改变tensor/tensor.grad value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1957327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:43:16.145432Z",
     "start_time": "2024-08-11T00:43:16.141257Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例1：一个简单的例子\n",
    "#  定义hook function\n",
    "def func(grad):\n",
    "    return grad * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "291c2df1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:43:17.120910Z",
     "start_time": "2024-08-11T00:43:17.111551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9064, -0.9064, -0.9064, -0.9064, -0.9064])\n",
      "tensor([-1.8891,  0.5235,  0.4592])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 1.without hook\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eccddf",
   "metadata": {},
   "source": [
    "By default, PyTorch only populates (fills) the .grad attributes of leaf tensors during the backward pass to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f03f98c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:43:24.005501Z",
     "start_time": "2024-08-11T00:43:23.995266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8128, -1.8128, -1.8128, -1.8128, -1.8128])\n",
      "tensor([-3.7783,  1.0471,  0.9184])\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# 2.with hook for loss\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "# # y.retain_grad()        #  y不是leaf，\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()     #  loss不是leaf，\n",
    "\n",
    "#  register hook\n",
    "loss.register_hook(func)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# 因为loss的gradient翻倍，导致前向传递，w和b的梯度也翻倍\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "# print(y.grad)\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a733af9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.009877Z",
     "start_time": "2024-08-11T00:38:34.002855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8128, -1.8128, -1.8128, -1.8128, -1.8128])\n",
      "tensor([-1.8891,  0.5235,  0.4592])\n",
      "tensor([-1.8891,  0.5235,  0.4592])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 3.with hook for w\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "y.retain_grad()\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "#  register hook for w\n",
    "w.register_hook(func)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# 因为w的gradient翻倍，前向传递不影响loss,y和b\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "print(y.grad)\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d36f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.014569Z",
     "start_time": "2024-08-11T00:38:34.011653Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2：Gradient clipping\n",
    "def grad_clipper(model, val):\n",
    "    for parameter in model.parameters():\n",
    "        parameter.register_hook(lambda grad: grad.climp_(floor, cap))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0750a030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.031649Z",
     "start_time": "2024-08-11T00:38:34.016255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "bias_grad in linear layer: tensor([-0.2000, -0.2000, -0.2000, -0.2000, -0.2000])\n"
     ]
    }
   ],
   "source": [
    "## 例3：用于module中的tensor objects\n",
    "#  要求：\n",
    "#    1. 将linear层中bias的梯度改为0\n",
    "#    2. conv layer从downstream拿到的gradient大小都不小于0\n",
    "\n",
    "# ----------------  原模型  ----------------\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = lambda x: x.view(-1)\n",
    "        self.fc = nn.Linear(160, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv(x))         \n",
    "        return self.fc(self.flatten(x))\n",
    "\n",
    "torch.manual_seed(2)\n",
    "x = torch.randn(1, 3, 8, 8)\n",
    "net = TestNet()\n",
    "out = net(x)\n",
    "loss = (1 - out).mean()\n",
    "loss.backward()\n",
    "\n",
    "print(out.shape)\n",
    "# bias_grad in linear layer should be -1/out.shape\n",
    "print('bias_grad in linear layer:', net.fc.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed48a508",
   "metadata": {},
   "source": [
    "### 1.2 查看intermediate tensor或其grad信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "357834bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.041122Z",
     "start_time": "2024-08-11T00:38:34.034260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8891,  0.5235,  0.4592])\n"
     ]
    }
   ],
   "source": [
    "## 例2：打印intermediate output，也就是Feature value\n",
    "#  定义hook\n",
    "def func(grad):\n",
    "    print(grad)\n",
    "    return grad\n",
    "\n",
    "torch.manual_seed(2)\n",
    "w = torch.randn(5, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "x = torch.ones((3, 5))\n",
    "y = x @ w + b\n",
    "loss = (y ** 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "#  register hook for y，y在这里是intermediate tensor\n",
    "#  如果不用hook，就要设置y.retain_grad()才能在bp结束后查看，会占用内存\n",
    "y.register_hook(func)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd1605",
   "metadata": {},
   "source": [
    "#### module中给tensor加入backward hook后，Backward pass的执行顺序\n",
    "1. 从root开始按照chainrule执行backward pass\n",
    "2. 遇到hook后，对制定tensor的grad执行相应的ops，如果同一位置有多个hook，按他们在module中出现的顺序执行操作，而不是想chainrule那样反向操作。如下例：\\\n",
    "fc layer backward method     -> \\\n",
    "flatten layer                -> \\\n",
    "register for clamp           -> \\\n",
    "register for print shape     -> \\\n",
    "register for check gradient  -> \\\n",
    "relu layer backward method   -> \\\n",
    "conv layer backward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e03686",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.063804Z",
     "start_time": "2024-08-11T00:38:34.049401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of output of flatten layer: torch.Size([160])\n",
      "size of output of relu layer: torch.Size([1, 10, 4, 4])\n",
      "any grad < 0? False\n",
      "bias_grad in linear layer: tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# ----------------  加入hook  ----------------\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = lambda x: x.view(-1)\n",
    "        self.fc = nn.Linear(160, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv(x))\n",
    "        \n",
    "        # 设置hook：让x.grad >= 0\n",
    "        # 这里处理的 x 是relu layer的output\n",
    "        # relu的梯度只有0和1，所以处理relu之后的x即可\n",
    "        x.register_hook(lambda grad: torch.clamp(grad, min=0))\n",
    "        \n",
    "        x.register_hook(lambda grad: print('size of output of relu layer:', grad.shape))\n",
    "        # 再加一个hook确认有没有x.grad是负值\n",
    "        # 放在这里改变的是relu运算后的x.grad\n",
    "        x.register_hook(lambda grad: print('any grad < 0?', \\\n",
    "                                           bool((grad < 0).any())))\n",
    "        x = self.flatten(x)\n",
    "        # 这里处理的 x 是flatten layer的output\n",
    "        x.register_hook(lambda grad: print('size of output of flatten layer:', grad.shape))\n",
    "\n",
    "        y = self.fc(x)\n",
    "        return y    \n",
    "    \n",
    "torch.manual_seed(2)\n",
    "x = torch.randn(1, 3, 8, 8)\n",
    "net = TestNet()\n",
    "\n",
    "# 在model外部，给参数tenosr设置hook: 这里将linear层中bias的梯度改为0\n",
    "# 这种方式可以在不改变module定义的条件下设置hook\n",
    "for name, param in net.named_parameters():\n",
    "    if 'fc' in name and 'bias' in name:\n",
    "        param.register_hook(lambda grad: torch.zeros(grad.shape))\n",
    "\n",
    "out = net(x)\n",
    "loss = (1 - out).mean()\n",
    "loss.backward()\n",
    "\n",
    "# 确认bias的grad成功改成了全0\n",
    "print('bias_grad in linear layer:', net.fc.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aaff52",
   "metadata": {},
   "source": [
    "### 1.3 hooks for saved tensors：\n",
    "**功能：** 改变saved tensor pack/unpack方式 \\\n",
    "**方法：** \n",
    "1. 定义两个hook function，一个实现pack，另一个unpack \\\n",
    "（1）定义pack_func(tensor): 只接受1个tensor作为argument，返回任意python类型。 \\\n",
    "（2）定义unpack_hook_func(output_of_pack_func)：只接受pack_func的返回值作为input argument，返回一个tensor用于后续的backward pass。该返回值的value要跟pack_func中的input的value相同，以达到原本想要从forward向backward传递信息的目的。\n",
    "（3）即使x = unpack(pack(x)),但pack会将x打包成另一个object，unpack输出的x同样也是新的tensor，如果pack(x)=x，那么他们虽然是三个不同的tensor，但share memory；如果pack(x)不等于x，则x和unpack输出的xshare memory。\n",
    "2. register 上述pack/unpack hooks \n",
    "\n",
    "**基本原则：** \n",
    "1. uppack_func(pack_func(tensor)) = tensor\n",
    "2. pack_func的input不能做in-place modify\n",
    "3. pack_func(tensor)的output可以是tenosr或者任意python type object \n",
    "4. pack_func和unpack_func单独作用于每个saved tensor\n",
    "\n",
    "**执行过程：** \n",
    "1. 每次forward pass执行过程中，pack func在对应operation存储信息的时候被调用，其output会代替原本module中定义的pack func输出的output tensor而被存储。\n",
    "2. 在backward pass中按照chainrule执行到对应operation的backward method的之前，unpack func会被调用，它用pack func的output作为唯一的input来计算一个new tensor。这个new tensor会作为backward method的input之一而被使用。\n",
    "\n",
    "**典型应用场景：** <font color=blue>将forward pass中要保存的tensor存到cpu或者disk上，节省GPU memory</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51d40a61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.107131Z",
     "start_time": "2024-08-11T00:38:34.065606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packing: tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n",
      "packing: tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"222pt\" height=\"282pt\"\n",
       " viewBox=\"0.00 0.00 222.00 282.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 278)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-278 218,-278 218,4 -4,4\"/>\n",
       "<!-- 138869189376688 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>138869189376688</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"133.5,-31 79.5,-31 79.5,0 133.5,0 133.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 138869186370704 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>138869186370704</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 62,-86 62,-67 151,-67 151,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 138869186370704&#45;&gt;138869189376688 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>138869186370704&#45;&gt;138869189376688</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-66.79C106.5,-60.07 106.5,-50.4 106.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-41.19 106.5,-31.19 103,-41.19 110,-41.19\"/>\n",
       "</g>\n",
       "<!-- 138869186377904 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>138869186377904</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 138869186377904&#45;&gt;138869186370704 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>138869186377904&#45;&gt;138869186370704</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.5,-121.98C67.69,-114.23 80.01,-102.58 89.97,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.48,-95.59 97.34,-86.17 87.67,-90.5 92.48,-95.59\"/>\n",
       "</g>\n",
       "<!-- 138869186527696 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>138869186527696</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-207 23.5,-207 23.5,-177 77.5,-177 77.5,-207\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">a</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 138869186527696&#45;&gt;138869186377904 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>138869186527696&#45;&gt;138869186377904</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.84C50.5,-169.21 50.5,-159.7 50.5,-151.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.27 50.5,-141.27 47,-151.27 54,-151.27\"/>\n",
       "</g>\n",
       "<!-- 138869186379536 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>138869186379536</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-141 119,-141 119,-122 208,-122 208,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 138869186379536&#45;&gt;138869186370704 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>138869186379536&#45;&gt;138869186370704</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.34,-121.98C146,-114.23 133.47,-102.58 123.32,-93.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.53,-90.42 115.82,-86.17 120.76,-95.54 125.53,-90.42\"/>\n",
       "</g>\n",
       "<!-- 138869186364416 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>138869186364416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-201.5 113,-201.5 113,-182.5 214,-182.5 214,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 138869186364416&#45;&gt;138869186379536 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>138869186364416&#45;&gt;138869186379536</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-182.37C163.5,-174.25 163.5,-161.81 163.5,-151.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-151.17 163.5,-141.17 160,-151.17 167,-151.17\"/>\n",
       "</g>\n",
       "<!-- 138869186528816 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>138869186528816</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-274 136.5,-274 136.5,-243 190.5,-243 190.5,-274\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 138869186528816&#45;&gt;138869186364416 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>138869186528816&#45;&gt;138869186364416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-242.86C163.5,-233.68 163.5,-221.75 163.5,-211.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-211.82 163.5,-201.82 160,-211.82 167,-211.82\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7e4d00863970>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 例1：saved_tensor_hooks工作过程\n",
    "#  定义hook func\n",
    "def pack(x):\n",
    "    print('packing:', x)\n",
    "    return x\n",
    "def unpack(x):\n",
    "    print('unpacking:', x)\n",
    "    return x\n",
    "\n",
    "a = torch.ones(5, requires_grad=True)\n",
    "b = torch.ones(5, requires_grad=True) * 2\n",
    "\n",
    "# register pack/unpack hooks\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = a * b\n",
    "\n",
    "# 从图上可以看到：\n",
    "# 1.第1个packing的tensor是a\n",
    "# 2.第2个packing的tensor是b，但b不是leaf，它是torch.ones(...)*2的output\n",
    "torchviz.make_dot(y, params={'a':a, 'b':b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9e909a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.112119Z",
     "start_time": "2024-08-11T00:38:34.108529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpacking: tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n",
      "unpacking: tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b39d50ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.118851Z",
     "start_time": "2024-08-11T00:38:34.113261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([0.8033, 0.1748, 0.0890], requires_grad=True)\n",
      "packing: tensor([3.2131, 0.6993, 0.3559])\n",
      "unpacking: tensor([0.8033, 0.1748, 0.0890])\n"
     ]
    }
   ],
   "source": [
    "## 例2.1：自定义pack和unpack规则：改变tensor大小后恢复\n",
    "#  pack/unpack func满足“unpack(pack(x)) = x”的规则即可\n",
    "\n",
    "# ---> 例2.1和例2.2没有实际意义，只是展示pack和unpack的自定义能力\n",
    "\n",
    "def pack(x):\n",
    "    print('packing:', x * 4)\n",
    "    return x * 4\n",
    "def unpack(x):\n",
    "    print('unpacking:', x / 4)\n",
    "    return x / 4\n",
    "# pack/unpack func满足“unpack(pack(x)) = x”的规则\n",
    "\n",
    "torch.manual_seed(3)\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print('x =', x)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = x ** 2\n",
    "    \n",
    "y.sum().backward()\n",
    "assert(x.grad.equal(x * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ecda1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.123550Z",
     "start_time": "2024-08-11T00:38:34.119987Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2.2：自定义pack和unpack规则：保存index of a list\n",
    "\n",
    "storage = []\n",
    "\n",
    "def pack(x):\n",
    "    storage.append(x)\n",
    "    return len(storage) - 1\n",
    "\n",
    "def unpack(ind):\n",
    "    return storage[ind]\n",
    "\n",
    "torch.manual_seed(3)\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = x ** 2\n",
    "    \n",
    "y.sum().backward()\n",
    "assert(x.grad.equal(x * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adbf8fc",
   "metadata": {},
   "source": [
    "#### 用pack/unpack将tensor存到GPU之外的地方\n",
    "1. 这是GPU memory和time的trade off。官方样例中，用A100GPU测试，把ResNet152(batch size=256)存到cpu上，可以将gpu内存使用量从48G降低到5G,但是耗时增加6倍\n",
    "2. 一种折中是只把部分layer的tensor传到cpu或者其他位置。方法是，define a special nn.Module，用来wraps module and save its tensors to cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa8b94c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.130496Z",
     "start_time": "2024-08-11T00:38:34.124747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 例3：saving tensor to cpu\n",
    "\n",
    "#  1.人工手写\n",
    "def pack(x):\n",
    "    return (x.device, x.cpu())\n",
    "\n",
    "def unpack(package):\n",
    "    device, x = package\n",
    "    return x.to(device)\n",
    "\n",
    "torch.manual_seed(3)\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack, unpack):\n",
    "    y = x ** 2\n",
    "    \n",
    "y.sum().backward()\n",
    "# assert(x.grad.equal(x * 2))\n",
    "torch.allclose(x.grad, (2 * x))  # x.grad与2x值的差异在默认区间内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d37fbb5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.286872Z",
     "start_time": "2024-08-11T00:38:34.131508Z"
    }
   },
   "outputs": [],
   "source": [
    "#  2.pytorch已经实现了上述功能\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(5))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.autograd.graph.save_on_cpu(pin_memory=True):\n",
    "            return self.w * x\n",
    "\n",
    "x = torch.randn(5)\n",
    "model = Model()\n",
    "loss = model(x).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bad29cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.293871Z",
     "start_time": "2024-08-11T00:38:34.288565Z"
    }
   },
   "outputs": [],
   "source": [
    "#  3.module wrapper\n",
    "\n",
    "class SaveToCpu(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        with torch.autograd.graph.save_on_cpu(pin_memory=True):\n",
    "            return self.module(*args, **kwargs)\n",
    "        \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 100), \n",
    "    nn.ReLU(), \n",
    "    SaveToCpu(nn.Linear(100, 100)), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(100, 10),\n",
    ")\n",
    "\n",
    "x = torch.randn(10)\n",
    "loss = model(x).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98459c50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.307315Z",
     "start_time": "2024-08-11T00:38:34.295164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Double access failed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7002/2057767111.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensor = torch.load(name)\n"
     ]
    }
   ],
   "source": [
    "## 例4：saving tensor to disk\n",
    "\n",
    "## 错误的方式\n",
    "\n",
    "import uuid\n",
    "import os\n",
    "import tempfile\n",
    "tmp_dir_obj = tempfile.TemporaryDirectory()\n",
    "tmp_dir = tmp_dir_obj.name\n",
    "\n",
    "def pack_hook(tensor):\n",
    "    name = os.path.join(tmp_dir, str(uuid.uuid4()))\n",
    "    torch.save(tensor, name)\n",
    "    return name\n",
    "\n",
    "def unpack_hook(name):\n",
    "    tensor = torch.load(name)\n",
    "    os.remove(name)  # 如果这里remove，那么unpack就不能被call第二次\n",
    "    return tensor\n",
    "\n",
    "x = torch.ones(5, requires_grad=True)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "    y = x.pow(2)\n",
    "print(y.grad_fn._saved_self)      # 每执行一次就会unpack一次\n",
    "try:\n",
    "    print(y.grad_fn._saved_self)  # 第二次会失败\n",
    "    print(\"Double access succeeded!\")\n",
    "except:\n",
    "    print(\"Double access failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b639dba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.312290Z",
     "start_time": "2024-08-11T00:38:34.308900Z"
    }
   },
   "outputs": [],
   "source": [
    "## 正确的方式：利用pytorch自动释放saved data的机制\n",
    "#  pytorch自动释放不再需要的object，即这里的SelfDeletingTempFile object\n",
    "\n",
    "class SelfDeletingTempFile():\n",
    "    def __init__(self):\n",
    "        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n",
    "\n",
    "    def __del__(self):\n",
    "        os.remove(self.name)\n",
    "\n",
    "def pack_hook(tensor):\n",
    "    temp_file = SelfDeletingTempFile()\n",
    "    torch.save(tensor, temp_file.name)\n",
    "    return temp_file\n",
    "\n",
    "def unpack_hook(temp_file):\n",
    "    return torch.load(temp_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a96fd760",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.340297Z",
     "start_time": "2024-08-11T00:38:34.313689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only save on disk tensors that have size >= 1000\n",
    "SAVE_ON_DISK_THRESHOLD = 1000\n",
    "\n",
    "def pack_hook(x):\n",
    "    if x.numel() < SAVE_ON_DISK_THRESHOLD:\n",
    "        return x\n",
    "    temp_file = SelfDeletingTempFile()\n",
    "    torch.save(tensor, temp_file.name)\n",
    "    return temp_file\n",
    "\n",
    "def unpack_hook(tensor_or_sctf):\n",
    "    if isinstance(tensor_or_sctf, torch.Tensor):\n",
    "        return tensor_or_sctf\n",
    "    return torch.load(tensor_or_sctf.name)\n",
    "\n",
    "class SaveToDisk(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "            return self.module(*args, **kwargs)\n",
    "\n",
    "net = nn.DataParallel(SaveToDisk(Model()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc33edc",
   "metadata": {},
   "source": [
    "## 2. Hooks for nn.Module objects\n",
    "用于nn.Module的hook有forward和backward hook。\\\n",
    "**signature of hook function：** \n",
    "1. for backward hook: <font color=red>hook_func_name(module, grad_input, grad_output) -> Tensor or None </font>\n",
    "2. for foreward hook: <font color=red>hook_func_name(module, input, output) -> None </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de807d1",
   "metadata": {},
   "source": [
    "### 2.1 register forward hooks\n",
    "forward hook在forward pass中被调用，具体又有两种执行位置，**pre_hook**在forward method之前执行；**hook**在forward method执行完之后执行。\\\n",
    "下面1和2只对当前hook所reigster上的module有效。3和4是global hook，也就是installed for all modules。\n",
    "1. <font color=green>**register_forward_pre_hook(hook_func_name)**</font>\n",
    "2. <font color=green>**register_forward_hook(hook_func_name)**</font>\n",
    "3. <font color=green>**register_module_forward_pre_hook(hook_func_name)**</font>\n",
    "4. <font color=green>**register_module_forward_hook(hook_func_name)**</font>："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d83818",
   "metadata": {},
   "source": [
    "### 2.2 register backward hooks\n",
    "backward hook只有一种，在backward method执行完后执行。下面1和2只对当前hook所reigster上的module有效。3和4是global hook，也就是installed for all modules。\n",
    "1. <font color=green>**register_full_backward_pre_hook(hook_func_name)**</font>\n",
    "2. <font color=green>**register_full_backward_hook(hook_func_name)**</font>：这个是原register_backward_hook()\n",
    "3. <font color=green>**register_module_full_backward_hook(hook_func_name)**</font>\n",
    "4. <font color=green>**register_module_full_backward_pre_hook(hook_func_name)**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c94ff382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.345637Z",
     "start_time": "2024-08-11T00:38:34.341973Z"
    }
   },
   "outputs": [],
   "source": [
    "### 例1：定义不同的hooks，看他们的工作方式\n",
    "\n",
    "##  新建module和input\n",
    "m = nn.Linear(3, 3)\n",
    "torch.manual_seed(1)\n",
    "x = torch.randn(2, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7339d548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.349154Z",
     "start_time": "2024-08-11T00:38:34.346714Z"
    }
   },
   "outputs": [],
   "source": [
    "## 定义foreward hook function\n",
    "#  1.用于在forward pass前，检查或者调整inputs\n",
    "def forward_pre_hook(m, inputs):  # 注，inputs都是wrapped成tuple类型的\n",
    "    input = inputs[0]\n",
    "    return input + 1.\n",
    "\n",
    "#  2.用于在forward pass后，检查inputs/outputs或者调整outputs\n",
    "def forward_hook(m, inputs, output):# inputs都wrapped成tuple，output按原类型传\n",
    "    # 按ResNet的方式计算residual\n",
    "    return output + inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e0d2a3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.353249Z",
     "start_time": "2024-08-11T00:38:34.350282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1644,  0.2539, -0.0903],\n",
      "        [-0.5065,  0.0724, -0.1563]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## register前的输出\n",
    "print(format(m(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8088d594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.357081Z",
     "start_time": "2024-08-11T00:38:34.354250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3663,  1.1868, -0.1191],\n",
      "        [ 0.0242,  1.0053, -0.1852]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# register forward_pre_hook后会产生不同的output：\n",
    "forward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)\n",
    "print(format(m(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c011d73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.361098Z",
     "start_time": "2024-08-11T00:38:34.358265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0277, 2.4538, 0.9425],\n",
      "        [1.6455, 1.5534, 0.6487]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# register forward_hook后会产生另一种不同的output：\n",
    "forward_hook_handle = m.register_forward_hook(forward_hook)\n",
    "print(format(m(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7458c052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.364933Z",
     "start_time": "2024-08-11T00:38:34.362294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1644,  0.2539, -0.0903],\n",
      "        [-0.5065,  0.0724, -0.1563]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 去掉hooks之后，output与register hooks之前的值一致\n",
    "forward_pre_hook_handle.remove()\n",
    "forward_hook_handle.remove()\n",
    "\n",
    "print(format(m(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b12f135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.369514Z",
     "start_time": "2024-08-11T00:38:34.366121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4999, 0.7264, 0.2085],\n",
      "        [0.4999, 0.7264, 0.2085]])\n"
     ]
    }
   ],
   "source": [
    "## 定义backward hook function\n",
    "#  功能：检查grad_inputs/grad_outputs或者调整剩余bp流程中用的grad_inputs\n",
    "#  注，grad_inputs/grad_outputs都wrapped成tuple\n",
    "def backward_hook(m, grad_inputs, grad_outputs): \n",
    "    new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]\n",
    "    return new_grad_inputs\n",
    "\n",
    "## 没有register backward hooks时的输出：\n",
    "m(x).sum().backward()\n",
    "print(format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acdf62c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.376679Z",
     "start_time": "2024-08-11T00:38:34.372634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[42., 42., 42.],\n",
      "        [42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 在重做Backward Propagation前要先Clear gradients\n",
    "m.zero_grad()\n",
    "x.grad.zero_()\n",
    "\n",
    "# register了backward hooks之后的输出：\n",
    "backward_hook_handle = m.register_full_backward_hook(backward_hook)\n",
    "m(x).sum().backward()\n",
    "print(format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02629b0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.380754Z",
     "start_time": "2024-08-11T00:38:34.377688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4999, 0.7264, 0.2085],\n",
      "        [0.4999, 0.7264, 0.2085]])\n"
     ]
    }
   ],
   "source": [
    "# 删除backward hooks之后的输出：\n",
    "backward_hook_handle.remove()\n",
    "\n",
    "m.zero_grad()\n",
    "x.grad.zero_()\n",
    "m(x).sum().backward()\n",
    "print(format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f277a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.385360Z",
     "start_time": "2024-08-11T00:38:34.381956Z"
    }
   },
   "outputs": [],
   "source": [
    "## 例2：用forward_hook查看module中activation value的例子\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.conv = nn.Conv2d(3,8,2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4,4))\n",
    "        self.fc = nn.Linear(8*4*4 , 1)\n",
    "    def forward(self, x):\n",
    "          x = F.relu(self.conv(x))\n",
    "          x = self.pool(x)\n",
    "          x = x.view(x.shape[0] , -1)\n",
    "          x = self.fc(x)\n",
    "          return x\n",
    "\n",
    "# 定义hook，用来提取activation的结果，也就是feature\n",
    "features = {} \n",
    "def hook_func(model, input ,output):\n",
    "    features['feature'] = output.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cbda9bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.391891Z",
     "start_time": "2024-08-11T00:38:34.386413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "# 在pooling layer上register hook\n",
    "net.pool.register_forward_hook(hook_func)\n",
    "\n",
    "x= torch.randn(1,3,10,10)\n",
    "output = net(x)\n",
    "print(features['feature'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf137ed",
   "metadata": {},
   "source": [
    "### 2.3 典型应用场景和应用方式\n",
    "#### 场景1. 在模型训练过程中打印所需信息\n",
    "**1. 方法：** 给module加wrapper，在wrapper module上加hooks\\\n",
    "**2. 优点：** \\\n",
    "（1）方便debug，避免手动增加和删除print的麻烦 \\\n",
    "（2）不仅可以在自定义module，还可以在pytorch自带的module和第三方module上使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec4d0f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:34.397002Z",
     "start_time": "2024-08-11T00:38:34.393505Z"
    }
   },
   "outputs": [],
   "source": [
    "## 场景1：在ResNet18上用hooks来打印model信息\n",
    "\n",
    "#  给model加一个wrapper class\n",
    "class VerboseNet(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "        # register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            # 虽然所有nn.Mudule的实例都继承了__name__属性，但有的module可能没赋值\n",
    "            # 这一步可以明确让layer都有对应的name\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer, _, output: print(f\"new --->: {layer.__name__}: {output.shape}\")\n",
    "            )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2506626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:35.329137Z",
     "start_time": "2024-08-11T00:38:34.398174Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roark/anaconda3/envs/231n/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/roark/anaconda3/envs/231n/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new --->: conv1: torch.Size([10, 64, 14, 14])\n",
      "new --->: bn1: torch.Size([10, 64, 14, 14])\n",
      "new --->: relu: torch.Size([10, 64, 14, 14])\n",
      "new --->: maxpool: torch.Size([10, 64, 7, 7])\n",
      "new --->: layer1: torch.Size([10, 64, 7, 7])\n",
      "new --->: layer2: torch.Size([10, 128, 4, 4])\n",
      "new --->: layer3: torch.Size([10, 256, 2, 2])\n",
      "new --->: layer4: torch.Size([10, 512, 1, 1])\n",
      "new --->: avgpool: torch.Size([10, 512, 1, 1])\n",
      "new --->: fc: torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "verbose_resnet = VerboseNet(resnet18(weights=ResNet18_Weights.DEFAULT))\n",
    "dummy_input = torch.ones(10, 3, 28, 28)\n",
    "\n",
    "# 这里用了assignment是为了不让jupyter打印函数的output\n",
    "_ = verbose_resnet(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b830b",
   "metadata": {},
   "source": [
    "#### 场景2. Feature extraction\n",
    "**1. 方法：** 给module加wrapper，在wrapper module上加hooks\\\n",
    "**2. 优点：** 在一个预训练好的模型上做transfer learning时，可能想查看该模型摸些layers上得到的feature。hook可以在不改变模型本身的条件下实现这一需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e02ff083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:35.335865Z",
     "start_time": "2024-08-11T00:38:35.330886Z"
    }
   },
   "outputs": [],
   "source": [
    "## 场景2：Feature extraction\n",
    "\n",
    "class FeatureExt(nn.Module):\n",
    "    def __init__(self, model, layers):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self._features = {layer: torch.empty(0) for layer in layers}\n",
    "        \n",
    "        for layer_i in layers:\n",
    "            # 把named_modules的sub-module list构造成dict，用layer_i索引\n",
    "            # [说明见下一个cell]\n",
    "            layer = dict([*self.model.named_modules()])[layer_i]\n",
    "            \n",
    "            # layer_i索引出来对应layer后，给该layer加上hook:\n",
    "            layer.register_forward_hook(self.save_outputs_hook(layer_i))\n",
    "         \n",
    "    # 定义hook function：\n",
    "    def save_outputs_hook(self, layer_i):\n",
    "        def fn(_, __, output):  # 下划线长度不同，因为arguments name不能相同\n",
    "            # 将该layer的output提取出来，存入feature dict\n",
    "            self._features[layer_i] = output\n",
    "        return fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _ = self.model(x)\n",
    "        return self._features  # 返回dict存放了指定layers的activation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5432737c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:35.532319Z",
     "start_time": "2024-08-11T00:38:35.337312Z"
    }
   },
   "outputs": [],
   "source": [
    "res18 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "## 对resnet.named_modules的说明：\n",
    "#  1. resnet18.named_modules是一个generator\n",
    "#     每次yield返回的是tuple，形如('layer_name', module)\n",
    "#  2. 加'*'来把generator解包成单独的tuples：*res18.named_modules()\n",
    "#  3. 将dict()用到list of tuples上，可以转换成key-value pair，便于用key索引\n",
    "#     这里要先将解包后的tuples打包到一个list里面，所以是用 dict([])\n",
    "\n",
    "#  ----- 打印sample layers 来查看网络的结构  -----\n",
    "# print([*res18.named_modules()][0]) # nest结构的顶层，描绘整个结构\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][5]) # nest结构的第2层，'layer1'\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][6]) # nest结构的第3层，'layer1'的sub-module\n",
    "# print('-' * 40)\n",
    "# print([*res18.named_modules()][7]) # nest结构的第4层，..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "576976ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T00:38:35.556794Z",
     "start_time": "2024-08-11T00:38:35.534011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4 -> torch.Size([10, 512, 1, 1])\n",
      "avgpool -> torch.Size([10, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "res18_feats = FeatureExt(res18, layers=['layer4', 'avgpool'])\n",
    "\n",
    "fests = res18_feats(dummy_input)\n",
    "for name, output in fests.items():\n",
    "    print(name, '->', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c735ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
