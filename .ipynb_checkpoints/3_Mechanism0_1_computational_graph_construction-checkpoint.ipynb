{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57bd4164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:52:10.355740Z",
     "start_time": "2024-08-11T07:52:08.997941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/roark/.local/lib/python3.10/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "print(torch.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828b863",
   "metadata": {},
   "source": [
    "# how computational graph is constructed\n",
    "参考：\n",
    "1. How Computational Graphs are Constructed in PyTorch https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/\n",
    "2. How Computational Graphs are Executed in PyTorch\n",
    "https://pytorch.org/blog/how-computational-graphs-are-executed-in-pytorch/\n",
    "3. pytorch internals http://blog.ezyang.com/category/pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b614b1",
   "metadata": {},
   "source": [
    "## I. pytorch的基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92981599",
   "metadata": {},
   "source": [
    "### I.1 pytorch源码核心文件概况\n",
    "<font color=blue>**四个最重要的文件夹：**</font>\n",
    "1. **torch**: 用户import和使用的python modules\n",
    "2. **torch/csrc/：**pytorch前端功能的c++ code实现，主要是包括：\n",
    "   1. python代码与c++代码之间转换的binding code。\n",
    "   2. autograd engine在其中的子文件夹autograd中。\n",
    "   3. JIT compiler在其中的子文件夹jit中。\n",
    "   4. c++前端代码\n",
    "3. **aten/：**‘A tensor library’的简称，用c++写的。是fundational tensor operation library。基本所有tensor的其他操作都是在aten提供的基础上实现的。 \n",
    "   - kernel proper在aten中\n",
    "4. **c10/：**'Caffe2'和'A Ten'的简称。是core abstractions of pytorch，包括Tensor和Storage data structure的具体实现。\n",
    "\n",
    "<font color=blue>**用户代码的工作方式：**</font>\n",
    "1. 用户用torch中提供的modules写代码\n",
    "2. pytorch的编译器会将用户代码转化成用户代码的c++实现。\n",
    "   - <font color=red>注:这部分代码是动态生成的，不是调用提前写好的package或者lib。</font>\n",
    "   - 这部分代码中包含了torch/csrc/的binding code，autograd engine等内容，用于明确用户需要的功能如何用c++实现。\n",
    "3. 上述c++代码调用torch/csrc/、aten和c10中的内容完成计算。\n",
    "   - aten中的operation lib\n",
    "   - c10中的data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f90b25",
   "metadata": {},
   "source": [
    "### I.2 tensor相关的数据结构：tensor, variable和storage\n",
    "#### 1. Storage\n",
    "   - storage是tensor的underlying data buffer。其数据结构定义为struct Storage，只有基础data相关的信息，如：data_ptr, data type, byte_size, device等，没有view需要的stride信息和autograd需要的autoMeta信息。\n",
    "   - 由于storage struct不含有stride信息，因此可以实现同一个data对应tensor的不同view。\n",
    "   - Storage的定义在<font color=green>'c10/core/StorageImpl.h'</font>文件中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd296d2d",
   "metadata": {},
   "source": [
    "#### 2. Variable：最初Variable wraps tensor\n",
    "   - 过去为了完成autograd，对tensor和function都做了wrap。Variable wraps tensor，这里wrapper的主要工作是加上了tensor参加autograd时需要的信息AutogradMeta。\n",
    "     - 所以，<font color=blue>过去一般把tensor理解成不需要requires_grad的Variable，此时就不需要对它们apply autograd machinery了。</font><font color=red>现在Variable也改为定义成了tensor。未来pytorch团队会取消Variable，把现在Variable实现的功能都在tensor中直接实现。</font>\n",
    "   - Variable以c++的形式定义在<font color=green>'torch/csrc/autograd/variable.h'</font>文件中。这里定义了struct AutogradMeta类型，主要包括的信息有：\n",
    "     - grad_\n",
    "     - 指向Node类型的ptr：grad_fn_\n",
    "     - 指向Node类型的ptr：grad_accumulator_\n",
    "   - 提供的常用接口：requires_grad(), grad()\n",
    "- <font color=orange>虽然现在varible和tensor是一样的。但为了方便理解，还是可以将Variable简单理解成：Variable可以实现tensor能实现的所有ops，并且能interact with autograd machinery.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02043973",
   "metadata": {},
   "source": [
    "#### 3. Tensor\n",
    "   - tensor的定义在<font color=green>'c10/core/TensorImpl.h'</font>文件中。其中主要的信息包括：\n",
    "     - 一个指向storage struct的pointer，在这个storage struct中还有一个指向底层data的pointer\n",
    "     - metadata：view specifical metadata，包括size, offset和strides等信息\n",
    "   - tensor的定义提供给用户的接口主要有两类：\n",
    "     1. <font color=blue>**查询tensor attribute的接口**</font>: layout(), device(), dtype(), stride(), is_continuous(), is_sparse(), is_cpu(), is_cuda(), etc.\n",
    "     2. <font color=blue>**autograd interface**</font>: set_requires_grad(), requires_grad(), grad(), is_neg(),etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80cbb98",
   "metadata": {},
   "source": [
    "## II. Computation Graph是如何构建的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5da08a",
   "metadata": {},
   "source": [
    "### II.1 autograd相关的源码文件\n",
    "三个关键文件夹：\n",
    "1. <font color=blue>**tools/autograd：**</font>其中包括以下内容：\n",
    "   1. 文档derivatives.yaml：定义了tensor基本ops的梯度计算方法，也就是这些function的vjp()。\n",
    "   2. 一个名为templates的文件夹和一些python scripts。他们在building time期间，按照derivatives.yaml中对应函数的定义，生成完成对应computation需要的c++ code。\n",
    "      - 另外，scripts还负责为aten中的functions生成wrapper，用它们wrapped之后的版本来构建计算图。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259a838",
   "metadata": {},
   "source": [
    "2. <font color=blue>**torch/autograd：**</font>用户用pytorch的autograd功能的接口都在这个文件中。其中主要有以下几个python script：\n",
    "   - <font color=norange>function.py：</font>这里定义了class torch.autograd.Function，所有pytorch中的differentiable function都是通过继承这个class定义的。\n",
    "     - 另外以function为base class，又定义了InplaceFunction和NestedIOFunction两个class\n",
    "   - <font color=norange>functional.py：</font>\"vjp\", \"jvp\", \"jacobian\", \"hessian\", \"hvp\", \"vhp\"这6个函数的实现。\n",
    "   - <font color=norange>grad_mode.py：</font>提供了用来set grad mode的几个function\n",
    "   - <font color=norange>grad_check.py：</font>gradcheck()和gradgradcheck()两个函数的实现\n",
    "   - 另外还有实现autograd profiler, anomaly detection等功能的python scripts。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9f061",
   "metadata": {},
   "source": [
    "3. <font color=blue>**torch/csrc/autograd：**</font>这里全部是c++ code。所有computation graph生成和执行相关的代码都在这个文件夹中。<font color=green>主要有三类文件，分别implement：autograd engine, metadata storage和其他需要的components。</font>另外还有一些python_开头的文件，用来允许autograd engine使用python objects。一些重要的文档包括：\n",
    "   - <font color=norange>variable.h：</font>定义class Variable，详见前文。\n",
    "   - <font color=norange>edge.h：</font>定义struct Edge，用来表示a particular input of a function.\n",
    "   - <font color=norange>function.h:</font> 定义class Node, 用Node定义的struct TraceableFunction\n",
    "   - <font color=norange>engine.h：</font>定义了struct Engine，用于管理整个梯度计算的工作进程。\n",
    "   - <font color=norange>autograd.h: </font>定义了backward()和grad()两个函数，他们的功能都是计算梯度，区别是:backward把梯度计算的结构直接accumulate到input tensor上；grad()会将计算出来的梯度作为函数返回值返回，不会改变对应input tensor的grad属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b0e5f",
   "metadata": {},
   "source": [
    "### II.2 计算图的三个数据结构\n",
    "#### Graph：只是抽象数据结构，没有具体的object\n",
    "- Graph在pytorch中是一个抽象概念，并没有一个Graph对应的数据结构，但是有Node和Edge的数据结构定义。Node obj由Edge obj连接起来构成了Graph这个抽象的数据结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38ce1f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:52:10.361155Z",
     "start_time": "2024-08-11T07:52:10.357933Z"
    }
   },
   "outputs": [],
   "source": [
    "## 3个operation function构成的计算图示意：Forward chain与Backward chain的对应关系\n",
    "#\n",
    "#    in2    |||||||||||||   out1=in2   |||||||||||||   out2=in3   ||||||||||||| out3\n",
    "#  -------> |    f_1    | -----------> |    f_2    | -----------> |    f_3    |-----> \n",
    "#           |||||||||||||              |||||||||||||              |||||||||||||\n",
    "#               |                          |                         |\n",
    "#               |out1.grad_fn              |out1.grad_fn             |out3.grad_fn\n",
    "#               V                          V                         V\n",
    "#           |||||||||||||              |||||||||||||              |||||||||||||\n",
    "#   outf1B  | Node:     | inf1B=outf2B | Node:     | inf2B=outf3B | Node:     |  inf3B\n",
    "#  <------- |f1_Backward| <----------- |f2_Backward| <----------- |f3_Backward| <-----\n",
    "#           |||||||||||||     dout1    |||||||||||||     dout2    |||||||||||||  dout3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcce9be",
   "metadata": {},
   "source": [
    "#### Node obj\n",
    "- 定义在<font color=orange>**torch/csrc/autograd/function.h**</font>中。定义为struct Node，但是继承了c++标准库的Node类，因此是一个class。\n",
    "- <font coloblue>所有autograd machinery中的函数都继承了这个class</font>\n",
    "- 定义中包括的信息: \n",
    "  1. <font color=blue>**override of the operator()**</font>，该override函数将'operator()'的功能重新定义为call apply(inputs)来perform the actual function。Node类型的函数f就可以用f()来invoke函数自己定义的apply(inputs)。\n",
    "     - 但operator()的定义中，apply()是virtual function，没有给实际定义，所以autograd machinery中的函数都要override apply()。<font color=deeppink>building time期间，tools/autograd中的python scripts文件自动生成grad_fn指向的Backward对象的c++ code，其中就包括override apply()函数。</font>\n",
    "  2. Nodes通过他们的next_edge接口而连在一起，实现Graph connectivity。Node定义中除了operator()的override之外，就是维护它的list of next edges。Node本身在定义的时候用的parameter(形式参数)就是edge_list()。\n",
    "     - class定义中提供的相关接口有：\n",
    "       - num_inputs()和num_outputs()：分别返回Nodes的inputs和outputs数量。注意，'grad_fn'指向的Node的inputs是原函数的outputs。 \n",
    "       - add_next_edge(edge)：用来给Nodes增加一个edge\n",
    "       - get_next_edge(index)：用来retrieve Node的next_edge信息\n",
    "       - next_edges(): 用来遍历所有的next_edges\n",
    "       - set_next_edge(index, edge): \n",
    "- 例子：如下图\n",
    "  - func3的output的grad_fn指向Node1\n",
    "  - func3有两个inputs，分别是func1和func2的output，因此在backward chain中：\n",
    "    - func3对应的Node1就有两个grad_outputs，分别是Node1和Node2的grad_inputs\n",
    "    - Node1的edge_list()中有两个next_edge，分别指向Node1和Node2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0aa3f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:52:10.377158Z",
     "start_time": "2024-08-11T07:52:10.362865Z"
    }
   },
   "outputs": [],
   "source": [
    "#     Node2    <------- ptr ------ next_edges[0] ---------------  \n",
    "#   (bacward)                                                   |\n",
    "#       |                                                       |\n",
    "#    func1: out, nr=1 -----                                     |\n",
    "#   (forward)              \\                                    |\n",
    "#                         input,nr=1                            |\n",
    "#                           \\                                   |\n",
    "#                           --> func3 - out, nr=3 - grad_fn -> Node1 \n",
    "#                           /                              (bacward) \n",
    "#                         input,nr=2                            |\n",
    "#                         /                                     |\n",
    "#    func2: out, nr=2 ----                                      |\n",
    "#   (forward)                                                   |\n",
    "#       |                                                       |\n",
    "#     Node3    <------- ptr ------ next_edges[1] ---------------  \n",
    "#   (backward) \n",
    "#\n",
    "#              <==================   ==================> \n",
    "#               backward chain方向    forward chain方向"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556c787",
   "metadata": {},
   "source": [
    "#### Edge obj\n",
    "- 定义在<font color=orange>**torch/csrc/autograd/edge.h**</font>中。定义为struct Edge.\n",
    "- 实例化的parameters(形式参数)是Node pointer和input_nr，所以Edge可以用(Node, input_nr)pair来表示。\n",
    "  - Node：direct edge的pointer所指向的Node。这个Nodey也是某个grad_fn指向的obj。这个grad_fn对应的forward function的output是backward chain中前序Node的input。\n",
    "  - input_nr：Node对应forward func的output作为bacward chain中前序Node的forward func的input时的序号。\n",
    "  - 还有几个Edge定义的ops: ==,!=,is_valid()  \n",
    "- Edge用pointer的方式把backward chain中涉及的grad_fn Nodes顺序连接起来。\n",
    "- 例子：如上图：\n",
    "    - func3对应的Node1有两个next_edge，分别指向Node1和Node2\n",
    "    - 这两个edge有各自相应的input number序号，即input_nr的取值     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d71439",
   "metadata": {},
   "source": [
    "### II.3 计算图的构造过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd7222",
   "metadata": {},
   "source": [
    "<font color=blue>**step1：定义一个requires_grad-True**</font>\n",
    "- 之后c10中的allocator会allocate一个AutogradMeta对象来hold graph信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29748e95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:52:10.386560Z",
     "start_time": "2024-08-11T07:52:10.379334Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.ones((2, 2), requires_grad=True)\n",
    "w = torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bd64c",
   "metadata": {},
   "source": [
    "- <font color=orange>struct AutogradMeta定义在torch/csrc/autograd/variable.h中，其中主要信息包括：\n",
    "  1. tensor的梯度\n",
    "  2. 一个指针grad_fn指向一个函数，后面engine会call这个function来计算梯度；\n",
    "  3. 一个梯度累积对象\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec5449d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:52:10.390984Z",
     "start_time": "2024-08-11T07:52:10.388140Z"
    }
   },
   "outputs": [],
   "source": [
    "## struct TORCH_API AutogradMeta的主要内容\n",
    "\n",
    "# struct TORCH_API AutogradMeta : Public c10::AutogradMetaInerface{\n",
    "#     std::string name_;\n",
    "    \n",
    "#     Variable grad_;\n",
    "#     std::shared_ptr<Node> grad_fn_;\n",
    "#     std::weak_ptr<Node> grad_accumulator_;\n",
    "#     // other contents\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5a998",
   "metadata": {},
   "source": [
    "<font color=blue>**step2：让tensor作为input，call a differentiable function**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228c3baa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:52:10.396147Z",
     "start_time": "2024-08-11T07:52:10.392432Z"
    }
   },
   "outputs": [],
   "source": [
    "v = x @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107fd9c",
   "metadata": {},
   "source": [
    "- tools/autograd中的python scripts文件会wrap aten中定义的乘法运算函数，用函数中的内容来构建backward graph。\n",
    "- 具体方式：tools/autograd/gen_autograd.py会call tools/autograd/gen_varible_type.py，后者负责按tools/autograd/templates中给的template自动写wrapper代码，并将代码输出到torch/csrc/autograd/generated文件夹中。\n",
    "- 文件中的内容包括：\n",
    "  1. wrap backward method的Node的定义\n",
    "  2. tensor multiplication function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09710b",
   "metadata": {},
   "source": [
    "#### <font color=green>1. wrap backward method的Node的定义：</font>\n",
    "<font color=deeppink>**class 'MulBackward0'**</font>继承自TraceableFunction class，后者继承自Node。内容包括：\n",
    "1. generic Node定义所需要的arguments：sequence_nr和edge_list()\n",
    "2. 重载generic Node中的apply method：按derivatives.yaml中定义的规则生成计算函数梯度的vjp函数，放到class的apply method中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea11a413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:52:10.400282Z",
     "start_time": "2024-08-11T07:52:10.397732Z"
    }
   },
   "outputs": [],
   "source": [
    "## MulBackward0的c++ code示意，claude生成的参考内容，不是真的官方代码\n",
    "\n",
    "# class MulBackward0 : public Node {\n",
    "# public:\n",
    "#     MulBackward0(edge_list edges, int64_t sequence_nr) \n",
    "#         : Node(std::move(edges), sequence_nr) {}\n",
    "\n",
    "#     virtual variable_list apply(variable_list&& inputs) override {\n",
    "#         // Implementation of vjp function of multiple operator\n",
    "#         // 调用:mul_tensor_backward(grad, self, other_scalar_type)\n",
    "#     }\n",
    "# };"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe721257",
   "metadata": {},
   "source": [
    "#### <font color=green>2. tensor multiplication function\n",
    "执行的核心逻辑：\n",
    "1. 声明grad_fn空指针\n",
    "2. 实例化MulBackward0对象，并赋值给grad_fn\n",
    "3. 从inputs中拿他们的grad_fn设置当前grad_fn的next_edges，<font color=norange>**构建Graph**</font>\n",
    "4. 完成forward计算，将结果赋值给result tensor，同时登记output_nr\n",
    "   - <font color=red>每个output在被return的时候都会给一个output number属性</font>\n",
    "5. 把grad_fn设为result tenosr的属性。<font color=norange>**实现output与Graph的link**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba5689a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:52:10.404793Z",
     "start_time": "2024-08-11T07:52:10.401496Z"
    }
   },
   "outputs": [],
   "source": [
    "# at::Tensor mul_Tensor(c10::DispatchKeySet ks, \n",
    "#                       const at::Tensor & self, \n",
    "#                       const at::Tensor & other) {\n",
    "#   ...\n",
    "#   // 判断input中有没有tensor有requires_grad=True\n",
    "#   auto _any_requires_grad = compute_requires_grad( self, other );\n",
    "\n",
    "#   // 声明一个指向MulBackward0类型的指针\n",
    "#   std::shared_ptr<MulBackward0> grad_fn;\n",
    "\n",
    "#   // 如果input中有tensor的requires_grad=True\n",
    "#   if (_any_requires_grad) {\n",
    "\n",
    "#     // 实例化一个MulBackward0对象，并用grad_fn指向它\n",
    "#     grad_fn = std::shared_ptr<MulBackward0>(new MulBackward0(), deleteNode);\n",
    "\n",
    "#     // 为Node设置next_edges，完成Node与backward chain上后序Node的连接\n",
    "#     // 用各个input tensor的grad_fn给edge中的ptr赋值即可\n",
    "#     grad_fn->set_next_edges(collect_next_edges( self, other ));\n",
    "#     ...\n",
    "#   }\n",
    "#   …\n",
    "\n",
    "#   // 执行ATen中真正的forward function运算\n",
    "#   auto _tmp = ([&]() {\n",
    "#     at::AutoDispatchBelowADInplaceOrView guard;\n",
    "#     return at::redispatch::mul(ks & c10::after_autograd_keyset, self_, other_);\n",
    "#   })();\n",
    "\n",
    "#   // 定义result tensor，将计算结果赋值给它\n",
    "#   auto result = std::move(_tmp);\n",
    "#     if (grad_fn) {\n",
    "\n",
    "#        // 将grad_fn设置为result tensor的属性，从而result tensor连到Graph\n",
    "#       set_history(flatten_tensor_args( result ), grad_fn);\n",
    "#   }\n",
    "#   ...\n",
    "#   return result;\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142259ab",
   "metadata": {},
   "source": [
    "- <font color=norange>**connect_next_edges(Variables&&... variables)**</font>\n",
    "  - 调用功能模块遍历输入的variables，分别对他们执行gradient_edge()。不同情况的处理：\n",
    "    1. intermediate output，grad_fn非none：\n",
    "       - 用variable.grad_fn设置edge的pointer\n",
    "       - 用variable.output_nr()设置edge的input_nr\n",
    "    2. leaf node，requires_grad=True：\n",
    "       - 它的grad_fn is null，edge ptr指向它的accumulator\n",
    "       - edge的input_nr设为0\n",
    "    3. 其他情况没有edge，在实现遍历功能的函数模块中处理了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7dbd367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:52:10.408572Z",
     "start_time": "2024-08-11T07:52:10.406297Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Edge gradient_edge(const Variable& self) { \n",
    "#     if (const auto& gradient = self.grad_fn()) {\n",
    "#       return Edge(gradient, self.output_nr());\n",
    "#     } else {\n",
    "#       return Edge(grad_accumulator(self), 0);\n",
    "#     }\n",
    "#   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add86904",
   "metadata": {},
   "source": [
    "- <font color=norange>**set_next_edges(next_edges)**</font>\n",
    "  - 将connect_next_edges返回的带index信息(每个edge构造时的output_nr()参数)的edge list设置为当前Node的next_egds。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   void set_next_edges(edge_list&& next_edges) {\n",
    "#     next_edges_ = std::move(next_edges);\n",
    "#     for(const auto& next_edge : next_edges_) {\n",
    "#       update_topological_nr(next_edge);\n",
    "#     }\n",
    "#   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c9b20",
   "metadata": {},
   "source": [
    "- <font color=norange>**set_history(result, grad_fn)**</font>\n",
    "  - 先获取result的output_nr。\n",
    "    - 调用grad_fn.add_input_metadata(variable)，从含义上看，Node添加的第i个input也就对应着forward function的第i个output。\n",
    "  - 将grad_fn和output_nr设置到variable的AutogradMeta obj中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f952f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inline void set_history(\n",
    "#     at::Tensor& variable,\n",
    "#     const std::shared_ptr<Node>& grad_fn) {\n",
    "#   AT_ASSERT(grad_fn);\n",
    "#   if (variable.defined()) {\n",
    "#     // If the codegen triggers this, you most likely want to add your newly added function\n",
    "#     // to the DONT_REQUIRE_DERIVATIVE list in tools/autograd/gen_variable_type.py\n",
    "#     TORCH_INTERNAL_ASSERT(isDifferentiableType(variable.scalar_type()));\n",
    "\n",
    "#     // 获取result的output_nr\n",
    "#     auto output_nr =\n",
    "#         grad_fn->add_input_metadata(variable);\n",
    "\n",
    "#     // 将grad_fn和output_nr设置到variable的AutogradMeta obj中\n",
    "#     impl::set_gradient_edge(variable, {grad_fn, output_nr});\n",
    "#   } else {\n",
    "#     grad_fn->add_input_metadata(Node::undefined_input());\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32dc6a",
   "metadata": {},
   "source": [
    "- <font color=norange>**add_input_metadata(variable)**</font>\n",
    "  - 是backward Node的method，用于将new input(对应forward方向的output)的type, shape等metadata中的信息加到Node的input_metadata_属性中。\n",
    "  - <font color=blue>返回input_nr，其值表示这是该Node的第几个input。等于该input是对应的forward function的第几个output。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d484e52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T08:41:44.853635Z",
     "start_time": "2024-08-11T08:41:44.850577Z"
    }
   },
   "outputs": [],
   "source": [
    "#   uint32_t add_input_metadata(const at::Tensor& t) noexcept {\n",
    "#     uint32_t input_nr = input_metadata_.size();\n",
    "#     input_metadata_.emplace_back(t);\n",
    "#     return input_nr;\n",
    "#   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae586974",
   "metadata": {},
   "source": [
    "- <font color=norange>**set_gradient_edge(variable, edge)**</font>\n",
    "  - 实际函数：set_gradient_edge(result, {grad_fn, output_nr})\n",
    "  - 功能：\n",
    "    - 从Graph的角度来看，就是帮result tensor设置连接Node的Edge，完成Graph的构造\n",
    "    - 从对Tensor本身的处理来看，就是将连接Backward method的Edge的信息填入它的AutogradMeta\n",
    "  - 实际完成的工作：\n",
    "    1. 填入AutogradMeta信息中的grad_fn\n",
    "    2. 填入AutogradMeta信息中的output_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7db978f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T09:19:01.843934Z",
     "start_time": "2024-08-11T09:19:01.839625Z"
    }
   },
   "outputs": [],
   "source": [
    "#  void set_gradient_edge(const Variable& self, Edge edge) {\n",
    "\n",
    "#     // 给output tensor加上AutogradMeta obj\n",
    "#     auto* meta = materialize_autograd_meta(self);\n",
    "\n",
    "#     // 填入AutogradMeta信息中的grad_fn和output_nr\n",
    "#     meta->grad_fn_ = std::move(edge.function);\n",
    "#     meta->output_nr_ = edge.input_nr;\n",
    "\n",
    "#     // This logic is only relevant for custom autograd Functions for which multiple\n",
    "#     // operations can happen on a given Tensor before its gradient edge is set when\n",
    "#     // exiting the custom Function.\n",
    "#     auto diff_view_meta = get_view_autograd_meta(self);\n",
    "#     if (diff_view_meta && diff_view_meta->has_bw_view()) {\n",
    "#       diff_view_meta->set_attr_version(self._version());\n",
    "#     }\n",
    "#   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32fc133d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:52:10.415511Z",
     "start_time": "2024-08-11T07:52:10.411044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9191, -0.5894,  0.4019],\n",
      "        [-0.9191, -0.5894,  0.4019]], grad_fn=<MmBackward0>) <class 'MmBackward0'>\n"
     ]
    }
   ],
   "source": [
    "y = v.sum()\n",
    "print(v, type(v.grad_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5d144bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T07:52:10.465490Z",
     "start_time": "2024-08-11T07:52:10.417020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"109pt\" height=\"271pt\"\n",
       " viewBox=\"0.00 0.00 109.00 271.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 267)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-267 105,-267 105,4 -4,4\"/>\n",
       "<!-- 129571388155088 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>129571388155088</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"77.5,-31 23.5,-31 23.5,0 77.5,0 77.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 129568406431264 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>129568406431264</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-86 6,-86 6,-67 95,-67 95,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">SumBackward0</text>\n",
       "</g>\n",
       "<!-- 129568406431264&#45;&gt;129571388155088 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>129568406431264&#45;&gt;129571388155088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-66.79C50.5,-60.07 50.5,-50.4 50.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-41.19 50.5,-31.19 47,-41.19 54,-41.19\"/>\n",
       "</g>\n",
       "<!-- 129568406431744 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>129568406431744</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"92,-141 9,-141 9,-122 92,-122 92,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MmBackward0</text>\n",
       "</g>\n",
       "<!-- 129568406431744&#45;&gt;129568406431264 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>129568406431744&#45;&gt;129568406431264</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-121.75C50.5,-114.8 50.5,-104.85 50.5,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-96.09 50.5,-86.09 47,-96.09 54,-96.09\"/>\n",
       "</g>\n",
       "<!-- 129568406431504 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>129568406431504</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-196 0,-196 0,-177 101,-177 101,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 129568406431504&#45;&gt;129568406431744 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>129568406431504&#45;&gt;129568406431744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.75C50.5,-169.8 50.5,-159.85 50.5,-151.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.09 50.5,-141.09 47,-151.09 54,-151.09\"/>\n",
       "</g>\n",
       "<!-- 129573394737296 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>129573394737296</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"80,-263 21,-263 21,-232 80,-232 80,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (2, 2)</text>\n",
       "</g>\n",
       "<!-- 129573394737296&#45;&gt;129568406431504 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>129573394737296&#45;&gt;129568406431504</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-231.92C50.5,-224.22 50.5,-214.69 50.5,-206.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-206.25 50.5,-196.25 47,-206.25 54,-206.25\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x75d77ed6e650>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c841be5c",
   "metadata": {},
   "source": [
    "## III. 计算图是如何执行的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257924ad",
   "metadata": {},
   "source": [
    "### III.1 call backward()/grad()会发生什么\n",
    "略\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c37657",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9047de82",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "884ddb51",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
