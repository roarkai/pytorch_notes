{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23724491",
   "metadata": {},
   "source": [
    "# Build model\n",
    "<font color=blue>[包含tutorial的Build model和developer note的Modules]</font>\n",
    "## 1. 什么是pytorch中的module,pytorch提供了哪些module类型？\n",
    "· module是构建神经网络的基础模块。pytorch提供了一个modules库，也支持自定义modules。用他们可以很容易地构建多层神经网络。具体实现来看，<font color=green>**namespace**</font> **torch.nn**提供了layers, containers和utilities三种主要的module类型，以及tensor类型的nn.Parameter作为modules parameter。\n",
    "1. <font color=lightblue>**Layers：**</font>NN通过layers对数据进行操作。pytorch用modules来表达这些layers,比如conv, affine, pooling, normalization, transformer和loss functions等\n",
    "2. <font color=lightblue>**containers：**</font>有3类container，nn.Module，nn.Sequential和holders of submodules。\\\n",
    "(1)**torch.nn.Module**。它是所有NN modules的base class，pytorch中所有的module都是**nn.Module**的子类\\\n",
    "(2)**torch.nn.Sequential**：以序列形式将1个或多个module顺序排列，体现了module的nestable\\\n",
    "(3)holders of submodules,其中：**nn.ModuleList，nn.ModuleDict**分别是以list和dictionary类型存储的module序列。**nn.ParamterList和nn.ParameterDict**分别是以list和dictionary形式存储的参数。\n",
    "3. <font color=lightblue>**utilities：**</font>把一些数据处理的函数以modules的形式表达。<font color=red>【具体待使用后描述？？？】</font>\n",
    "\n",
    "## 2. module的特点\n",
    "1. module和autograd system一起工作：modules使optimizer update参数非常方便。因为module能在autograd system的管理下自动完成requires_grad=True的tensor的梯度计算，optimizer可以在此基础上自动工作。\n",
    "2. pytorch中的module可以nest：每个神经网络模型自身都是一个module，该module又由其他modules(layers)构成。这种nest structure可以很方便的构造复杂的网络架构。<font color=red>【理解？？？】</font>\n",
    "3. **nn.Module**的子类会自动track参数，可以用两个method来查看：parameters()和named_parameters()\n",
    "4. 很容易与Transform配合使用：modules的save和restore都很直接，在CPU/GPU之间移动，做prune，quantize和其他很多操作都很方便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ea8140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.124032Z",
     "start_time": "2023-10-25T14:22:16.831308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn           # for torch.nn.Module\n",
    "import torch.nn.functional as F # for the activation function\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0e4f0",
   "metadata": {},
   "source": [
    "## 3. 定义一个NN\n",
    "1. 自定义model也得定义为**nn.Module**的子类，每个子类必须定义\\__init__和forward()两个method。\n",
    "2. 模型对input data的操作都放在forward()中。即，forward()用来指定要执行的computation，用的operation是nn.autograd.Function的子类的实例。这些子类可以是pytorch定义好的，也可以是自定义的。\n",
    "3. **用nn.Mudule来实例化module时，只implement forward() method不用implement backward() method**，因为：\\\n",
    "(1)<font color=blue>用nn.autograd.Function来实例化（自定义）Function时，要同时implement forward() and backward() methods</font> \\\n",
    "(2)<font color=blue>autograd system会用Function中的backward来自动处理module中用到的function的backward pass。</font>\n",
    "4. 如果module中要定义parameters，就要在\\__init\\__()中register。方式是在\\__init\\__()中将parameter定义为nn.Parameter的实例。此时，这些parameters就是parameters registered by the module。这也是autograd system运行需要的。\n",
    "5. Parameter class是torch.Tensor的子类，但他们可以被assigned as attributes of a Module。一旦实例化后，这些parameters就会被加到lists of the module's parameters，之后可以通过module.parameters()和model.namedparameters()来iterate throgh。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be88166",
   "metadata": {},
   "source": [
    "### 3.1 自定义一个简单的Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2951b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.129693Z",
     "start_time": "2023-10-25T14:22:18.126235Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):  # 必须是nn.Module的子类\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        # registering parameters: 参数定义成nn.Parameter的实例\n",
    "        # 此时autograd会自动tracking并让optimizer在迭代时update\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "  # implement forward() method\n",
    "    def forward(self, input):\n",
    "        return input @ self.weight + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63c55dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.147890Z",
     "start_time": "2023-10-25T14:22:18.130910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8722,  3.7005, -0.6856], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyLinear(4, 3)          \n",
    "sample_input = torch.randn(4)  \n",
    "\n",
    "# model is callable, calling invoke forward function\n",
    "model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e2b36d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.154605Z",
     "start_time": "2023-10-25T14:22:18.150087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.7241, -1.8295,  0.4082],\n",
      "        [-1.0282, -0.3029,  0.0207],\n",
      "        [ 0.1802,  0.3633, -0.2619],\n",
      "        [ 1.1791, -1.1295, -0.4416]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1510,  0.4955,  0.4743], requires_grad=True)\n",
      "\n",
      "\n",
      "('weight', Parameter containing:\n",
      "tensor([[-0.7241, -1.8295,  0.4082],\n",
      "        [-1.0282, -0.3029,  0.0207],\n",
      "        [ 0.1802,  0.3633, -0.2619],\n",
      "        [ 1.1791, -1.1295, -0.4416]], requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([-0.1510,  0.4955,  0.4743], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "## 遍历parameters()\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "## 遍历parameters named_parameters()\n",
    "#  这里weights和bias是parameter的name\n",
    "for parameter in model.named_parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f145b3d",
   "metadata": {},
   "source": [
    "### 3.2 将modules作为模型的基础模块(building blocks)\n",
    "· modules contain other modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297ab23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T09:10:48.862543Z",
     "start_time": "2023-10-20T09:10:48.859143Z"
    }
   },
   "source": [
    "#### i. 用nn.Sequential定义一个简单的module\n",
    "· Sequential会自动将上一层的输出传给下一层作为输入。<font color=red>但只在输入和输出都是单变量的情况时有效。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b22cb2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.160459Z",
     "start_time": "2023-10-25T14:22:18.155791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3434], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Sequential本身也是nn.Module的子类，所以实例化得到的也是module\n",
    "net = nn.Sequential(\n",
    "    MyLinear(4, 3),\n",
    "    nn.ReLU(),\n",
    "    MyLinear(3, 1)\n",
    ")\n",
    "\n",
    "simple_input = torch.randn(4)\n",
    "net(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87baec6",
   "metadata": {},
   "source": [
    "####  ii. 自定义module\n",
    "· 除了上面例子中非常简单的案例，通常都不会直接用Sequential来定义module，更多还是直接自定义module的方式.\\\n",
    "· 在__init__()中定义的submodule对应NN中的layer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9106542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.164925Z",
     "start_time": "2023-10-25T14:22:18.161831Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = MyLinear(4, 3)\n",
    "        self.layer1 = MyLinear(3, 1)  # 定义了两个submodule\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = F.relu(x)                 # relu不是submodule\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e718a358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.169452Z",
     "start_time": "2023-10-25T14:22:18.166265Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = MyLinear(4, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = MyLinear(3, 1)  # 定义了两个submodule\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.relu(x)                 # relu不是submodule\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f029af",
   "metadata": {},
   "source": [
    "**module的Immediate children可以用children() or named_children()来iterated through** \\\n",
    "上例中的children(也就是submodule)不包括rely层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ed2044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.173962Z",
     "start_time": "2023-10-25T14:22:18.170825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('layer0', MyLinear())\n",
      "('layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "for child in net.named_children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42b29f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.177711Z",
     "start_time": "2023-10-25T14:22:18.175669Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对比前面例子中直接用tensor operation定义的module\n",
    "# 此时module中没有child\n",
    "for child in model.children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0886481a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.183476Z",
     "start_time": "2023-10-25T14:22:18.180555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('layer0', MyLinear())\n",
      "('relu', ReLU())\n",
      "('layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "# 也可以把relu处理成module\n",
    "net2 = Net2()\n",
    "for child in net2.named_children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd1733",
   "metadata": {},
   "source": [
    "**modules() and named_modules() recursively iterate through a module and its child modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2152ffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.188882Z",
     "start_time": "2023-10-25T14:22:18.184860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "('', BigNet(\n",
      "  (l1): MyLinear()\n",
      "  (net): Net(\n",
      "    (layer0): MyLinear()\n",
      "    (layer1): MyLinear()\n",
      "  )\n",
      "))\n",
      "----------------------------------------------------\n",
      "('l1', MyLinear())\n",
      "----------------------------------------------------\n",
      "('net', Net(\n",
      "  (layer0): MyLinear()\n",
      "  (layer1): MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('net.layer0', MyLinear())\n",
      "----------------------------------------------------\n",
      "('net.layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "class BigNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = MyLinear(5, 4)\n",
    "        self.net = Net()\n",
    "    def forward(self, x):\n",
    "        return self.net(self.l1(x))\n",
    "\n",
    "big_net = BigNet()\n",
    "for module in big_net.named_modules():\n",
    "    print('-' * 52)\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0e3a1",
   "metadata": {},
   "source": [
    "#### iii. dynamically define submodule\n",
    "· 用ModuleList或者ModuleDict \\\n",
    "· calls to parameters() and named_parameters() will recursively include child parameters, allowing for convenient optimization of all parameters within the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "551de685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.194867Z",
     "start_time": "2023-10-25T14:22:18.190285Z"
    }
   },
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList(\n",
    "            [MyLinear(4, 4) for _ in range(num_layers)])\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU()\n",
    "        })\n",
    "        self.final = MyLinear(4, 1)\n",
    "        \n",
    "    def forward(self, x, act):\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "        x = self.activations[act](x)\n",
    "        # x = self.final(x)\n",
    "        return x\n",
    "\n",
    "dynamic_net = DynamicNet(3)\n",
    "sample_input = torch.randn(4)\n",
    "output = dynamic_net(sample_input, 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af90dd5",
   "metadata": {},
   "source": [
    "** · child module由__init__()中排列的module sequence决定，不由forward()实际执行的computation决定**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f75c3991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.199157Z",
     "start_time": "2023-10-25T14:22:18.196069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "('', DynamicNet(\n",
      "  (linears): ModuleList(\n",
      "    (0-2): 3 x MyLinear()\n",
      "  )\n",
      "  (activations): ModuleDict(\n",
      "    (relu): ReLU()\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (final): MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('linears', ModuleList(\n",
      "  (0-2): 3 x MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('linears.0', MyLinear())\n",
      "----------------------------------------------------\n",
      "('linears.1', MyLinear())\n",
      "----------------------------------------------------\n",
      "('linears.2', MyLinear())\n",
      "----------------------------------------------------\n",
      "('activations', ModuleDict(\n",
      "  (relu): ReLU()\n",
      "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "))\n",
      "----------------------------------------------------\n",
      "('activations.relu', ReLU())\n",
      "----------------------------------------------------\n",
      "('activations.lrelu', LeakyReLU(negative_slope=0.01))\n",
      "----------------------------------------------------\n",
      "('final', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "for module in dynamic_net.named_modules():\n",
    "    print('-'*52)\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f225eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.205538Z",
     "start_time": "2023-10-25T14:22:18.200506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "('linears.0.weight', Parameter containing:\n",
      "tensor([[-1.2449, -1.5810, -0.9684,  0.7997],\n",
      "        [-0.5384, -1.0668,  1.1007,  1.0570],\n",
      "        [-1.5931, -0.3755,  1.8301,  0.3028],\n",
      "        [ 2.0470,  1.0902,  1.1974,  1.2223]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.0.bias', Parameter containing:\n",
      "tensor([ 0.9569,  1.0514,  0.3401, -0.3270], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.1.weight', Parameter containing:\n",
      "tensor([[ 1.3122,  1.1817,  0.0877, -0.2661],\n",
      "        [ 0.4310,  0.0309, -0.5424, -0.6220],\n",
      "        [-0.7864, -1.7989, -1.6627, -0.0295],\n",
      "        [ 0.5377, -2.4508,  1.9957, -1.0140]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.1.bias', Parameter containing:\n",
      "tensor([-1.7527, -1.5700, -0.7644,  0.9201], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.2.weight', Parameter containing:\n",
      "tensor([[ 1.2662, -0.1218, -1.7088, -0.0435],\n",
      "        [ 0.1718, -2.0501,  0.9420, -0.9882],\n",
      "        [ 2.3130,  1.2494,  0.3295, -1.0240],\n",
      "        [-1.5375,  0.5149,  0.1883,  1.1739]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.2.bias', Parameter containing:\n",
      "tensor([ 0.5765,  0.7087,  1.4704, -1.5028], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('final.weight', Parameter containing:\n",
      "tensor([[-0.2520],\n",
      "        [-1.3106],\n",
      "        [-1.3819],\n",
      "        [ 0.8514]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('final.bias', Parameter containing:\n",
      "tensor([0.5726], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for parameter in dynamic_net.named_parameters():\n",
    "    print('-'*68)\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a485448",
   "metadata": {},
   "source": [
    "#### vi. 移动参数的设备，改变参数精度，用.to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3ebc9d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.535325Z",
     "start_time": "2023-10-25T14:22:18.206913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, 19.5688,  0.0000, 13.0718], device='cuda:0',\n",
       "       dtype=torch.float64, grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move all parameters to a CUDA device\n",
    "dynamic_net.to(device='cuda')\n",
    "\n",
    "# Change precision of all parameters\n",
    "dynamic_net.to(dtype=torch.float64)\n",
    "\n",
    "dynamic_net(torch.randn(4, device='cuda', dtype=torch.float64), 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555f4a6",
   "metadata": {},
   "source": [
    "#### v. module和submodule可以apply任意函数，包括自定义函数\n",
    "an arbitrary function can be applied to a module and its submodules recursively by using the apply() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56d8daee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:18.542052Z",
     "start_time": "2023-10-25T14:22:18.537123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicNet(\n",
       "  (linears): ModuleList(\n",
       "    (0-2): 3 x MyLinear()\n",
       "  )\n",
       "  (activations): ModuleDict(\n",
       "    (relu): ReLU()\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (final): MyLinear()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to initialize Linear weights.\n",
    "# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.\n",
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.fill_(0.0)\n",
    "\n",
    "# Apply the function recursively on the module and its submodules.\n",
    "dynamic_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f63538",
   "metadata": {},
   "source": [
    "## 4. 使用module训练NN\n",
    "**module有两种mode：trainning mode和evaluation mode**\n",
    "1. module默认处于training mode。用training()和eval()可以改变module所处mode。\n",
    "2. 如果module中有submodule在training mode和evaluation mode的时候输出不同，那么就应该在inference的时候将mode改为evaluation mode，比如batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85d6837a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.654861Z",
     "start_time": "2023-10-25T14:22:18.543261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer0): MyLinear()\n",
       "  (layer1): MyLinear()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新建network和optimizer\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, \n",
    "                            weight_decay=1e-2, momentum=0.9)\n",
    "\n",
    "# trainging the netword\n",
    "for _ in range(10000):\n",
    "    input = torch.randn(4)\n",
    "    output = net(input)\n",
    "    loss = torch.abs(output) # 用abs做loss，会让weights趋于0\n",
    "    \n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# training完成后，将module转到eval mode\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a99c86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.660023Z",
     "start_time": "2023-10-25T14:22:21.656267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.0018],\n",
      "        [0.7507],\n",
      "        [0.4740]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.layer1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31db1caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.671776Z",
     "start_time": "2023-10-25T14:22:21.661423Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training mode output: tensor([ 0.8559,  2.5287, -0.5101,  1.3260])\n",
      "evaluation mode output: tensor([-0.1441,  1.5287, -1.5101,  0.3260])\n"
     ]
    }
   ],
   "source": [
    "# 在training和evaluation mode下输出不同的例子\n",
    "class ModalModule(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.training:\n",
    "      # Add a constant only in training mode.\n",
    "      return x + 1.\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "m = ModalModule()\n",
    "x = torch.randn(4)\n",
    "print('training mode output: {}'.format(m(x)))\n",
    "\n",
    "m.eval()\n",
    "print('evaluation mode output: {}'.format(m(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47a97f",
   "metadata": {},
   "source": [
    "## 5. module state\n",
    "1. 如果要保存a trained model，可以存该module的state_dict，state_dict中保存了影响module运算的状态。state包括parameters和buffers。\\\n",
    "(1)**parameters**: learnable aspects of computation,存在state_dict中。\\\n",
    "(2)**buffers**: non-learnable aspects of computation. 有的module会存储参数之外的其他信息到state_dict，和参数不同的是，这些信息不需要learn，他们会被存在buffers中。\n",
    "2. 有两种buffers：Persistent buffers存在state_dict中，non-Persistent buffers不存在state_dict中。\\\n",
    "(1)Persistent buffers: 比如：serialized when saving and loading\n",
    "(2)non-Persistent buffers: 比如：left out of serialization\n",
    "3. Persistent buffers的特点：\\\n",
    "(1)如果state被存为state_dict的一部分，那么loading a serialized form of the module的时候，它就能被restore。 \\\n",
    "(2)这部分变量不会像parameters那样被optimizer处理，因而是non-leanable\n",
    "4. non-Persistent buffers的特点：\\\n",
    "不存为state_dict的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a3e8b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.678659Z",
     "start_time": "2023-10-25T14:22:21.673284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save the module\n",
    "torch.save(net.state_dict(), 'net.pt')\n",
    "\n",
    "## load the module\n",
    "#  1. 新建一个结构相同的module\n",
    "new_net = Net()\n",
    "#  2. load state\n",
    "new_net.load_state_dict(torch.load('net.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec5290bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.683748Z",
     "start_time": "2023-10-25T14:22:21.680054Z"
    }
   },
   "outputs": [],
   "source": [
    "## 使用buffers：module中要保存running mean\n",
    "#  将running mean的当前值存到state_dict用register_buffer()\n",
    "\n",
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('mean', torch.zeros(num_features))\n",
    "        # 此时，self.mean会被存到state_dict中\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 每次迭代时更新running mean的值\n",
    "        # 作为state_dict的一部分，当loading module的时候会被restore\n",
    "        self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n",
    "        return self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14a2fe3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.691794Z",
     "start_time": "2023-10-25T14:22:21.684888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))])\n",
      "tensor(True)\n",
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "m = RunningMean(4)\n",
    "for _ in range(10):\n",
    "    input = torch.randn(4)\n",
    "    m(input)\n",
    "\n",
    "print(m.state_dict())\n",
    "\n",
    "# Serialized form will contain the 'mean' tensor\n",
    "torch.save(m.state_dict(), 'mean.pt')\n",
    "\n",
    "m_loaded = RunningMean(4)\n",
    "m_loaded.load_state_dict(torch.load('mean.pt'))\n",
    "\n",
    "# 注意这里几种assert和print的差异\n",
    "assert(torch.all(m.mean == m_loaded.mean))\n",
    "print(torch.all(m.mean == m_loaded.mean))\n",
    "print(m.mean == m_loaded.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3629e24d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.699211Z",
     "start_time": "2023-10-25T14:22:21.693200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "## 将running mean存为non-Persistent buffers\n",
    "#  还是用register_buffer()，参数Persistent=False\n",
    "\n",
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('mean', torch.zeros(num_features), persistent=False)\n",
    "        # 此时，self.mean不会被存到state_dict中\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n",
    "        return self.mean\n",
    "\n",
    "torch.manual_seed(0)\n",
    "m2 = RunningMean(4)\n",
    "for _ in range(10):\n",
    "    input = torch.randn(4)\n",
    "    m2(input)\n",
    "\n",
    "print(m2.state_dict()) # 此时输出的state_dict是空的\n",
    "\n",
    "torch.save(m2.state_dict(), 'mean.pt')\n",
    "m2_loaded = RunningMean(4)\n",
    "m2_loaded.load_state_dict(torch.load('mean.pt'))\n",
    "print(torch.all(m2.mean == m2_loaded.mean)) # 输出False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6f7b2",
   "metadata": {},
   "source": [
    "#### 一个module的buffers可以用buffers()和named_buffers()来迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "082c83e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.703747Z",
     "start_time": "2023-10-25T14:22:21.700597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))\n"
     ]
    }
   ],
   "source": [
    "for buffer in m.named_buffers():\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20216c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.708190Z",
     "start_time": "2023-10-25T14:22:21.705065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))\n"
     ]
    }
   ],
   "source": [
    "for buffer in m2.named_buffers():\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f872834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:42:48.068015Z",
     "start_time": "2023-10-21T10:42:48.065554Z"
    }
   },
   "source": [
    "#### 两种buffers都受model-wide device/type changes所使用的.to() method影响\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84ffd956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.713265Z",
     "start_time": "2023-10-25T14:22:21.709593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunningMean()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.to(device='cuda', dtype=torch.float64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80cf5f4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.725726Z",
     "start_time": "2023-10-25T14:22:21.714654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('param1', tensor([-0.0404,  0.2881])), ('param2', tensor([-0.0075, -0.9145, -1.0886])), ('buffer1', tensor([ 1.3232,  0.0371, -0.2849, -0.1334])), ('param_list.0', tensor([-0.2666,  0.1894])), ('param_list.1', tensor([-0.2190,  2.0576])), ('param_list.2', tensor([-0.0354,  0.0627])), ('param_dict.bar', tensor([ 0.1753, -0.9315, -1.5055, -0.6610])), ('param_dict.foo', tensor([-0.7663,  1.0993,  2.7565])), ('linear.weight', tensor([[ 0.0197, -0.0610],\n",
      "        [ 0.1431,  0.4496],\n",
      "        [ 0.6698,  0.4491]])), ('linear.bias', tensor([ 0.6713, -0.0511, -0.6352]))])\n"
     ]
    }
   ],
   "source": [
    "## 一个综合例子\n",
    "class StatefulModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 用nn.Parameter实例化的参数会自动将tensor register为module parameter\n",
    "        self.param1 = nn.Parameter(torch.randn(2))\n",
    "\n",
    "        # 另一种将tensor register为module parameter的方式：用register_parameter() method\n",
    "        self.register_parameter('param2', nn.Parameter(torch.randn(3)))\n",
    "\n",
    "        # 将attribute： \"param3\" 定义为一个parameter，但不做初始化。\n",
    "        # 它的值'None'不会出现在state_dict中    \n",
    "        self.register_parameter('param3', None)\n",
    "\n",
    "        # Registers a list of parameters：没有name\n",
    "        self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])\n",
    "\n",
    "        # Registers a dictionary of parameters：有name\n",
    "        self.param_dict = nn.ParameterDict({\n",
    "            'foo': nn.Parameter(torch.randn(3)),\n",
    "            'bar': nn.Parameter(torch.randn(4))\n",
    "        })\n",
    "\n",
    "        # Registers a persistent buffer\n",
    "        self.register_buffer('buffer1', torch.randn(4), persistent=True)\n",
    "\n",
    "        # Registers a non-persistent buffer\n",
    "        self.register_buffer('buffer2', torch.randn(5), persistent=False)\n",
    "\n",
    "        # 将attribute：\"buffer3\" 定义为一个buffer，但不做初始化\n",
    "        # 它的值'None'也不会出现在state_dict中    \n",
    "        self.register_buffer('buffer3', None)\n",
    "\n",
    "        # 添加一个submodule就会将其parameters自动register为module的parameters\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "\n",
    "m = StatefulModule()\n",
    "\n",
    "# Save and load state_dict.\n",
    "torch.save(m.state_dict(), 'state.pt')\n",
    "m_loaded = StatefulModule()\n",
    "m_loaded.load_state_dict(torch.load('state.pt'))\n",
    "\n",
    "# state_dict中没有non-persistent buffer和reserved attributes \"param3\"与\"buffer3\"\n",
    "print(m_loaded.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405a4ab",
   "metadata": {},
   "source": [
    "## 6. module初始化\n",
    "1. 默认情况下，torch.nn提供的module中的parameter和浮点数buffer会在module实例化的时候初始化为存在CPU上的32位浮点数值。\n",
    "2. 如果要改变默认的初始化设置，可以在module实例化的时候设置对应的arguments或者直接用skip_init()method，之后自定义初始化方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62b1949b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.731877Z",
     "start_time": "2023-10-25T14:22:21.729298Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将module直接初始化到GPU上，参数类型为16位浮点数\n",
    "m = nn.Linear(5, 3, device='cuda', dtype=torch.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ae1ae7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.736762Z",
     "start_time": "2023-10-25T14:22:21.733257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# 除参数外，上述初始化方式也适用于floating-point buffers registered for the module\n",
    "m = nn.BatchNorm2d(3, dtype=torch.half)\n",
    "print(m.running_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69ec708b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.756522Z",
     "start_time": "2023-10-25T14:22:21.738252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3930, -0.1617,  0.4533, -0.0858,  0.7788],\n",
       "        [ 0.0826,  0.2477,  0.6222, -0.6544, -0.3411],\n",
       "        [ 0.1306,  0.3920,  0.5422,  0.7263, -0.0882]], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例：自定义参数初始化为正交矩阵\n",
    "m = torch.nn.utils.skip_init(nn.Linear, 5, 3)\n",
    "nn.init.orthogonal_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13a7e1",
   "metadata": {},
   "source": [
    "#### 自定义module的时候，建议按照torch.nn所遵守的规则那样：\n",
    "1. 提供一个device constructor kwarg，可以应用在任意的parameter和buffers registered by the module上\n",
    "2. 提供一个dtype constructor kwarg，可以应用在任意的parameter和floating-point buffers registered by the module上\n",
    "3. 只用初始化函数（比如：torch.nn.init package提供的函数）来初始化module constructor中的parameters和buffers。注意，此时要使用skip_init()。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337005f",
   "metadata": {},
   "source": [
    "## 7. torch自带module中的典型layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d35d19",
   "metadata": {},
   "source": [
    "### nn.Flatten\n",
    "1. 参数：torch.nn.Flatten(start_dim=1, end_dim=-1)\n",
    "2. 压缩[start_dim, end_dim]范围的dims\n",
    "2. 默认将输入的data压成2维数据，保留原第一维，压缩剩下的维度，比如输出(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d741eb78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.760931Z",
     "start_time": "2023-10-25T14:22:21.757771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 784])\n",
      "torch.Size([84, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "flatten2 = nn.Flatten(0, 1)  # 压缩[0, 1]范围的dims\n",
    "flat_image2 = flatten2(input_image)\n",
    "print(flat_image2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15422dc0",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "1. affine layer\n",
    "2. 参数：torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "   · in_features (int) – size of each input sample\n",
    "   · out_features (int) – size of each output sample\n",
    "   · bias (bool)取False时, 就不会learn bias. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2975059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.765475Z",
     "start_time": "2023-10-25T14:22:21.762200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=6)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107dfd54",
   "metadata": {},
   "source": [
    "### nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9664ae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.769810Z",
     "start_time": "2023-10-25T14:22:21.766811Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU:\n",
      " tensor([[-0.1059, -0.3650, -0.2354,  0.1382,  0.0520,  0.0531],\n",
      "        [-0.2357, -0.4797, -0.2849, -0.0148,  0.2071,  0.2770],\n",
      "        [-0.0447, -0.2520, -0.0628,  0.0251,  0.4561,  0.1185]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "After ReLU:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.1382, 0.0520, 0.0531],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2071, 0.2770],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0251, 0.4561, 0.1185]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU:\\n {hidden1}\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU:\\n {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c6442",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "1. an ordered container of modules.\n",
    "2. 数据会按照Sequential中定义的layer顺序做处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88abdeb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T14:22:21.773893Z",
     "start_time": "2023-10-25T14:22:21.770865Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "scores = seq_modules(input_image)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
