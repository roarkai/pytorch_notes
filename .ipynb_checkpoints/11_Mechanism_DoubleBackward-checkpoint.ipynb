{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1cc1eb",
   "metadata": {},
   "source": [
    "# Double backward with custom functions\n",
    "**Custom Function有两种方式影响grad mode**：\n",
    "1. 在forward pass执行过程中，autograd不会在Graph上记录其中执行的ops。forward pass结束后，自定义函数的backward methods会被记录为forward ops的各个outputs的grad_fn\n",
    "2. backward过程中，autograd才会记录用来计算backward pass的computation graph\n",
    "\n",
    "**save_for_backward如何与上述机制协作**：\n",
    "1. 如果autograd能记录backward中执行的computations，则函数自动支持double backward。 <font color=green>[double backward本质上就是对backward中的computation再做了一次backward。]</font>\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d426c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 例1：说明\n",
    "#  \n",
    "\n",
    "import torch\n",
    "\n",
    "class Square(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x**2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        x, = ctx.saved_tensors\n",
    "        return grad_out * 2 * x\n",
    "\n",
    "# finite differencing method会放大error，所以用双精度\n",
    "x = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\n",
    "torch.autograd.gradcheck(Square.apply, x)\n",
    "\n",
    "# 用gradcheck来验证second-order derivatives\n",
    "torch.autograd.gradgradcheck(Square.apply, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
