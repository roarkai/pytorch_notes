{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1151dcc0",
   "metadata": {},
   "source": [
    "# Manipulating tensor shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c18bb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.723631Z",
     "start_time": "2023-10-11T08:01:40.546764Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d86957",
   "metadata": {},
   "source": [
    "## 1. tensor的存储方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6945895",
   "metadata": {},
   "source": [
    "### 1.1 tensor.storage()\n",
    "输出tensor存在memory中的1D data sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fca0855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.733437Z",
     "start_time": "2023-10-11T08:01:41.725880Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4246/806302124.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 1\n",
       " 2\n",
       " 3\n",
       " 6\n",
       " 7\n",
       " 8\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [6, 7, 8]])\n",
    "x.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7cadaa",
   "metadata": {},
   "source": [
    "### 1.2 tensor.layout attribute\n",
    "1. 根据存储方式的不同，tensor有两种类型，dense tensor和sparse tensor，他们可以通过tensor.layout attribute来区分\\\n",
    "(1)**dense tensor** is stored linearly in a contiguous block of memory，也就是其存储占用了连续的memory block。\\\n",
    "(2)**sparse tensor**中大部分是0，只有很少比例的值是非零值，为了提高存储效率，pytorch为这类数据提供了特殊的存储方式。\\\n",
    "2. torch.layout attribute是表达tensor的memory layout特征的object。这个attribute有两种类型的取值：torch.stride和torch.sparse_coo，分别对应上面的两种tensor类型\\\n",
    "(1)如果<font color=blue>**layout=torch.strided**</font>，那么该tensor是dense tensor\\\n",
    "(2)如果<font color=blue>**layout=torch.sparse**</font>，那么该tensor是sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8a8811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.748054Z",
     "start_time": "2023-10-11T08:01:41.734768Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense tensor x:\n",
      " tensor([[0.4545, 0.6053, 0.2413],\n",
      "        [0.5411, 0.7210, 0.6547]], requires_grad=True)\n",
      "True\n",
      "sparse tensor y:\n",
      " tensor(indices=tensor([], size=(2, 0)),\n",
      "       values=tensor([], size=(0,)),\n",
      "       size=(100, 200), nnz=0, layout=torch.sparse_coo)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 新建一个dense tensor\n",
    "x = torch.rand((2, 3), layout=torch.strided, requires_grad=True)\n",
    "y = torch.zeros((100, 200), layout=torch.sparse_coo, requires_grad=False)\n",
    "print(\"dense tensor x:\\n\", x)\n",
    "print(x.is_contiguous())        # x是连续存储\n",
    "print(\"sparse tensor y:\\n\", y)\n",
    "print(y.is_contiguous())        # y不是连续存储"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20998a20",
   "metadata": {},
   "source": [
    "## 2. 改变dense tensor的view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495f825",
   "metadata": {},
   "source": [
    "### 2.1 dense tensor的contiguous特征\n",
    "1. dense tensor可能是contiguous tensor或者non-contiguous tensor。虽然他们在存储空间中都占用了连续的存储单元，但non-contiguous tensor的元素排列顺序与其元素在存储空间的排列顺序不一致。\\\n",
    "**· <font color=blue>tensor.view()</font>** 会保持contiguous tensor的contiguity。也只有contiguous tensor有view() method\\\n",
    "**· <font color=blue>tensor.transpose()</font>** 会改变tensor的contiguity。contiguous和non-contiguous tensor都可以transpose。当base tensor是dense tensor时，transpose不copy data，所以output tensor也只是原underlying data的一个new view。\n",
    "2. 用is_contiguous()可以查看是否contiguous\n",
    "3. 用tensor.contiguous()可以把non-contiguous tensor转变成contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403313af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.754320Z",
     "start_time": "2023-10-11T08:01:41.750432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 104839808\n",
      "False 104839808\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "base = torch.tensor([[0, 1],[2, 3]])\n",
    "print(base.is_contiguous(), base.storage().data_ptr())\n",
    "\n",
    "t = base.transpose(0, 1) \n",
    "print(t.is_contiguous(), t.storage().data_ptr())  # 同样的data\n",
    "print(base.is_contiguous())                       # contiguity变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fee84a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.761016Z",
     "start_time": "2023-10-11T08:01:41.755717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 9\n",
      " 10\n",
      " 11\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 12] \n",
      "\n",
      "x: \n",
      " tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "(6, 3, 1) True \n",
      "\n",
      "y: \n",
      " tensor([[[ 0,  1,  2],\n",
      "         [ 6,  7,  8]],\n",
      "\n",
      "        [[ 3,  4,  5],\n",
      "         [ 9, 10, 11]]])\n",
      "(3, 6, 1) False\n"
     ]
    }
   ],
   "source": [
    "## 用tensor.storage()可以获得该tensor在memory上的1D sequence\n",
    "x = torch.arange(0,12)\n",
    "print(x.storage(), '\\n')\n",
    "\n",
    "## view返回的tensor中，data sequence的排序和原数据相同，都是0, 1, 2, 3, 4...\n",
    "x = x.view(2,2,3)\n",
    "print('x: \\n', x)\n",
    "print(x.stride(), x.is_contiguous(), '\\n')\n",
    "\n",
    "## transpose返回的tensor中，data sequence的排序和原数据在memory中的排序不同\n",
    "#  y中元素的排序是：0, 1, 2, 6, 7...\n",
    "#  transpose不copy data，它仍然是原data的view，但是是non-contiguous ‘View’\n",
    "#  按新tensor的element顺序从memory中取值的时候无法连续获得这些值\n",
    "y = x.transpose(0,1)\n",
    "print('y: \\n', y)\n",
    "print(y.stride(), y.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592de342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.766640Z",
     "start_time": "2023-10-11T08:01:41.762520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## transpose将non-contiguous转变成contiguous\n",
    "w = y.transpose(0, 1)\n",
    "w.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c6c55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.771788Z",
     "start_time": "2023-10-11T08:01:41.767997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: \n",
      " tensor([[[ 0,  1,  2],\n",
      "         [ 6,  7,  8]],\n",
      "\n",
      "        [[ 3,  4,  5],\n",
      "         [ 9, 10, 11]]])\n",
      "(6, 3, 1) True\n",
      " 0\n",
      " 1\n",
      " 2\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 9\n",
      " 10\n",
      " 11\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 12]\n"
     ]
    }
   ],
   "source": [
    "## 用tensor.contiguous()转变成contiguous tensor\n",
    "#  此时发生了copy，按照tensor view的排序方式复制了一个新的data\n",
    "z = y.contiguous()\n",
    "print('z: \\n', z)\n",
    "print(z.stride(), z.is_contiguous())\n",
    "print(z.storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e777cf",
   "metadata": {},
   "source": [
    "### 2.2 tensor.view() method\n",
    "<font color=red>相当于numpy里面的reshape。</font>\n",
    "1. 只有contiguous dense tensor才有view method。\n",
    "2. dense tensor在计算机中以1D data sequence的形式存在congtiguous block of memory上。tensor.view()本质上是告诉计算机如何(根据其参数设置)stride over the 1D data sequence。所以，只提供了new view of the base tensor，不改变底层data。view的改变也就意味着tensor.stride()也会随之改变。\n",
    "3. tensor.view()返回的new tensor从memory上读取elements的顺序与其线性存储的顺序一致，只是改变了划分layer,column和row等unit单位的位置。\n",
    "4. 因为View tensor与它的base tensor的underlying data是同一个data，即tensor.view()返回new tensor，但并不copy data，所以，其中一个tensor元素值改变，另一个也变。\n",
    "\n",
    "**tensor.data_ptr()**: 返回1st element的存储地址，即data pointer指向的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b9588b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.778922Z",
     "start_time": "2023-10-11T08:01:41.773247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原data pointer: 104895232\n",
      "原id: 140235751104304\n",
      "原stride和size: (1,) torch.Size([15]) \n",
      "\n",
      "新data pointer与原值相同: 104895232\n",
      "id变化，说明新建了一个tensor: 140235751104464\n",
      "stride和size变化: (5, 1) torch.Size([3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tensor.view()返回的新tensor不copy data，但是会改变stride和shape(size)\n",
    "a = torch.arange(15)\n",
    "print('原data pointer:', a.data_ptr()) # \n",
    "print('原id:', id(a))\n",
    "print('原stride和size:', a.stride(), a.size(), '\\n')\n",
    "\n",
    "# change view\n",
    "b = a.view(3, 5)\n",
    "print('新data pointer与原值相同:', b.data_ptr())\n",
    "print('id变化，说明新建了一个tensor:', id(b))                 # id变化\n",
    "print('stride和size变化:', b.stride(), b.size()) \n",
    "\n",
    "# data地址不变\n",
    "x.untyped_storage().data_ptr() == y.untyped_storage().data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1765a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.787221Z",
     "start_time": "2023-10-11T08:01:41.780352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原shape: \n",
      " tensor([0.2335, 0.1410, 0.4894, 0.9493, 0.9713, 0.8237, 0.8002, 0.1175, 0.6101,\n",
      "        0.2457, 0.7504, 0.3586, 0.4528, 0.6221, 0.6928, 0.4970, 0.2736, 0.9745])\n",
      "torch.Size([18]) 140235751106784 \n",
      "\n",
      "用view不改变元素在memory中的排序: \n",
      " tensor([[[[0.2335, 0.1410, 0.4894],\n",
      "          [0.9493, 0.9713, 0.8237]],\n",
      "\n",
      "         [[0.8002, 0.1175, 0.6101],\n",
      "          [0.2457, 0.7504, 0.3586]],\n",
      "\n",
      "         [[0.4528, 0.6221, 0.6928],\n",
      "          [0.4970, 0.2736, 0.9745]]]])\n",
      "torch.Size([1, 3, 2, 3]) 140235771701952 \n",
      "\n",
      "用view不改变元素在memory中的排序: \n",
      " tensor([[0.2335, 0.1410, 0.4894, 0.9493, 0.9713, 0.8237],\n",
      "        [0.8002, 0.1175, 0.6101, 0.2457, 0.7504, 0.3586],\n",
      "        [0.4528, 0.6221, 0.6928, 0.4970, 0.2736, 0.9745]])\n",
      "torch.Size([3, 6]) 140235751104944 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tensor.view()不改变tensor layout in memory\n",
    "a = torch.rand(18)\n",
    "print(\"原shape:\", '\\n', a)\n",
    "print(a.size(), id(a), '\\n')\n",
    "\n",
    "b = a.view(1, 3, 2, 3)\n",
    "print(\"用view不改变元素在memory中的排序:\", '\\n', b)\n",
    "print(b.size(), id(b), '\\n')\n",
    "\n",
    "c = a.view(3, 6)  # 不改变tensor layout in memory\n",
    "print(\"用view不改变元素在memory中的排序:\", '\\n', c)\n",
    "print(c.size(), id(c), '\\n')\n",
    "\n",
    "# c和b只是shape相同，但是元素值不同\n",
    "torch.equal(b, c), c is a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f53313",
   "metadata": {},
   "source": [
    "### 2.3 tensor.stride() method\n",
    "1. dense tensor才有**tensor.stride()**。contiguous和non-contiguous tensor也都有stride。\n",
    "2. strided tensor的stride是一个int list,其第k-th元素的值表示tensor的k-th dimension上，一个元素到下个元素之间，用units of elements衡量的长度。\\\n",
    "· the k-th stride represents the jump in the memory necessary to go from one element to the next one in the k-th dimension of the Tensor.\n",
    "3. 每个strided tensor都有一个与之关联的torch.Storage存放data. 这些tensors 则提供strided view of a storage。一个torch.Storage可以对应不同的strided tensors。这些tensors有相同的data，不同的tensor.view()\\\n",
    "<font color=blue>**· Numpy strides()** returns (N bytes to Next Row, M bytes to Next Column)\\\n",
    "**· Pytorch stride()** returns (N elements to Next Row, M elements to Next Column).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ef9bd37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.793382Z",
     "start_time": "2023-10-11T08:01:41.789625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "5\n",
      "1\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5],\n",
    "                  [6, 7, 8, 9, 10]])\n",
    "\n",
    "print(x.stride())  # 不指定dim，A tuple of all strides is returned\n",
    "                   # 在tuple(5, 1)中，5在第0维，1在第1维对应traverse的维度\n",
    "print(x.stride(0)) # 指定dim=0，返回该维度上的stride值5\n",
    "                   # traverse along the 0th dim, 比如从'1'跳到'6',距离是5\n",
    "print(x.stride(-1))# 指定dim=1，返回该维度上的stride值1\n",
    "                   # traverse along the 1st dim, 比如从'7'跳到'8',距离是1\n",
    "\n",
    "## 改变view，stride也改变\n",
    "y = x.view(5, 2)\n",
    "print(y.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5eba6",
   "metadata": {},
   "source": [
    "### 2.4 tensor.transpose() method\n",
    "1. 和view method不同的是，transpose的output tensor与input tensor的shape不兼容。也就是改变了elements在view中的layout。如果把transpose之后的tensor排成一个vector，其顺序与input tensor不相同。\n",
    "2. 和view method相同的是，当tensor是dense tensor时，output只是input的一个view，不发生copy。改变其中一个的元素值，另一个也变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40630e22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.799055Z",
     "start_time": "2023-10-11T08:01:41.794833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原shape: \n",
      " tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11]],\n",
      "\n",
      "         [[12, 13, 14, 15],\n",
      "          [16, 17, 18, 19],\n",
      "          [20, 21, 22, 23]]]])\n",
      "torch.Size([1, 2, 3, 4]) 140235751105824 \n",
      "\n",
      "用transpose交换了2nd和3rd dim: \n",
      " tensor([[[[ 0,  1,  2,  3],\n",
      "          [12, 13, 14, 15]],\n",
      "\n",
      "         [[ 4,  5,  6,  7],\n",
      "          [16, 17, 18, 19]],\n",
      "\n",
      "         [[ 8,  9, 10, 11],\n",
      "          [20, 21, 22, 23]]]])\n",
      "torch.Size([1, 3, 2, 4]) 140239248608928 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(1, 2, 3, 4)\n",
    "print(\"原shape:\", '\\n', a)\n",
    "print(a.size(), id(a), '\\n')\n",
    "\n",
    "b = a.transpose(1, 2)\n",
    "print(\"用transpose交换了2nd和3rd dim:\", '\\n', b)\n",
    "print(b.size(), id(b), '\\n')\n",
    "\n",
    "# b.view(24) # 错，b这时不是contiguous，不能用view method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b0d79",
   "metadata": {},
   "source": [
    "### 2.5 squeeze and unsqueeze\n",
    "· squeeze和unsqueeze只改变原数据的view，新生成的tensor与input tensor指向相同的memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d83887",
   "metadata": {},
   "source": [
    "**2.5.1 unsqueeze**\n",
    "1. pytorch默认是按照batch处理数据，以image为例，默认的input大小是(N, C, H, W)。如果想让model处理单个数据，就要把当个样本的大小从(C, H, W)改成(1, C, H, W)\n",
    "2. 常用于ease broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "114a298f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.804229Z",
     "start_time": "2023-10-11T08:01:41.800428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "增加一个长度为1的维度到新的第0维： torch.Size([1, 3, 226, 226])\n",
      "增加一个长度为1的维度到新的第1维： torch.Size([3, 1, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "# 用tensor.unsqueeze()来增加长度为1的新维度\n",
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0) # 在第0维增加一个维度\n",
    "print('增加一个长度为1的维度到新的第0维：',b.shape)\n",
    "\n",
    "c = a.unsqueeze(1)\n",
    "print('增加一个长度为1的维度到新的第1维：',c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73732b73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.809556Z",
     "start_time": "2023-10-11T08:01:41.805645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[[0.9940, 0.9940],\n",
      "         [0.6353, 0.6353],\n",
      "         [0.0612, 0.0612]],\n",
      "\n",
      "        [[0.9940, 0.9940],\n",
      "         [0.6353, 0.6353],\n",
      "         [0.0612, 0.0612]],\n",
      "\n",
      "        [[0.9940, 0.9940],\n",
      "         [0.6353, 0.6353],\n",
      "         [0.0612, 0.0612]],\n",
      "\n",
      "        [[0.9940, 0.9940],\n",
      "         [0.6353, 0.6353],\n",
      "         [0.0612, 0.0612]]])\n"
     ]
    }
   ],
   "source": [
    "# unsqueeze常用于方便broadcast\n",
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(   3)     # a * b不能直接运算\n",
    "c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\n",
    "print(c.shape)\n",
    "print(a * c)             # broadcasting works again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a156d9d",
   "metadata": {},
   "source": [
    "**2.5.2 squeeze**\n",
    "1. 用tensor.squeeze()来压缩长度为1的维度(dimensions of extent 1)\n",
    "2. 最好指定dim number，因为pytorch按batch处理数据，squeeze()会把第1维N压掉，导致模型出错 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80fd7f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.815282Z",
     "start_time": "2023-10-11T08:01:41.810989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 1, 4])\n",
      "tensor([[[[2., 2., 2., 2.]],\n",
      "\n",
      "         [[2., 2., 2., 2.]]]])\n",
      " 2.0\n",
      " 2.0\n",
      " 2.0\n",
      " 2.0\n",
      " 2.0\n",
      " 2.0\n",
      " 2.0\n",
      " 2.0\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 8]\n",
      "(8, 4, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(1, 2, 1, 4)\n",
    "b = a.squeeze()   # 压缩所有维度为1的dims\n",
    "c = a.squeeze(0)  # 压缩第0维\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "\n",
    "## 改变b之后，a的值也变\n",
    "b += 1\n",
    "print(a)\n",
    "print(a.storage())\n",
    "print(a.stride())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c64d9c09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.819733Z",
     "start_time": "2023-10-11T08:01:41.816641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1) (1, 5)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
    "print(x.stride(), x.T.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d20a70",
   "metadata": {},
   "source": [
    "## 3. reshape工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574cf63f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T04:24:54.337209Z",
     "start_time": "2023-10-11T04:24:54.330499Z"
    }
   },
   "source": [
    "### 3.1 只改变view，不copy data的reshape工具\n",
    "1. tensor.view\n",
    "2. tensor.transpose\n",
    "3. tensor.squeeze and tensor.unsqueeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dee657",
   "metadata": {},
   "source": [
    "### 3.2 tensor.reshape()\n",
    "1. 通常情况下，reshape返回的tensor只是原tensor的一个view，两者指向的是相同的memory location。但实际使用的时候，不要依赖这里的view vs. copy关系，不然容易出错。\\\n",
    "2. reshape实际上是打包了两种处理方式的method，当reshape的input tensor和outpu tensor的shape是compatible的时候，它调用的是tensor.view()，此时不copy data，当两者的shape不兼容的时候，它调用tensor.contiguous()，这时候就会copy data。\\\n",
    "<font color=blue>· 可以理解成，处理contiguous tensor时，调用view();处理non-contiguous tensor时，调用contiguous(). </font>\n",
    "3. pyrotch要求'shape'参数必须是tuple of ints。但是当shape是第一个参数的时候，可以用series if integers或者单个integer作为shape的值，但如果shape不是第一个参数，就必须是tuple。\\\n",
    "· 注：在extent of dim为1时，要注意，x=(3)是int，x=(3,)是tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "693c59dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.825664Z",
     "start_time": "2023-10-11T08:01:41.821104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> <class 'int'> <class 'tuple'>\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) torch.Size([12])\n",
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "## tensor.reshape()的shape参数：x=(3)是int，不能用，x=(3,)是tuple\n",
    "x = (1, 2, 3)\n",
    "y = (3)\n",
    "z = (2,)\n",
    "print(type(x), type(y), type(z))\n",
    "\n",
    "## 用tensor.reshape,这时shape参数是第一个参数\n",
    "output3d = torch.ones(2, 2, 3)\n",
    "\n",
    "input1d = output3d.reshape(12) # 12 = 2 * 2 * 3\n",
    "print(input1d, input1d.shape)\n",
    "\n",
    "## 用torch module的reshape method，这时shape参数不是第一个参数\n",
    "# input1d_2 = torch.reshape(output3d, (12)) # 错,(12)被识别为int12\n",
    "input1d_2 = torch.reshape(output3d, (12,))  # 用(12,)才是tuple\n",
    "print(input1d_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c7bde51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.831443Z",
     "start_time": "2023-10-11T08:01:41.827038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]) True\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "True\n",
      "tensor([  1,   2, 100,   4,   5,   6,   7,   8,   9,  10,  11,  12]) \n",
      "\n",
      "tensor([[  1,   2, 100],\n",
      "        [  4,   5,   6],\n",
      "        [  7,   8,   9],\n",
      "        [ 10,  11,  12]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 处理contiguous tensor\n",
    "x = torch.arange(1,13)\n",
    "print(x, x.is_contiguous())\n",
    "\n",
    "y = x.reshape(4,3)\n",
    "print(y)\n",
    "print(x.storage().data_ptr() == y.storage().data_ptr())\n",
    "\n",
    "# 改变x中元素值, y也变\n",
    "x[2] = 100\n",
    "print(x, '\\n')\n",
    "print(y, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e5452e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T08:01:41.837281Z",
     "start_time": "2023-10-11T08:01:41.832812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  3,  5,  7,  9, 11],\n",
      "        [ 2,  4,  6,  8, 10, 12]]) False\n",
      "tensor([[ 1,  3,  5],\n",
      "        [ 7,  9, 11],\n",
      "        [ 2,  4,  6],\n",
      "        [ 8, 10, 12]])\n",
      "False\n",
      "tensor([[  1,   3,   5,   7,   9,  11],\n",
      "        [100, 100, 100, 100, 100, 100]]) \n",
      "\n",
      "tensor([[ 1,  3,  5],\n",
      "        [ 7,  9, 11],\n",
      "        [ 2,  4,  6],\n",
      "        [ 8, 10, 12]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 处理non-contiguous tensor\n",
    "x = torch.arange(1,13).view(6,2).transpose(0,1)\n",
    "print(x, x.is_contiguous())\n",
    "\n",
    "y = x.reshape(4,3)\n",
    "print(y)\n",
    "print(x.storage().data_ptr() == y.storage().data_ptr())\n",
    "\n",
    "# 改变x中元素值, y不变\n",
    "x[1] = 100\n",
    "print(x, '\\n')\n",
    "print(y, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
