{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23724491",
   "metadata": {},
   "source": [
    "# Build model\n",
    "<font color=blue>[包含tutorial的Build model和developer note的Modules]</font>\n",
    "## 1. 什么是module，pytorch有哪些module类型？\n",
    " - module是构建神经网络的基础模块，pytorch用module来表达神经网络。\n",
    " - pytorch提供了一个modules库，也支持自定义modules。用他们可以很容易地构建多层神经网络。\n",
    "   - 具体实现来看，<font color=green>**namespace**</font> **torch.nn**提供了layers, containers和utilities三种主要的module类型，并使用**tensor类型的nn.Parameter**作为modules parameter。\n",
    "1. <font color=blue>**Layers：**</font>NN通过layers对数据进行操作。pytorch用modules来表达这些layers,比如conv, affine, pooling, normalization, transformer和loss functions等\n",
    "2. <font color=blue>**containers：**</font>有3类container，nn.Module，nn.Sequential和holders of submodules。\\\n",
    "(1)**torch.nn.Module**。它是所有NN modules的base class，pytorch中所有的module都是**nn.Module**的子类\\\n",
    "(2)**torch.nn.Sequential**：以序列形式将1个或多个module顺序排列，体现了module的nestable\\\n",
    "(3)holders of submodules,其中：**nn.ModuleList，nn.ModuleDict**分别是以list和dictionary类型存储的module序列。**nn.ParamterList和nn.ParameterDict**分别是以list和dictionary形式存储的参数。\n",
    "3. <font color=blue>**utilities：**</font>把一些数据处理的函数以modules的形式表达。\n",
    "\n",
    "## 2. module的特点\n",
    "- module和autograd system一起工作：modules使optimizer update参数非常方便。因为module能在autograd system的管理下自动完成requires_grad=True的tensor的梯度计算，optimizer可以在此基础上自动工作。\n",
    "- pytorch中的module可以nest：每个神经网络模型自身都是一个module，该module又由其他modules(layers)构成。这种nest structure可以很方便的构造复杂的网络架构。\n",
    "- **nn.Module**的子类会自动track参数，可以用两个method来查看：parameters()和named_parameters()\n",
    "   - <font color=orange>module和autograd.Functional相比，可以自动维护module.parameters()中的参数。</font>\n",
    "- 好用，而且也容易对module做各种调整：modules的save和restore都很直接，在CPU/GPU之间移动，做prune，quantize和其他很多操作都很方便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ea8140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.542695Z",
     "start_time": "2024-08-06T02:37:16.615207Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roark/anaconda3/envs/231n/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/roark/anaconda3/envs/231n/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn           # for torch.nn.Module\n",
    "import torch.nn.functional as F # for the activation function\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0e4f0",
   "metadata": {},
   "source": [
    "## 3. 自定义module实现不同的功能\n",
    "- 自定义model要定义为**nn.Module**的子类，每个子类必须定义<font color=blue>**\\__init\\__()和forward()**</font>两个method。\n",
    "  - module通过他们package <font color=blue>state and computation</font> together。\n",
    "    - 模型对input data的操作都放在forward() method中。forward()用来指定要执行的computation，用的operation是nn.autograd.Function的子类的实例。这些子类可以是pytorch定义好的，也可以是自定义的。\n",
    "    - \\__init__()中要对computation涉及的参数做register，登记后参数就会成为state的一部分，作为<font color=brown>learnable aspects of module's computation</font>。autograd system工作时就能track他们。\n",
    "      - register的方法是在\\__init\\__()中将parameter定义为nn.Parameter的实例。\n",
    "- module本身是callable的，call module会invoke forward function。\n",
    "- Parameter class是torch.Tensor的子类，但他们可以被assigned as attributes of a Module。一旦实例化后，这些parameters就会被加到lists of the module's parameters。\n",
    "  - 可以通过module.parameters()和model.namedparameters()来iterate throgh。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67326e23",
   "metadata": {},
   "source": [
    "- <font color=green>**用nn.Mudule来实例化module时，只implement forward() method不用implement backward() method**。</font>因为：\n",
    "   - <font color=blue>用nn.autograd.Function来实例化（自定义）Function时，要同时implement forward() and backward() methods</font> \n",
    "   - <font color=blue>autograd system会用Function中的backward来自动处理module中用到的function的backward pass。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be88166",
   "metadata": {},
   "source": [
    "### 3.1 自定义一个简单的layer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2951b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.549171Z",
     "start_time": "2024-08-06T02:37:18.545305Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):  # 必须是nn.Module的子类\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        # registering parameters: 参数定义成nn.Parameter的实例\n",
    "        # 此时autograd会自动tracking并让optimizer在迭代时update\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "  # implement forward() method\n",
    "    def forward(self, input):\n",
    "        return input @ self.weight + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63c55dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.568767Z",
     "start_time": "2024-08-06T02:37:18.551033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8378,  0.2388, -0.7407], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyLinear(4, 3)          \n",
    "sample_input = torch.randn(4)  \n",
    "\n",
    "# model is callable, calling invoke forward function\n",
    "model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e2b36d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.576607Z",
     "start_time": "2024-08-06T02:37:18.571379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.9021,  0.1307,  0.9263],\n",
      "        [ 0.1946, -0.4758, -0.5258],\n",
      "        [-1.1283,  2.2938, -1.0398],\n",
      "        [-0.2965,  0.2735,  2.0126]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2904, -0.5984, -0.4593], requires_grad=True)\n",
      "\n",
      "\n",
      "('weight', Parameter containing:\n",
      "tensor([[-0.9021,  0.1307,  0.9263],\n",
      "        [ 0.1946, -0.4758, -0.5258],\n",
      "        [-1.1283,  2.2938, -1.0398],\n",
      "        [-0.2965,  0.2735,  2.0126]], requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([ 0.2904, -0.5984, -0.4593], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "## 遍历parameters()\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "## 遍历parameters named_parameters()\n",
    "#  这里weights和bias是parameter的name\n",
    "for parameter in model.named_parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f145b3d",
   "metadata": {},
   "source": [
    "### 3.2 将modules作为network的基础模块(building blocks)\n",
    "- modules contain other modules让他们成为实现复杂功能的building blocks。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297ab23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T09:10:48.862543Z",
     "start_time": "2023-10-20T09:10:48.859143Z"
    }
   },
   "source": [
    "#### 3.2.1 用nn.Sequential来chain together多个layer modules\n",
    "- nn.Sequential会自动将上一层的输出传给下一层作为输入。<font color=red>但只在输入和输出都是单变量的情况时有效。</font>\n",
    "- 除了下面例子中非常简单的案例，不建议直接用Sequential来定义module，因为sequential中用submodule的方式单一。<font color=green>最好自定义module，这样module中使用submodule来做computation的方式可以非常灵活。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b22cb2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.583494Z",
     "start_time": "2024-08-06T02:37:18.578729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6825], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Sequential本身也是nn.Module的子类，所以实例化得到的也是module\n",
    "net = nn.Sequential(\n",
    "    MyLinear(4, 3),\n",
    "    nn.ReLU(),\n",
    "    MyLinear(3, 1)\n",
    ")\n",
    "\n",
    "simple_input = torch.randn(4)\n",
    "net(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87baec6",
   "metadata": {},
   "source": [
    "####  3.2.2 自定义module来搭建network：只用layer module\n",
    "- 在__init__()中定义的submodule通常用torch.nn中或者自定义的layer module，对应神经网络中的layer。\n",
    "  - **对比这里和前面用module类自定义layer module**：\n",
    "    - 在用module自定义layer module时__init__()中定义的是forward computation中使用的parameters。此时，这些parameters被registered，从而被autograd system track到\n",
    "    - 这里的__init__()中定义的是forward computation中使用的有parameters的 modules，这些modules中的parameters也会被registered。并且如果module不是简单的layer module，而是有submodule的话，submodule中的参数全都会被registered。\n",
    "- 在__init__()中定义的submodule也可以用nn.ModuleList,nn.ModuleDict，他们可以实现动态定义submodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9106542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.588086Z",
     "start_time": "2024-08-06T02:37:18.584688Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = MyLinear(4, 3)\n",
    "        self.layer1 = MyLinear(3, 1)  # 定义了两个submodule\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = F.relu(x)                 # relu不是submodule，是function\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e718a358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.592841Z",
     "start_time": "2024-08-06T02:37:18.589247Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = MyLinear(4, 3)\n",
    "        self.relu = nn.ReLU()         # 用的nn中的ReLU module\n",
    "        self.layer1 = MyLinear(3, 1)  # 定义了3个submodule\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.relu(x)              # 这里relu是submodule\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "# 可以这么处理，但不必要，因为ReLU没有参数，没必要用module，就function就好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f029af",
   "metadata": {},
   "source": [
    "- **可以用children() or named_children()来iterate遍历module的immediate children**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ed2044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.597477Z",
     "start_time": "2024-08-06T02:37:18.594133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('layer0', MyLinear())\n",
      "('layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "for child in net.named_children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42b29f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.601429Z",
     "start_time": "2024-08-06T02:37:18.598800Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对比前面例子中直接用tensor operation定义的module\n",
    "# 此时module中没有child\n",
    "for child in model.children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0886481a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.609320Z",
     "start_time": "2024-08-06T02:37:18.605649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('layer0', MyLinear())\n",
      "('relu', ReLU())\n",
      "('layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "# 也可以把relu处理成module\n",
    "net2 = Net2()\n",
    "for child in net2.named_children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd1733",
   "metadata": {},
   "source": [
    "- 如果不止想查看immediate children，想深入看所有nested的module，就用**modules() and named_modules() recursively iterate through a module and its child modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2152ffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.614935Z",
     "start_time": "2024-08-06T02:37:18.611018Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BigNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = MyLinear(5, 4)\n",
    "        self.net = Net()\n",
    "    def forward(self, x):\n",
    "        return self.net(self.l1(x))\n",
    "\n",
    "big_net = BigNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c6d8ac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.619802Z",
     "start_time": "2024-08-06T02:37:18.616506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLinear()\n",
      "Net(\n",
      "  (layer0): MyLinear()\n",
      "  (layer1): MyLinear()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 只看immediate children\n",
    "for child in big_net.children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55fab76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.624982Z",
     "start_time": "2024-08-06T02:37:18.621296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "('', BigNet(\n",
      "  (l1): MyLinear()\n",
      "  (net): Net(\n",
      "    (layer0): MyLinear()\n",
      "    (layer1): MyLinear()\n",
      "  )\n",
      "))\n",
      "----------------------------------------------------\n",
      "('l1', MyLinear())\n",
      "----------------------------------------------------\n",
      "('net', Net(\n",
      "  (layer0): MyLinear()\n",
      "  (layer1): MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('net.layer0', MyLinear())\n",
      "----------------------------------------------------\n",
      "('net.layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "# 看所有nested children\n",
    "for module in big_net.named_modules():\n",
    "    print('-' * 52)\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0e3a1",
   "metadata": {},
   "source": [
    "####  3.2.3 自定义module来搭建network：用ModuleList/ModuleDict来dynamically define submodule\n",
    "- 用ModuleList或者ModuleDict在自定义module的__init__()中定义submodule\n",
    "  - ModuleList和ModuleDict都会自动register list/Dict中的submodule。实际上登记的是submodule中的参数。此时，calls to parameters() and named_parameters() will recursively include child parameters, 使得对network中所有parameters做optimization非常方便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551de685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.632562Z",
     "start_time": "2024-08-06T02:37:18.626540Z"
    }
   },
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList(\n",
    "            [MyLinear(4, 4) for _ in range(num_layers)])\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU()\n",
    "        })\n",
    "        self.final = MyLinear(4, 1)\n",
    "        \n",
    "    def forward(self, x, act):\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "        x = self.activations[act](x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "dynamic_net = DynamicNet(3)\n",
    "sample_input = torch.randn(4)\n",
    "output = dynamic_net(sample_input, 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af90dd5",
   "metadata": {},
   "source": [
    "- **child module由__init__()中排列的module sequence决定，不由forward()实际执行的computation决定**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f75c3991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.637685Z",
     "start_time": "2024-08-06T02:37:18.634171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "('', DynamicNet(\n",
      "  (linears): ModuleList(\n",
      "    (0-2): 3 x MyLinear()\n",
      "  )\n",
      "  (activations): ModuleDict(\n",
      "    (relu): ReLU()\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (final): MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('linears', ModuleList(\n",
      "  (0-2): 3 x MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('linears.0', MyLinear())\n",
      "----------------------------------------------------\n",
      "('linears.1', MyLinear())\n",
      "----------------------------------------------------\n",
      "('linears.2', MyLinear())\n",
      "----------------------------------------------------\n",
      "('activations', ModuleDict(\n",
      "  (relu): ReLU()\n",
      "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "))\n",
      "----------------------------------------------------\n",
      "('activations.relu', ReLU())\n",
      "----------------------------------------------------\n",
      "('activations.lrelu', LeakyReLU(negative_slope=0.01))\n",
      "----------------------------------------------------\n",
      "('final', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "for module in dynamic_net.named_modules():\n",
    "    print('-'*52)\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f225eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.646372Z",
     "start_time": "2024-08-06T02:37:18.639629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "('linears.0.weight', Parameter containing:\n",
      "tensor([[ 0.9202, -0.0112,  0.8948,  0.7689],\n",
      "        [ 0.7405,  0.5361, -0.4328,  0.6393],\n",
      "        [ 1.0142, -0.5760,  0.4918, -0.0412],\n",
      "        [-1.5591, -1.2559,  0.5378,  1.1736]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.0.bias', Parameter containing:\n",
      "tensor([ 0.8008,  0.1709, -0.2768,  0.0088], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.1.weight', Parameter containing:\n",
      "tensor([[-0.0610,  0.3081, -0.9926,  0.6799],\n",
      "        [ 1.8385, -0.1458, -0.6596, -1.3792],\n",
      "        [ 0.3271, -2.7487,  0.6659,  0.4090],\n",
      "        [ 0.6527,  0.2495, -0.2761,  0.3969]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.1.bias', Parameter containing:\n",
      "tensor([-1.1170,  1.1378,  1.9007, -1.4661], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.2.weight', Parameter containing:\n",
      "tensor([[ 0.6457, -0.0395,  0.0904, -1.7996],\n",
      "        [-0.0123,  0.1354, -1.3541, -3.0605],\n",
      "        [-1.0791, -0.2295, -0.7768, -0.7968],\n",
      "        [-0.1592,  0.1466, -1.0527, -0.5124]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.2.bias', Parameter containing:\n",
      "tensor([ 0.5133, -0.0798, -1.0320,  0.2408], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('final.weight', Parameter containing:\n",
      "tensor([[ 0.3192],\n",
      "        [-0.5992],\n",
      "        [-1.7811],\n",
      "        [ 1.0200]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('final.bias', Parameter containing:\n",
      "tensor([0.4188], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for parameter in dynamic_net.named_parameters():\n",
    "    print('-'*68)\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a485448",
   "metadata": {},
   "source": [
    "#### 3.2.4 移动参数的设备，改变参数精度，用.to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3ebc9d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.980798Z",
     "start_time": "2024-08-06T02:37:18.647857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5520], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将module移动到cuda\n",
    "dynamic_net.to(device='cuda')\n",
    "\n",
    "# 改变数据类型\n",
    "dynamic_net.to(dtype=torch.float64)\n",
    "\n",
    "dynamic_net(torch.randn(4, device='cuda', dtype=torch.float64), 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555f4a6",
   "metadata": {},
   "source": [
    "#### 3.2.5 module和submodule可以apply任意函数，包括自定义函数\n",
    "- module.apply(func)可以将任意函数recursively apply到module及其submodule上\n",
    "- <font color=green>此时要注意判断func是否需要参与梯度计算，不需要的话要设置no_grad属性</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56d8daee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:18.987233Z",
     "start_time": "2024-08-06T02:37:18.982293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicNet(\n",
       "  (linears): ModuleList(\n",
       "    (0-2): 3 x MyLinear()\n",
       "  )\n",
       "  (activations): ModuleDict(\n",
       "    (relu): ReLU()\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (final): MyLinear()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 定义一个初始化整个module的Linear weights的函数\n",
    "# no_grad()用来avoid tracking this computation in the autograd graph.\n",
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight) # init操作用in-place节省空间\n",
    "        m.bias.fill_(0.0)\n",
    "\n",
    "# Apply the function recursively on the module and its submodules.\n",
    "dynamic_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f63538",
   "metadata": {},
   "source": [
    "## 4. 使用module训练NN\n",
    "**module有两种mode：trainning mode和evaluation mode**\n",
    "1. module默认处于training mode。用training()和eval()可以改变module所处mode。\n",
    "2. 如果module中有submodule在training mode和evaluation mode的时候输出不同，那么就应该在inference的时候将mode改为evaluation mode，比如batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85d6837a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:22.640072Z",
     "start_time": "2024-08-06T02:37:18.988795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer0): MyLinear()\n",
       "  (layer1): MyLinear()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新建network和optimizer\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, \n",
    "                            weight_decay=1e-2, momentum=0.9)\n",
    "\n",
    "# trainging the netword\n",
    "for _ in range(10000):\n",
    "    input = torch.randn(4)\n",
    "    output = net(input)\n",
    "    loss = torch.abs(output) # 用abs做loss，会让weights趋于0\n",
    "    \n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# training完成后，将module转到eval mode\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a99c86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:22.648041Z",
     "start_time": "2024-08-06T02:37:22.642562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0007],\n",
      "        [-0.0004],\n",
      "        [ 0.0044]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.layer1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31db1caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:22.661887Z",
     "start_time": "2024-08-06T02:37:22.649398Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training mode output: tensor([ 0.7025,  1.9967,  0.2140, -0.5901])\n",
      "evaluation mode output: tensor([-0.2975,  0.9967, -0.7860, -1.5901])\n"
     ]
    }
   ],
   "source": [
    "# 在training和evaluation mode下输出不同的例子\n",
    "class ModalModule(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.training:\n",
    "      # Add a constant only in training mode.\n",
    "      return x + 1.\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "m = ModalModule()\n",
    "x = torch.randn(4)\n",
    "print('training mode output: {}'.format(m(x)))\n",
    "\n",
    "m.eval()\n",
    "print('evaluation mode output: {}'.format(m(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405a4ab",
   "metadata": {},
   "source": [
    "## 5. module初始化\n",
    "- **建议方式：先skip_init()method，之后自定义初始化方式。**\n",
    "- **不建议使用**：\n",
    "  - (1) 默认初始化，torch.nn提供的module中的parameter和浮点数buffer会在module实例化的时候初始化为存在CPU上的32位浮点数值。\n",
    "  - (2) 在module实例化的时候设置对应的arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62b1949b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:22.727668Z",
     "start_time": "2024-08-06T02:37:22.724804Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将module直接初始化到GPU上，参数类型为16位浮点数\n",
    "m = nn.Linear(5, 3, device='cuda', dtype=torch.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ae1ae7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:22.732702Z",
     "start_time": "2024-08-06T02:37:22.728866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# 除参数外，上述初始化方式也适用于floating-point buffers registered for the module\n",
    "m = nn.BatchNorm2d(3, dtype=torch.half)\n",
    "print(m.running_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69ec708b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:22.740829Z",
     "start_time": "2024-08-06T02:37:22.734485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3930, -0.1617,  0.4533, -0.0858,  0.7788],\n",
       "        [ 0.0826,  0.2477,  0.6222, -0.6544, -0.3411],\n",
       "        [ 0.1306,  0.3920,  0.5422,  0.7263, -0.0882]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例：自定义参数初始化为正交矩阵\n",
    "m = torch.nn.utils.skip_init(nn.Linear, 5, 3)\n",
    "nn.init.orthogonal_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b983495",
   "metadata": {},
   "source": [
    "- <font color=blue>**使用skip_init() method要满足两个条件：**</font>\n",
    "  1. module的constructor中必须有device kwarg，他要传给constructor中所有的parameters和buffers\n",
    "  2. 除了nn.init中的初始化操作之外，parameters和buffers在constructor中不能参与任何其他computation。\n",
    "  - 另外还有一个不是必须，单建议遵守的规则是：module的constructor中加上dtype  kwarg，传给constructor中定义的所有parameter和floating-point buffers 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be100f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "  def __init__(self, foo, bar, device=None):\n",
    "    super().__init__()\n",
    "\n",
    "    # ==== Case 1: Module直接创建parameters. ====\n",
    "    # 必须把device参数传给所有新建的parameters.\n",
    "    self.param1 = nn.Parameter(torch.empty((foo, bar), device=device))\n",
    "    self.register_parameter('param2', nn.Parameter(torch.empty(bar, device=device)))\n",
    "\n",
    "    # 为了支持skip_init中用的meta device,除了torch.nn.init中定义的初始化操作外\n",
    "    # 不能对module's constructor中直接定义的parameters做任何其他操作\n",
    "    with torch.no_grad():\n",
    "        nn.init.kaiming_uniform_(self.param1)\n",
    "        nn.init.uniform_(self.param2)\n",
    "\n",
    "\n",
    "    # ==== Case 2: Module中创建了submodules. ====\n",
    "    # Pass device along recursively. \n",
    "    # All submodules will need to support them as well\n",
    "    # 所有torch.nn提供的modules都满足条件\n",
    "    self.fc = nn.Linear(bar, 5, device=device)\n",
    "\n",
    "    # This also works with containers.\n",
    "    self.linears = nn.Sequential(\n",
    "        nn.Linear(5, 5, device=device),\n",
    "        nn.Linear(5, 1, device=device)\n",
    "    )\n",
    "\n",
    "\n",
    "    # ==== Case 3: Module中创建了buffers. ====\n",
    "    # Pass device along during buffer tensor creation.\n",
    "    self.register_buffer('some_buffer', torch.ones(7, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337005f",
   "metadata": {},
   "source": [
    "## 6. torch自带module中的典型layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d35d19",
   "metadata": {},
   "source": [
    "### nn.Flatten\n",
    "1. 参数：torch.nn.Flatten(start_dim=1, end_dim=-1)\n",
    "2. 压缩[start_dim, end_dim]范围的dims\n",
    "2. 默认将输入的data压成2维数据，保留原第一维，压缩剩下的维度，比如输出(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d741eb78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:22.745346Z",
     "start_time": "2024-08-06T02:37:22.742109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 784])\n",
      "torch.Size([84, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "flatten2 = nn.Flatten(0, 1)  # 压缩[0, 1]范围的dims\n",
    "flat_image2 = flatten2(input_image)\n",
    "print(flat_image2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15422dc0",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "1. affine layer\n",
    "2. 参数：torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "   · in_features (int) – size of each input sample\n",
    "   · out_features (int) – size of each output sample\n",
    "   · bias (bool)取False时, 就不会learn bias. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2975059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:22.749436Z",
     "start_time": "2024-08-06T02:37:22.746750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=6)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107dfd54",
   "metadata": {},
   "source": [
    "### nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9664ae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:22.753923Z",
     "start_time": "2024-08-06T02:37:22.750543Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU:\n",
      " tensor([[-0.1059, -0.3650, -0.2354,  0.1382,  0.0520,  0.0531],\n",
      "        [-0.2357, -0.4797, -0.2849, -0.0148,  0.2071,  0.2770],\n",
      "        [-0.0447, -0.2520, -0.0628,  0.0251,  0.4561,  0.1185]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "After ReLU:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.1382, 0.0520, 0.0531],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2071, 0.2770],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0251, 0.4561, 0.1185]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU:\\n {hidden1}\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU:\\n {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c6442",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "1. an ordered container of modules.\n",
    "2. 数据会按照Sequential中定义的layer顺序做处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88abdeb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T02:37:22.757978Z",
     "start_time": "2024-08-06T02:37:22.755018Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "scores = seq_modules(input_image)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
