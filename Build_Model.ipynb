{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23724491",
   "metadata": {},
   "source": [
    "# Build model\n",
    "<font color=blue>[包含tutorial的Build model和developer note的Modules]</font>\n",
    "## 1. 什么是pytorch中的module,pytorch提供了哪些module类型？\n",
    "· module是构建神经网络的基础模块。pytorch提供了一个modules库，也支持自定义modules。用他们可以很容易地构建多层神经网络。具体实现来看，<font color=green>**namespace**</font> **torch.nn**提供了layers, containers和utilities三种主要的module类型，以及tensor类型的nn.Parameter作为modules parameter。\n",
    "1. <font color=lightblue>**Layers：**</font>NN通过layers对数据进行操作。pytorch用modules来表达这些layers,比如conv, affine, pooling, normalization, transformer和loss functions等\n",
    "2. <font color=lightblue>**containers：**</font>有3类container，nn.Module，nn.Sequential和holders of submodules。\\\n",
    "(1)**torch.nn.Module**。它是所有NN modules的base class，pytorch中所有的module都是**nn.Module**的子类\\\n",
    "(2)**torch.nn.Sequential**：以序列形式将1个或多个module顺序排列，体现了module的nestable\\\n",
    "(3)holders of submodules,其中：**nn.ModuleList，nn.ModuleDict**分别是以list和dictionary类型存储的module序列。**nn.ParamterList和nn.ParameterDict**分别是以list和dictionary形式存储的参数。\n",
    "3. <font color=lightblue>**utilities：**</font>把一些数据处理的函数以modules的形式表达。<font color=red>【具体待使用后描述？？？】</font>\n",
    "\n",
    "## 2. module的特点\n",
    "1. module和autograd system一起工作：modules使optimizer update参数非常方便<font color=red>【理解？？？】</font>\n",
    "2. pytorch中的module可以nest：每个神经网络模型自身都是一个module，该module又由其他modules(layers)构成。这种nest structure可以很方便的构造复杂的网络架构。<font color=red>【理解？？？】</font>\n",
    "3. **nn.Module**的子类会自动track参数，可以用两个method来查看：parameters()和named_parameters()\n",
    "4. 很容易与Transform配合使用：modules的save和restore都很直接，在CPU/GPU之间移动，做prune，quantize和其他很多操作都很方便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ea8140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.824971Z",
     "start_time": "2023-10-21T10:41:05.478726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn           # for torch.nn.Module\n",
    "import torch.nn.functional as F # for the activation function\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0e4f0",
   "metadata": {},
   "source": [
    "## 3. 定义一个NN\n",
    "1. 自定义model也得定义为**nn.Module**的子类，每个子类必须定义\\__init__和forward()两个method。\n",
    "2. 模型对input data的操作都放在forward()中。即，forward()用来指定要执行的computation，用的operation是nn.autograd.Function的子类的实例。这些子类可以是pytorch定义好的，也可以是自定义的。\n",
    "3. **用nn.Mudule来实例化module时，只implement forward() method不用implement backward() method**，因为：\\\n",
    "(1)<font color=blue>用nn.autograd.Function来实例化（自定义）Function时，要同时implement forward() and backward() methods</font> \\\n",
    "(2)<font color=blue>autograd system会用Function中的backward来自动处理module中用到的function的backward pass。</font>\n",
    "4. 如果module中要定义parameters，就要在\\__init\\__()中register。方式是在\\__init\\__()中将parameter定义为nn.Parameter的实例。此时，这些parameters就是parameters registered by the module。这也是autograd system运行需要的。\n",
    "5. Parameter class是torch.Tensor的子类，但他们可以被assigned as attributes of a Module。一旦实例化后，这些parameters就会被加到lists of the module's parameters，之后可以通过module.parameters()和model.namedparameters()来iterate throgh。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb9df4",
   "metadata": {},
   "source": [
    "### 3.1 自定义一个简单的Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2951b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.831094Z",
     "start_time": "2023-10-21T10:41:06.827309Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):  # 必须是nn.Module的子类\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        # registering parameters: 参数定义成nn.Parameter的实例\n",
    "        # 此时autograd会自动tracking并让optimizer在迭代时update\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "  # implement forward() method\n",
    "    def forward(self, input):\n",
    "        return input @ self.weight + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63c55dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.868292Z",
     "start_time": "2023-10-21T10:41:06.832451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.0309, -0.7185,  2.5375], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyLinear(4, 3)          \n",
    "sample_input = torch.randn(4)  \n",
    "\n",
    "# model is callable, calling invoke forward function\n",
    "model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b45953",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.875050Z",
     "start_time": "2023-10-21T10:41:06.870328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 2.1661, -0.5888, -0.2786],\n",
      "        [ 0.2892, -0.3643, -1.0176],\n",
      "        [-0.2201, -0.3658,  0.7272],\n",
      "        [ 1.7846,  0.7567, -0.3533]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2654, -0.6061,  1.3762], requires_grad=True)\n",
      "\n",
      "\n",
      "('weight', Parameter containing:\n",
      "tensor([[ 2.1661, -0.5888, -0.2786],\n",
      "        [ 0.2892, -0.3643, -1.0176],\n",
      "        [-0.2201, -0.3658,  0.7272],\n",
      "        [ 1.7846,  0.7567, -0.3533]], requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([ 0.2654, -0.6061,  1.3762], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "## 遍历parameters()\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "## 遍历parameters named_parameters()\n",
    "#  这里weights和bias是parameter的name\n",
    "for parameter in model.named_parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec1897",
   "metadata": {},
   "source": [
    "### 3.2 将modules作为模型的基础模块(building blocks)\n",
    "· modules contain other modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed768239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T09:10:48.862543Z",
     "start_time": "2023-10-20T09:10:48.859143Z"
    }
   },
   "source": [
    "#### i. 用nn.Sequential定义一个简单的module\n",
    "· Sequential会自动将上一层的输出传给下一层作为输入。但只在输入和输出都是单变量的情况时有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d27ec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.881333Z",
     "start_time": "2023-10-21T10:41:06.876404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.4740], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Sequential本身也是nn.Module的子类，所以实例化得到的也是module\n",
    "net = nn.Sequential(\n",
    "    MyLinear(4, 3),\n",
    "    nn.ReLU(),\n",
    "    MyLinear(3, 1)\n",
    ")\n",
    "\n",
    "simple_input = torch.randn(4)\n",
    "net(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f1dcf",
   "metadata": {},
   "source": [
    "####  ii. 自定义module\n",
    "· 除了上面例子中非常简单的案例，通常都不会直接用Sequential来定义module，更多还是直接自定义module的方式.\\\n",
    "· 在__init__()中定义的submodule对应NN中的layer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd6b459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.886330Z",
     "start_time": "2023-10-21T10:41:06.882634Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = MyLinear(4, 3)\n",
    "        self.layer1 = MyLinear(3, 1)  # 定义了两个submodule\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = F.relu(x)                 # relu不是submodule\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64dc5a09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.891399Z",
     "start_time": "2023-10-21T10:41:06.887659Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = MyLinear(4, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = MyLinear(3, 1)  # 定义了两个submodule\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.relu(x)                 # relu不是submodule\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293354c",
   "metadata": {},
   "source": [
    "**module的Immediate children可以用children() or named_children()来iterated through** \\\n",
    "上例中的children(也就是submodule)不包括rely层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fea79e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.896350Z",
     "start_time": "2023-10-21T10:41:06.892755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('layer0', MyLinear())\n",
      "('layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "for child in net.named_children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7b6526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.900444Z",
     "start_time": "2023-10-21T10:41:06.897726Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对比前面例子中直接用tensor operation定义的module\n",
    "# 此时module中没有child\n",
    "for child in model.children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53edd8c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.906820Z",
     "start_time": "2023-10-21T10:41:06.903439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('layer0', MyLinear())\n",
      "('relu', ReLU())\n",
      "('layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "# 也可以把relu处理成module\n",
    "net2 = Net2()\n",
    "for child in net2.named_children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b11a5a",
   "metadata": {},
   "source": [
    "**modules() and named_modules() recursively iterate through a module and its child modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b7a02f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.912653Z",
     "start_time": "2023-10-21T10:41:06.908220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "('', BigNet(\n",
      "  (l1): MyLinear()\n",
      "  (net): Net(\n",
      "    (layer0): MyLinear()\n",
      "    (layer1): MyLinear()\n",
      "  )\n",
      "))\n",
      "----------------------------------------------------\n",
      "('l1', MyLinear())\n",
      "----------------------------------------------------\n",
      "('net', Net(\n",
      "  (layer0): MyLinear()\n",
      "  (layer1): MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('net.layer0', MyLinear())\n",
      "----------------------------------------------------\n",
      "('net.layer1', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "class BigNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = MyLinear(5, 4)\n",
    "        self.net = Net()\n",
    "    def forward(self, x):\n",
    "        return self.net(self.l1(x))\n",
    "\n",
    "big_net = BigNet()\n",
    "for module in big_net.named_modules():\n",
    "    print('-' * 52)\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff8d30",
   "metadata": {},
   "source": [
    "#### iii. dynamically define submodule\n",
    "· 用ModuleList或者ModuleDict \\\n",
    "· calls to parameters() and named_parameters() will recursively include child parameters, allowing for convenient optimization of all parameters within the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e91dcaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.918811Z",
     "start_time": "2023-10-21T10:41:06.914006Z"
    }
   },
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList(\n",
    "            [MyLinear(4, 4) for _ in range(num_layers)])\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU()\n",
    "        })\n",
    "        self.final = MyLinear(4, 1)\n",
    "        \n",
    "    def forward(self, x, act):\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "        x = self.activations[act](x)\n",
    "        # x = self.final(x)\n",
    "        return x\n",
    "\n",
    "dynamic_net = DynamicNet(3)\n",
    "sample_input = torch.randn(4)\n",
    "output = dynamic_net(sample_input, 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc64b6",
   "metadata": {},
   "source": [
    "** · child module由__init__()中排列的module sequence决定，不由forward()实际执行的computation决定**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3382a2f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.923464Z",
     "start_time": "2023-10-21T10:41:06.920253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "('', DynamicNet(\n",
      "  (linears): ModuleList(\n",
      "    (0-2): 3 x MyLinear()\n",
      "  )\n",
      "  (activations): ModuleDict(\n",
      "    (relu): ReLU()\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (final): MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('linears', ModuleList(\n",
      "  (0-2): 3 x MyLinear()\n",
      "))\n",
      "----------------------------------------------------\n",
      "('linears.0', MyLinear())\n",
      "----------------------------------------------------\n",
      "('linears.1', MyLinear())\n",
      "----------------------------------------------------\n",
      "('linears.2', MyLinear())\n",
      "----------------------------------------------------\n",
      "('activations', ModuleDict(\n",
      "  (relu): ReLU()\n",
      "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "))\n",
      "----------------------------------------------------\n",
      "('activations.relu', ReLU())\n",
      "----------------------------------------------------\n",
      "('activations.lrelu', LeakyReLU(negative_slope=0.01))\n",
      "----------------------------------------------------\n",
      "('final', MyLinear())\n"
     ]
    }
   ],
   "source": [
    "for module in dynamic_net.named_modules():\n",
    "    print('-'*52)\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "732d092a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:06.930174Z",
     "start_time": "2023-10-21T10:41:06.924952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "('linears.0.weight', Parameter containing:\n",
      "tensor([[ 0.2085,  0.4250, -1.4256,  0.9457],\n",
      "        [ 0.5809, -0.8168, -2.0355, -0.9586],\n",
      "        [ 0.7818,  0.7880, -0.1888, -0.8931],\n",
      "        [ 0.0041, -1.5959, -0.6526,  1.0554]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.0.bias', Parameter containing:\n",
      "tensor([-0.4171,  0.8124, -0.0701, -0.3478], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.1.weight', Parameter containing:\n",
      "tensor([[-0.1231,  0.6416, -0.6966,  0.1392],\n",
      "        [ 1.1746,  0.2638, -0.1142,  2.4611],\n",
      "        [-0.2381, -0.8532,  0.4539, -0.5426],\n",
      "        [-0.4069,  0.3406, -0.4137, -1.4952]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.1.bias', Parameter containing:\n",
      "tensor([-0.3339, -0.4314,  2.0615,  0.3918], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.2.weight', Parameter containing:\n",
      "tensor([[ 0.6430,  1.0690,  0.8880, -2.0829],\n",
      "        [-0.1778, -0.5018, -0.4777,  0.4206],\n",
      "        [-0.8981, -0.5841, -0.3553, -0.3681],\n",
      "        [-0.0639,  0.3816, -0.2552, -0.5321]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('linears.2.bias', Parameter containing:\n",
      "tensor([-1.3722, -1.1008,  0.0686,  0.6246], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('final.weight', Parameter containing:\n",
      "tensor([[ 2.4542e-01],\n",
      "        [ 6.4197e-04],\n",
      "        [-1.7379e+00],\n",
      "        [ 1.1674e+00]], requires_grad=True))\n",
      "--------------------------------------------------------------------\n",
      "('final.bias', Parameter containing:\n",
      "tensor([-0.7379], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for parameter in dynamic_net.named_parameters():\n",
    "    print('-'*68)\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457186a9",
   "metadata": {},
   "source": [
    "#### vi. 移动参数的设备，改变参数精度，用.to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99fcfad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:07.267110Z",
     "start_time": "2023-10-21T10:41:06.931483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.7393, 0.0000, 0.0000], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move all parameters to a CUDA device\n",
    "dynamic_net.to(device='cuda')\n",
    "\n",
    "# Change precision of all parameters\n",
    "dynamic_net.to(dtype=torch.float64)\n",
    "\n",
    "dynamic_net(torch.randn(4, device='cuda', dtype=torch.float64), 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d6774",
   "metadata": {},
   "source": [
    "#### v. module和submodule可以apply任意函数，包括自定义函数\n",
    "an arbitrary function can be applied to a module and its submodules recursively by using the apply() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cfa9bc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:07.274359Z",
     "start_time": "2023-10-21T10:41:07.268849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicNet(\n",
       "  (linears): ModuleList(\n",
       "    (0-2): 3 x MyLinear()\n",
       "  )\n",
       "  (activations): ModuleDict(\n",
       "    (relu): ReLU()\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (final): MyLinear()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to initialize Linear weights.\n",
    "# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.\n",
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.fill_(0.0)\n",
    "\n",
    "# Apply the function recursively on the module and its submodules.\n",
    "dynamic_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e9db5",
   "metadata": {},
   "source": [
    "## 4. 使用module训练NN\n",
    "**module有两种mode：trainning mode和evaluation mode**\n",
    "1. module默认处于training mode。用training()和eval()可以改变module所处mode。\n",
    "2. 如果module中有submodule在training mode和evaluation mode的时候输出不同，那么就应该在inference的时候将mode改为evaluation mode，比如batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e95af2a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.427623Z",
     "start_time": "2023-10-21T10:41:07.275553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer0): MyLinear()\n",
       "  (layer1): MyLinear()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新建network和optimizer\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, \n",
    "                            weight_decay=1e-2, momentum=0.9)\n",
    "\n",
    "# trainging the netword\n",
    "for _ in range(10000):\n",
    "    input = torch.randn(4)\n",
    "    output = net(input)\n",
    "    loss = torch.abs(output) # 用abs做loss，会让weights趋于0\n",
    "    \n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# training完成后，将module转到eval mode\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b35f50e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.434910Z",
     "start_time": "2023-10-21T10:41:10.429681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0006],\n",
      "        [-0.0039],\n",
      "        [-0.0005]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.layer1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88587166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.446600Z",
     "start_time": "2023-10-21T10:41:10.436363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training mode output: tensor([1.1455, 1.2983, 2.2661, 0.2162])\n",
      "evaluation mode output: tensor([ 0.1455,  0.2983,  1.2661, -0.7838])\n"
     ]
    }
   ],
   "source": [
    "# 在training和evaluation mode下输出不同的例子\n",
    "class ModalModule(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.training:\n",
    "      # Add a constant only in training mode.\n",
    "      return x + 1.\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "m = ModalModule()\n",
    "x = torch.randn(4)\n",
    "print('training mode output: {}'.format(m(x)))\n",
    "\n",
    "m.eval()\n",
    "print('evaluation mode output: {}'.format(m(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd31a1",
   "metadata": {},
   "source": [
    "## 5. module state\n",
    "1. 如果要保存a trained model，可以存该module的state_dict，state_dict中保存了影响module运算的状态。state包括parameters和buffers。\\\n",
    "(1)**parameters**: learnable aspects of computation,存在state_dict中。\\\n",
    "(2)**buffers**: non-learnable aspects of computation. 有的module会存储参数之外的其他信息到state_dict，和参数不同的是，这些信息不需要learn，他们会被存在buffers中。\n",
    "2. 有两种buffers：Persistent buffers存在state_dict中，non-Persistent buffers不存在state_dict中。\\\n",
    "(1)Persistent buffers: 比如：serialized when saving and loading\n",
    "(2)non-Persistent buffers: 比如：left out of serialization\n",
    "3. Persistent buffers的特点：\\\n",
    "(1)如果state被存为state_dict的一部分，那么loading a serialized form of the module的时候，它就能被restore。 \\\n",
    "(2)这部分变量不会像parameters那样被optimizer处理，因而是non-leanable\n",
    "4. non-Persistent buffers的特点：\\\n",
    "不存为state_dict的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f574254",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.454321Z",
     "start_time": "2023-10-21T10:41:10.447873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save the module\n",
    "torch.save(net.state_dict(), 'net.pt')\n",
    "\n",
    "## load the module\n",
    "#  1. 新建一个结构相同的module\n",
    "new_net = Net()\n",
    "#  2. load state\n",
    "new_net.load_state_dict(torch.load('net.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64a45baa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.459292Z",
     "start_time": "2023-10-21T10:41:10.455374Z"
    }
   },
   "outputs": [],
   "source": [
    "## 使用buffers：module中要保存running mean\n",
    "#  将running mean的当前值存到state_dict用register_buffer()\n",
    "\n",
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('mean', torch.zeros(num_features))\n",
    "        # 此时，self.mean会被存到state_dict中\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 每次迭代时更新running mean的值\n",
    "        # 作为state_dict的一部分，当loading module的时候会被restore\n",
    "        self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n",
    "        return self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2b31eb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.467854Z",
     "start_time": "2023-10-21T10:41:10.460465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))])\n",
      "tensor(True)\n",
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "m = RunningMean(4)\n",
    "for _ in range(10):\n",
    "    input = torch.randn(4)\n",
    "    m(input)\n",
    "\n",
    "print(m.state_dict())\n",
    "\n",
    "# Serialized form will contain the 'mean' tensor\n",
    "torch.save(m.state_dict(), 'mean.pt')\n",
    "\n",
    "m_loaded = RunningMean(4)\n",
    "m_loaded.load_state_dict(torch.load('mean.pt'))\n",
    "\n",
    "# 注意这里几种assert和print的差异\n",
    "assert(torch.all(m.mean == m_loaded.mean))\n",
    "print(torch.all(m.mean == m_loaded.mean))\n",
    "print(m.mean == m_loaded.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c8ab8e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.475182Z",
     "start_time": "2023-10-21T10:41:10.468971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "## 将running mean存为non-Persistent buffers\n",
    "#  还是用register_buffer()，参数Persistent=False\n",
    "\n",
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('mean', torch.zeros(num_features), persistent=False)\n",
    "        # 此时，self.mean不会被存到state_dict中\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n",
    "        return self.mean\n",
    "\n",
    "torch.manual_seed(0)\n",
    "m2 = RunningMean(4)\n",
    "for _ in range(10):\n",
    "    input = torch.randn(4)\n",
    "    m2(input)\n",
    "\n",
    "print(m2.state_dict()) # 此时输出的state_dict是空的\n",
    "\n",
    "torch.save(m2.state_dict(), 'mean.pt')\n",
    "m2_loaded = RunningMean(4)\n",
    "m2_loaded.load_state_dict(torch.load('mean.pt'))\n",
    "print(torch.all(m2.mean == m2_loaded.mean)) # 输出False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d9821",
   "metadata": {},
   "source": [
    "#### 一个module的buffers可以用buffers()和named_buffers()来迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d438e109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:44:14.606505Z",
     "start_time": "2023-10-21T10:44:14.600564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))\n"
     ]
    }
   ],
   "source": [
    "for buffer in m.named_buffers():\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30acb35d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:44:34.016016Z",
     "start_time": "2023-10-21T10:44:34.010610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean', tensor([-0.1494,  0.1179, -0.3679, -0.1974]))\n"
     ]
    }
   ],
   "source": [
    "for buffer in m2.named_buffers():\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c081a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:42:48.068015Z",
     "start_time": "2023-10-21T10:42:48.065554Z"
    }
   },
   "source": [
    "#### 两种buffers都受model-wide device/type changes所使用的.to() method影响\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86d454fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:45:14.753088Z",
     "start_time": "2023-10-21T10:45:14.696520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunningMean()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.to(device='cuda', dtype=torch.float64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c37348dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:56:34.346063Z",
     "start_time": "2023-10-21T10:56:34.327397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('param1', tensor([ 0.2126, -0.6581])), ('param2', tensor([ 0.3668,  1.1357, -0.3493])), ('buffer1', tensor([ 0.4541, -0.6406,  0.5335,  0.3342])), ('param_list.0', tensor([-2.1108, -2.1679])), ('param_list.1', tensor([0.2092, 0.6620])), ('param_list.2', tensor([-0.6632, -0.2582])), ('param_dict.bar', tensor([-0.4564,  0.8851,  1.3771,  0.1046])), ('param_dict.foo', tensor([ 0.9113, -1.2949, -0.9157])), ('linear.weight', tensor([[ 0.2240, -0.5740],\n",
      "        [-0.2254,  0.4324],\n",
      "        [-0.3923, -0.3800]])), ('linear.bias', tensor([-0.2514,  0.3433,  0.1073]))])\n"
     ]
    }
   ],
   "source": [
    "## 一个综合例子\n",
    "class StatefulModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 用nn.Parameter实例化的参数会自动将tensor register为module parameter\n",
    "        self.param1 = nn.Parameter(torch.randn(2))\n",
    "\n",
    "        # 另一种将tensor register为module parameter的方式：用register_parameter() method\n",
    "        self.register_parameter('param2', nn.Parameter(torch.randn(3)))\n",
    "\n",
    "        # 将attribute： \"param3\" 定义为一个parameter，但不做初始化。\n",
    "        # 它的值'None'不会出现在state_dict中    \n",
    "        self.register_parameter('param3', None)\n",
    "\n",
    "        # Registers a list of parameters：没有name\n",
    "        self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])\n",
    "\n",
    "        # Registers a dictionary of parameters：有name\n",
    "        self.param_dict = nn.ParameterDict({\n",
    "            'foo': nn.Parameter(torch.randn(3)),\n",
    "            'bar': nn.Parameter(torch.randn(4))\n",
    "        })\n",
    "\n",
    "        # Registers a persistent buffer\n",
    "        self.register_buffer('buffer1', torch.randn(4), persistent=True)\n",
    "\n",
    "        # Registers a non-persistent buffer\n",
    "        self.register_buffer('buffer2', torch.randn(5), persistent=False)\n",
    "\n",
    "        # 将attribute：\"buffer3\" 定义为一个buffer，但不做初始化\n",
    "        # 它的值'None'也不会出现在state_dict中    \n",
    "        self.register_buffer('buffer3', None)\n",
    "\n",
    "        # 添加一个submodule就会将其parameters自动register为module的parameters\n",
    "        self.linear = nn.Linear(2, 3)\n",
    "\n",
    "m = StatefulModule()\n",
    "\n",
    "# Save and load state_dict.\n",
    "torch.save(m.state_dict(), 'state.pt')\n",
    "m_loaded = StatefulModule()\n",
    "m_loaded.load_state_dict(torch.load('state.pt'))\n",
    "\n",
    "# state_dict中没有non-persistent buffer和reserved attributes \"param3\"与\"buffer3\"\n",
    "print(m_loaded.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b9a9c",
   "metadata": {},
   "source": [
    "## 6. module初始化\n",
    "1. 默认情况下，torch.nn提供的module中的parameter和浮点数buffer会在module实例化的时候初始化为存在CPU上的32位浮点数值。\n",
    "2. 如果要改变默认的初始化设置，可以在module实例化的时候设置对应的arguments或者直接用skip_init()method，之后自定义初始化方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa148d03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T12:39:48.680077Z",
     "start_time": "2023-10-21T12:39:48.623122Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将module直接初始化到GPU上，参数类型为16位浮点数\n",
    "m = nn.Linear(5, 3, device='cuda', dtype=torch.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd9f2a1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T12:41:59.924331Z",
     "start_time": "2023-10-21T12:41:59.910966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# 除参数外，上述初始化方式也适用于floating-point buffers registered for the module\n",
    "m = nn.BatchNorm2d(3, dtype=torch.half)\n",
    "print(m.running_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcdf79a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T12:39:49.740095Z",
     "start_time": "2023-10-21T12:39:49.724751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.9562,  0.2601,  0.0900,  0.0803, -0.0594],\n",
       "        [ 0.0368, -0.2720, -0.2461,  0.8976,  0.2415],\n",
       "        [-0.1160,  0.7019, -0.6861, -0.0115,  0.1517]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例：自定义参数初始化为正交矩阵\n",
    "m = torch.nn.utils.skip_init(nn.Linear, 5, 3)\n",
    "nn.init.orthogonal_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857bbe7",
   "metadata": {},
   "source": [
    "#### 自定义module的时候，建议按照torch.nn所遵守的规则那样：\n",
    "1. 提供一个device constructor kwarg，可以应用在任意的parameter和buffers registered by the module上\n",
    "2. 提供一个dtype constructor kwarg，可以应用在任意的parameter和floating-point buffers registered by the module上\n",
    "3. 只用初始化函数（比如：torch.nn.init package提供的函数）来初始化module constructor中的parameters和buffers。注意，此时要使用skip_init()。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5872e3",
   "metadata": {},
   "source": [
    "## 7. module hooks\n",
    "1. 什么是hooks\\\n",
    "为了更具体地控制training的过程，pytorch提供了hooks可以用来在forward和backward过程中进行任意的computation，甚至可以用来改变原pass如何执行。常见的使用场景包括：debugging，visualization activation，检查梯度等。hooks可以用用在pytorch或者第三方提供的modules上。\n",
    "\n",
    "2. 两种类型的hooks\\\n",
    "(1)forward hooks: 在forward pass中被调用，可以通过<font color=green>**register_forward_pre_hook()**</font>和<font color=green>**register_forward_hook()**</font>两种函数来install。他们分别会在调用forward function的前一步和后一步被调用。\\\n",
    "· 如果要在所有module中都调用hook，可以用对应的<font color=green>**register_module_forward_pre_hook()**</font>和<font color=green>**register_module_forward_hook()**</font>两种method。\\\n",
    "(2)backward hooks:在backward pass中被调用，可以通过<font color=green>**register_full_backward_pre_hook()**</font>和<font color=green>**register_full_backward_hook()**</font>两种函数来install。前者可以用来获取outputs的梯度，后者可以获取inputs和outputs的梯度。\n",
    "· 他们也有对应的global版本，也就是installed for all modules的版本，<font color=green>**register_module_full_backward_hook()**</font>和<font color=green>**register_module_full_backward_pre_hook()**</font>。\n",
    "\n",
    "3. 特点：所有hooks都可以返回更新后的value，这些value也会被用到hooks所在pass的后续计算中。因此，hooks可以用来在常规module的forward/backward过程中执行任意的code，或者用来调整inputs/outputs而不用改变module本身的forward() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec6959e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T13:32:32.867702Z",
     "start_time": "2023-10-21T13:32:32.860293Z"
    }
   },
   "outputs": [],
   "source": [
    "## 定义几个hooks\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 用于在forward pass前，检查或者调整inputs\n",
    "# 注意，inputs都是wrapped成tuple类型的\n",
    "def forward_pre_hook(m, inputs):\n",
    "    input = inputs[0]\n",
    "    return input + 1.\n",
    "\n",
    "# 用于在forward pass后，检查inputs/outputs或者调整outputs\n",
    "# 注意，inputs都是wrapped成tuple类型的，output都是are passed as-is\n",
    "def forward_hook(m, inputs, output):\n",
    "    # 按ResNet的方式计算residual\n",
    "    return output + inputs[0]\n",
    "\n",
    "# 检查grad_inputs/grad_outputs或者调整剩余bp流程中用的grad_inputs\n",
    "# 注意，grad_inputs/grad_outputs都是wrapped成tuple类型的\n",
    "def backward_hook(m, grad_inputs, grad_outputs):\n",
    "    new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]\n",
    "    return new_grad_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43696e5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T13:46:02.212709Z",
     "start_time": "2023-10-21T13:46:02.194801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output with no forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n",
      "        [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward0>)\n",
      "output with forward pre hook: tensor([[-0.5752, -0.7421,  0.4942],\n",
      "        [-0.0736,  0.5461,  0.0838]], grad_fn=<AddmmBackward0>)\n",
      "output with both forward hooks: tensor([[-1.0980,  0.6396,  0.4666],\n",
      "        [ 0.3634,  0.6538,  1.0256]], grad_fn=<AddBackward0>)\n",
      "output after removing forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n",
      "        [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward0>)\n",
      "x.grad with no backwards hook: tensor([[ 0.4497, -0.5046,  0.3146],\n",
      "        [ 0.4497, -0.5046,  0.3146]])\n",
      "x.grad with backwards hook: tensor([[42., 42., 42.],\n",
      "        [42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 新建module和input\n",
    "m = nn.Linear(3, 3)\n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "\n",
    "# forward hooks：\n",
    "# Run input through module before and after adding hooks.\n",
    "print('output with no forward hooks: {}'.format(m(x)))\n",
    "\n",
    "# input调整后会产生不同的output：\n",
    "forward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)\n",
    "print('output with forward pre hook: {}'.format(m(x)))\n",
    "\n",
    "# 调整后的output：\n",
    "forward_hook_handle = m.register_forward_hook(forward_hook)\n",
    "print('output with both forward hooks: {}'.format(m(x)))\n",
    "\n",
    "# 去掉hooks之后，output与adding hooks之前的值一致\n",
    "forward_pre_hook_handle.remove()\n",
    "forward_hook_handle.remove()\n",
    "print('output after removing forward hooks: {}'.format(m(x)))\n",
    "\n",
    "# backward hooks:\n",
    "m(x).sum().backward()\n",
    "print('x.grad with no backwards hook: {}'.format(x.grad))\n",
    "\n",
    "# Clear gradients before running backward pass again.\n",
    "m.zero_grad()\n",
    "x.grad.zero_()\n",
    "\n",
    "m.register_full_backward_hook(backward_hook)\n",
    "m(x).sum().backward()\n",
    "print('x.grad with backwards hook: {}'.format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656e1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74cfdaed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.479987Z",
     "start_time": "2023-10-21T10:41:10.476496Z"
    }
   },
   "outputs": [],
   "source": [
    "# 自定义NN\n",
    "class RKNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15acc066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.489550Z",
     "start_time": "2023-10-21T10:41:10.481218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RKNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 创建自定义NN的实例\n",
    "model = RKNet().to(device)  # model要建在gpu上\n",
    "print(model)                # 打印model的structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb7a088f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.509010Z",
     "start_time": "2023-10-21T10:41:10.490692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict class:tensor([9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "scores = model(X)\n",
    "\n",
    "prob = nn.Softmax(dim=1)(scores) # dim决定softmax求解的维度\n",
    "y_pred = prob.argmax(1)\n",
    "print(f'predict class:{y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337005f",
   "metadata": {},
   "source": [
    "## 典型layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d35d19",
   "metadata": {},
   "source": [
    "### nn.Flatten\n",
    "1. 参数：torch.nn.Flatten(start_dim=1, end_dim=-1)\n",
    "2. 压缩[start_dim, end_dim]范围的dims\n",
    "2. 默认将输入的data压成2维数据，保留原第一维，压缩剩下的维度，比如输出(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d741eb78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.514478Z",
     "start_time": "2023-10-21T10:41:10.510423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 784])\n",
      "torch.Size([84, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "flatten2 = nn.Flatten(0, 1)  # 压缩[0, 1]范围的dims\n",
    "flat_image2 = flatten2(input_image)\n",
    "print(flat_image2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15422dc0",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "1. affine layer\n",
    "2. 参数：torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "   · in_features (int) – size of each input sample\n",
    "   · out_features (int) – size of each output sample\n",
    "   · bias (bool)取False时, 就不会learn bias. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2975059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.523119Z",
     "start_time": "2023-10-21T10:41:10.517984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=6)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107dfd54",
   "metadata": {},
   "source": [
    "### nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9664ae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.527866Z",
     "start_time": "2023-10-21T10:41:10.524350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU:\n",
      " tensor([[-0.4493,  0.2986,  0.1468,  0.5824, -0.2445, -0.4647],\n",
      "        [-0.2795,  0.2134,  0.3590,  0.6150, -0.1564, -0.1344],\n",
      "        [-0.3351,  0.0757,  0.2176, -0.1208,  0.0721, -0.0469]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "After ReLU:\n",
      " tensor([[0.0000, 0.2986, 0.1468, 0.5824, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2134, 0.3590, 0.6150, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0757, 0.2176, 0.0000, 0.0721, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU:\\n {hidden1}\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU:\\n {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c6442",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "1. an ordered container of modules.\n",
    "2. 数据会按照Sequential中定义的layer顺序做处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88abdeb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.532228Z",
     "start_time": "2023-10-21T10:41:10.529175Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "scores = seq_modules(input_image)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d1ff4e",
   "metadata": {},
   "source": [
    "## 模型参数\n",
    "1. NN中的一些layers有参数，比如有的layers在training后都有weights和bias\n",
    "2. 把model定义为nn.Module的子类后，nn.Module能自动track所有model object中定义的fields，而参数可以通过model的parameters()和named_parameters()两种method来获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f185b9d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T10:41:10.542346Z",
     "start_time": "2023-10-21T10:41:10.533685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: RKNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0333,  0.0317,  0.0272,  ...,  0.0353,  0.0072, -0.0024],\n",
      "        [ 0.0352, -0.0153, -0.0029,  ..., -0.0003, -0.0164,  0.0121]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0028, -0.0085], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0304,  0.0364,  0.0080,  ..., -0.0357, -0.0252, -0.0362],\n",
      "        [ 0.0225,  0.0356, -0.0259,  ...,  0.0257,  0.0016,  0.0228]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0402,  0.0176], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0427,  0.0136,  0.0137,  ..., -0.0142,  0.0276,  0.0203],\n",
      "        [-0.0187, -0.0265, -0.0128,  ...,  0.0314, -0.0431,  0.0440]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0136, -0.0178], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
